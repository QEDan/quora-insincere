#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path, errno

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")
                    
        make_package(os.path.dirname(path))
        
        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('''src/Data.py''', '''from collections import defaultdict\n\nimport logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom src.config import config_data\nfrom src.text_cleaning import clean_contractions, clean_specials, clean_spelling, \\\n    clean_acronyms, clean_non_dictionary, clean_numbers\n\n\nclass Data:\n    """ Loads and preprocesses data """\n    def __init__(self, train_path="../input/train.csv", test_path="../input/test.csv",\n                 text_col='question_text', id_col='qid', label_col='target'):\n\n        self.text_col = text_col\n        self.id_col = id_col\n        self.label_col = label_col\n        self.train_path = train_path\n        self.test_path = test_path\n\n        self.train_df = None\n        self.test_df = None\n\n    def load(self, dev_size=None):\n        logging.info("Loading data...")\n        if dev_size is not None:\n            logging.warning("Using dev set of size=" + str(dev_size))\n        self.train_df = pd.read_csv(self.train_path, nrows=dev_size)\n        self.test_df = pd.read_csv(self.test_path, nrows=dev_size)\n        logging.info("Train shape : {}".format(self.train_df.shape))\n        logging.info("Test shape : {}".format(self.test_df.shape))\n\n    def split(self):\n        self.train_qs, self.val_qs, self.train_labels, self.val_labels = self.get_training_split()\n        return self.train_qs, self.val_qs, self.train_labels, self.val_labels\n\n    @staticmethod\n    def preprocessing(questions):\n\n        questions = questions.fillna("_na_")\n        preprocess_config = config_data.get('preprocess')\n        case_sensitive = not preprocess_config.get('lower_case')\n        if preprocess_config.get('lower_case'):\n            questions = questions.str.lower()\n        # trouble removing stop words before we have tokenized the text, this has to happen later\n        # if preprocess_config.get('remove_stop_words'):\n            # questions = questions.apply(remove_stops)\n        if preprocess_config.get('remove_contractions'):\n            questions = questions.apply(lambda x: clean_contractions(x))\n        if preprocess_config.get('remove_specials'):\n            questions = questions.apply(lambda x: clean_specials(x))\n        if preprocess_config.get('correct_spelling'):\n            questions = questions.apply(lambda x: clean_spelling(x, case_sensitive=case_sensitive))\n        if preprocess_config.get('replace_acronyms'):\n            questions = questions.apply(lambda x: clean_acronyms(x, case_sensitive=case_sensitive))\n        if preprocess_config.get('replace_non_words'):\n            questions = questions.apply(lambda x: clean_non_dictionary(x, case_sensitive=case_sensitive))\n        if preprocess_config.get('replace_numbers'):\n            questions = questions.apply(lambda x: clean_numbers(x))\n        return questions\n\n    def get_questions(self, subset='train'):\n        # todo: add functionality to only get data with a certain label (if we want to fine tune word embeddings...)\n        if subset == 'train':\n            data = list(self.train_df[self.text_col])\n        if subset == 'test':\n            data = list(self.test_df[self.text_col])\n        if subset == 'all':\n            data = list(self.train_df[self.text_col]) + list(self.test_df[self.text_col])\n        return data\n\n    def get_training_labels(self):\n        labels = self.train_df.loc[:, self.label_col].values\n        return labels\n\n    def get_training_split(self, test_size=0.1, seed=0):\n        train_qs, val_qs, train_labels, val_labels = train_test_split(self.train_df[self.text_col].tolist(),\n                                                                      self.train_df[self.label_col].tolist(),\n                                                                      stratify=self.train_df[self.label_col].tolist(),\n                                                                      test_size=test_size,\n                                                                      random_state=seed)\n        return train_qs, val_qs, train_labels, val_labels\n\n\nclass CorpusInfo:\n    """ Calculates corpus information to be referenced during feature engineering later """\n    # todo: pass in a general tokenizer (so that it matches the tokenizer in the rest of the pipeline)\n    # todo: how can we make this run faster? can be parallelized...?\n    def __init__(self, questions, nlp, word_lowercase=True, char_lowercase=True):\n        self.nlp = nlp\n        self.word_lowercase = word_lowercase\n        self.char_lowercase = char_lowercase\n\n        self.word_counts = []\n        self.char_counts = []\n        self.sent_lengths = []\n        self.word_lengths = []\n\n        self.calc_corpus_info(questions)\n\n    def calc_corpus_info(self, questions):\n        word_counters = defaultdict(int)\n        char_counters = defaultdict(int)\n\n        for question in questions:\n            tokenized_question = self.nlp(question)\n            self.sent_lengths.append(len(tokenized_question))\n            for token in tokenized_question:\n                text = token.text\n                self.word_lengths.append(len(text))\n                word_to_count = text.lower() if self.word_lowercase else text\n                word_counters[word_to_count] += 1\n                for char in text:\n                    char_to_count = char.lower() if self.char_lowercase else char\n                    char_counters[char_to_count] += 1\n\n        self.word_counts = sorted(word_counters.items(), key=lambda x: x[1], reverse=True)\n        self.char_counts = sorted(char_counters.items(), key=lambda x: x[1], reverse=True)\n\n    def plot_word_lengths(self, max_len):\n        plt.hist(self.word_lengths, bins=np.arange(0, max_len, 2), cumulative=True, normed=1)\n        plt.hlines(0.975, 0, 30, colors='red')\n\n    def plot_sent_lengths(self, max_len):\n        plt.hist(self.sent_lengths, bins=np.arange(0, max_len, 2), cumulative=True, normed=1)\n        plt.hlines(0.975, 0, 30, colors='red')\n\n''')
    __stickytape_write_module('''src/config.py''', '''"""Model and hyperparameter configurations. This file is generated by generate_config(). Do not edit!"""\n\n\nrandom_state = 2018\n\nconfig_data = {'max_feature': 50000,\n 'max_seq_len': 200,\n 'preprocess': {'correct_spelling': True,\n                'lower_case': False,\n                'remove_contractions': True,\n                'remove_specials': True,\n                'remove_stop_words': False,\n                'replace_acronyms': True,\n                'replace_non_words': True,\n                'replace_numbers': False,\n                'use_custom_features': True},\n 'test_size': 0.1}\n\nconfig_insincere_model = {'callbacks': {'checkpoint': {'mode': 'max',\n                              'monitor': 'val_f1_score',\n                              'save_best_only': True,\n                              'verbose': True},\n               'early_stopping': {'mode': 'max',\n                                  'monitor': 'val_f1_score',\n                                  'patience': 5,\n                                  'verbose': True}},\n 'fit': {'batch_size': 1000,\n         'epochs': 5,\n         'pseudo_labels': False,\n         'save_curve': True},\n 'predict': {'batch_size': 1024, 'verbose': True}}\n\nconfig_lrfinder = {'loss_smoothing_beta': 0.98,\n 'lr_scale': 'exp',\n 'maximum_lr': 10.0,\n 'minimum_lr': 1e-05,\n 'save_dir': None,\n 'stopping_criterion_factor': 4,\n 'validation_sample_rate': 5,\n 'verbose': True}\n\nconfig_one_cycle = {'end_percentage': 0.1,\n 'max_lr': 0.1,\n 'maximum_momentum': 0.95,\n 'minimum_momentum': 0.85,\n 'scale_percentage': None,\n 'verbose': True}\n\nconfig_main = {'dev_size': None,\n 'embedding_files': ['../input/embeddings/glove.840B.300d/glove.840B.300d.txt',\n                     '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt']}\n\n''')
    __stickytape_write_module('''src/__init__.py''', '''''')
    __stickytape_write_module('''src/text_cleaning.py''', '''import re\nfrom nltk.corpus import stopwords\n\n\ndef remove_stops(sentence):\n    """ Should apply only after tokenization """\n    stop = set(stopwords.words('english'))\n    filtered = list()\n    for w in sentence.split(" "):\n        if w not in stop:\n            filtered.append(w)\n    return " ".join(filtered)\n\n\ndef clean_contractions(text):\n    contraction_mapping = {"ain't": "is not", "aren't": "are not", "can't": "cannot", "'cause": "because",\n                           "could've": "could have", "couldn't": "could not", "didn't": "did not",\n                           "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not",\n                           "haven't": "have not", "he'd": "he would", "he'll": "he will", "he's": "he is",\n                           "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",\n                           "I'd": "i would", "I'd've": "I would have", "I'll": "i will", "I'll've": "i will have",\n                           "I'm": "i am", "I've": "i have", "i'd": "i would", "i'd've": "i would have",\n                           "i'll": "i will", "i'll've": "i will have", "i'm": "i am", "i've": "i have",\n                           "isn't": "is not", "it'd": "it would", "it'd've": "it would have", "it'll": "it will",\n                           "it'll've": "it will have", "it's": "it is", "let's": "let us", "ma'am": "madam",\n                           "mayn't": "may not", "might've": "might have", "mightn't": "might not",\n                           "mightn't've": "might not have", "must've": "must have", "mustn't": "must not",\n                           "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have",\n                           "o'clock": "of the clock", "oughtn't": "ought not", "oughtn't've": "ought not have",\n                           "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",\n                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will",\n                           "she'll've": "she will have", "she's": "she is", "should've": "should have",\n                           "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have",\n                           "so's": "so as", "this's": "this is", "that'd": "that would",\n                           "that'd've": "that would have", "that's": "that is", "there'd": "there would",\n                           "there'd've": "there would have", "there's": "there is", "here's": "here is",\n                           "they'd": "they would", "they'd've": "they would have", "they'll": "they will",\n                           "they'll've": "they will have", "they're": "they are", "they've": "they have",\n                           "to've": "to have", "wasn't": "was not", "we'd": "we would", "we'd've": "we would have",\n                           "we'll": "we will", "we'll've": "we will have", "we're": "we are", "we've": "we have",\n                           "weren't": "were not", "what'll": "what will", "what'll've": "what will have",\n                           "what're": "what are", "what's": "what is", "what've": "what have", "when's": "when is",\n                           "when've": "when have", "where'd": "where did", "where's": "where is",\n                           "where've": "where have", "who'll": "who will", "who'll've": "who will have",\n                           "who's": "who is", "who've": "who have", "why's": "why is", "why've": "why have",\n                           "will've": "will have", "won't": "will not", "won't've": "will not have",\n                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have",\n                           "y'all": "you all", "y'all'd": "you all would", "y'all'd've": "you all would have",\n                           "y'all're": "you all are", "y'all've": "you all have", "you'd": "you would",\n                           "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",\n                           "you're": "you are", "you've": "you have"}\n    specials = ["\u2019", "\u2018", "\xb4", "`"]\n    for s in specials:\n        text = text.replace(s, "'")\n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(" ")])\n    return text\n\n\ndef clean_specials(text):\n    punct = "/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~" + '""\u201c\u201d\u2019' + '\u221e\u03b8\xf7\u03b1\u2022\xe0\u2212\u03b2\u2205\xb3\u03c0\u2018\u20b9\xb4\xb0\xa3\u20ac\\\xd7\u2122\u221a\xb2\u2014\u2013&'\n    puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']', '>', '%', '=',\n              '#', '*', '+', '\\\\', '\u2022', '~', '@', '\xa3',\n              '\xb7', '_', '{', '}', '\xa9', '^', '\xae', '`', '<', '\u2192', '\xb0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\xd7', '\xa7', '\u2033', '\u2032',\n              '\xc2', '\u2588', '\xbd', '\xe0', '\u2026',\n              '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\xe2', '\u25ba', '\u2212', '\xa2', '\xb2', '\xac', '\u2591', '\xb6', '\u2191', '\xb1', '\xbf', '\u25be', '\u2550', '\xa6', '\u2551',\n              '\u2015', '\xa5', '\u2593', '\u2014', '\u2039', '\u2500',\n              '\u2592', '\uff1a', '\xbc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\xa8', '\u2584', '\u266b', '\u2606', '\xe9', '\xaf', '\u2666', '\xa4', '\u25b2', '\xe8',\n              '\xb8', '\xbe', '\xc3', '\u22c5', '\u2018', '\u221e',\n              '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\xbb', '\uff0c', '\u266a', '\u2569', '\u255a', '\xb3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\xef',\n              '\xd8', '\xb9', '\u2264', '\u2021', '\u221a', ]\n    punct_mapping = {"\u2018": "'", "\u20b9": "e", "\xb4": "'", "\xb0": "", "\u20ac": "e", "\u2122": "tm", "\u221a": " sqrt ", "\xd7": "x", "\xb2": "2",\n                     "\u2014": "-", "\u2013": "-", "\u2019": "'", "_": "-", "`": "'", '\u201c': '"', '\u201d': '"', '\u201c': '"', "\xa3": "e",\n                     '\u221e': 'infinity', '\u03b8': 'theta', '\xf7': '/', '\u03b1': 'alpha', '\u2022': '.', '\xe0': 'a', '\u2212': '-',\n                     '\u03b2': 'beta', '\u2205': '', '\xb3': '3', '\u03c0': 'pi', }\n    for p in punct_mapping:\n        text = text.replace(p, punct_mapping[p])\n    for p in set(list(punct) + puncts) - set(punct_mapping.keys()):\n        text = text.replace(p, f' {p} ')\n\n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '',\n                '\u0939\u0948': ''}\n    for s in specials:\n        text = text.replace(s, specials[s])\n    return text\n\n\ndef clean_spelling(text, case_sensitive=False):\n    misspell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n                     'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                     'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n                     'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                     'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n                     'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n                     'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n                     'mastrubate': 'masturbate', "mastrubating": 'masturbating', 'pennis': 'penis',\n                     'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n                     '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n                     "whst": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                     'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n    for word in misspell_dict.keys():\n        if case_sensitive:\n            text = text.replace(word, misspell_dict[word])\n        else:\n            re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n            text = re_insensitive.sub(misspell_dict[word], text)\n    return text\n\n\ndef clean_acronyms(text, case_sensitive=False):\n    acronym_dict = {'upsc': 'union public service commission',\n                    'aiims': 'all india institute of medical sciences',\n                    'cgl': 'graduate level examination',\n                    'icse': 'indian school certificate exam',\n                    'iiit': 'indian institute of information technology',\n                    'cgpa': 'cumulative grade point average',\n                    'ielts': 'international english language training system',\n                    'ncert': 'national council of education research training',\n                    'isro': 'indian space research organization',\n                    'clat': 'common law admission test',\n                    'ibps': 'institute of banking personnel selection',\n                    'iiser': 'indian institute of science education and research',\n                    'iisc': 'indian institute of science',\n                    'iims': 'indian institutes of management',\n                    'cpec': 'china pakistan economic corridor'\n\n                    }\n    for word in acronym_dict.keys():\n        if case_sensitive:\n            text = text.replace(word, acronym_dict[word])\n        else:\n            re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n            text = re_insensitive.sub(acronym_dict[word], text)\n    return text\n\n\ndef clean_non_dictionary(text, case_sensitive=False):\n    replace_dict = {'quorans': 'users',\n                    'quoran': 'user',\n                    'jio': 'phone manufacturer',\n                    'manipal': 'city',\n                    'bitsat': 'exam',\n                    'mtech': 'technical university',\n                    'pilani': 'town',\n                    'bhu': 'university',\n                    'h1b': 'visa',\n                    'redmi': 'phone manufacturer',\n                    'nift': 'university',\n                    'kvpy': 'exam',\n                    'thanos': 'comic villain',\n                    'paytm': 'payment system',\n                    'comedk': 'medical consortium',\n                    'accenture': 'management consulting company',\n                    'llb': 'bachelor of laws',\n                    'ignou': 'university',\n                    'dtu': 'university',\n                    'aadhar': 'social number',\n                    'lenovo': 'computer manufacturer',\n                    'gmat': 'exam',\n                    'kiit': 'institute of technology',\n                    'shopify': 'music streaming',\n                    'fitjee': 'exam',\n                    'kejriwal': 'politician',\n                    'wbjee': 'exam',\n                    'pgdm': 'master of business administration',\n                    'trudeau': 'politician',\n                    'nri': 'research institute',\n                    'deloitte': 'accounting company',\n                    'jinping': 'politician',\n                    'bcom': 'bachelor of commerce',\n                    'mcom': 'masters of commerce',\n                    'virat': 'athlete',\n                    'kcet': 'television network',\n                    'wipro': 'information technology company',\n                    'articleship': 'internship',\n                    'comey': 'law enforcement director',\n                    'jnu': 'university',\n                    'acca': 'chartered accountants',\n                    'aakash': 'phone manufacturer',\n                    'brexit': 'british succession',\n                    'crypto': 'digital currency',\n                    'cryptocurrency': 'digital currency',\n                    'cryptocurrencies': 'digital currencies',\n                    'etherium': 'digital currency',\n                    'bitcoin': 'digital currency',\n                    'viteee': 'exam',\n                    'iocl': 'indian oil company',\n                    'nmims': 'management school',\n                    'rohingya': 'myanmar people',\n                    'fortnite': 'videogame',\n                    'upes': 'university',\n                    'nsit': 'university',\n                    'coinbase': 'digital currency exchange'\n                    }\n    for word in replace_dict.keys():\n        if case_sensitive:\n            text = text.replace(word, replace_dict[word])\n        else:\n            re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n            text = re_insensitive.sub(replace_dict[word], text)\n    return text\n\n\ndef clean_numbers(text, min_magnitude=2, max_magnitude=10):\n    for n in range(min_magnitude, max_magnitude):\n        text = re.sub('[0-9]{' + str(n) + '}', '#' * n, text)\n    return text\n''')
    __stickytape_write_module('''src/data_mappers.py''', '''import numpy as np\n\n\nclass TextMapper:\n    """ Maps text into model input x """\n\n    def __init__(self, word_counts, char_counts, nlp, max_sent_len=100, word_threshold=10, word_lowercase=False,\n                 max_word_len=12, char_threshold=5, char_lowercase=False):\n        """\n        :param word_counts: list of tuples of (word, count)\n        :param nlp: from pre-loaded spaCy\n        :param max_sent_len: maximum words in a doc\n        :param word_threshold: number of times a word must appear in word_counts for it to get a representation\n        :param word_lowercase: boolean, whether words should be lowercased\n        """\n        self.max_sent_len = max_sent_len\n        self.max_word_len = max_word_len\n        self.word_mapper = WordMapper(word_counts=word_counts, threshold=word_threshold, max_sent_len=max_sent_len,\n                                      word_lowercase=word_lowercase)\n        self.char_mapper = CharMapper(char_counts=char_counts, threshold=char_threshold, max_word_len=max_word_len,\n                                      char_lowercase=char_lowercase)\n\n        self.max_sent_len = max_sent_len\n        self.max_word_len = max_word_len\n        self.nlp = nlp\n\n    def text_to_x(self, text):\n        """ Handles mapping one text doc into model inputs """\n        words_x = np.zeros(self.max_sent_len)\n        chars_x = np.zeros((self.max_sent_len, self.max_word_len))\n\n        tokenized_question = self.nlp(text)\n\n        for word_ind, token in enumerate(tokenized_question[:self.max_sent_len]):\n            word = token.text\n            words_x[word_ind] = self.word_mapper.get_symbol_index(word)\n            for char_ind, char in enumerate(word[:self.max_word_len]):\n                chars_x[word_ind][char_ind] = self.char_mapper.get_symbol_index(char)\n\n        return words_x, chars_x\n\n    def x_to_words(self, words_x, remove_padding=True):\n        words = [self.word_mapper.ix_to_symbol[int(i)] for i in words_x]\n        comment_text = " ".join(words)\n\n        # remove padding\n        if remove_padding:\n            comment_text = comment_text.split(self.word_mapper.PADDING_SYMBOL)[0]\n\n        return comment_text\n\n    def x_to_chars(self, chars_x, remove_padding=True):\n        chars = []\n        for word_x in chars_x:\n            word_chars = [self.char_mapper.ix_to_symbol[x] for x in word_x]\n            word = "".join(word_chars)\n            # remove_padding\n            if remove_padding:\n                word = word.split(self.char_mapper.PADDING_SYMBOL)[0]\n\n            # don't add empty words\n            if word:\n                chars.append(word)\n\n        question_chars = " ".join(chars)\n\n        return question_chars\n\n    def texts_to_x(self, texts):\n        inputs_x = [self.text_to_x(text) for text in texts]\n        words_input, chars_input = map(np.array, zip(*inputs_x))\n        # return words_input\n        return {"words_input": words_input, "chars_input": chars_input}\n\n    def set_max_sentence_len(self, max_sent_len):\n        self.word_mapper.set_max_len(max_sent_len)\n\n    def set_max_char_len(self, max_char_len):\n        self.char_mapper.set_max_len(max_char_len)\n\n    def get_words_vocab(self):\n        return self.word_mapper.vocab\n\n\nclass SymbolMapper:\n    """ Handles mapping of any symbol (words or characters) into something an model an ingest """\n    PADDING_SYMBOL = "<PAD>"\n    UNKNOWN_SYMBOL = "<UNK>"\n    BASE_ALPHABET = [PADDING_SYMBOL, UNKNOWN_SYMBOL]\n\n    def __init__(self, symbol_counts, threshold, max_len, lowercase):\n        self.symbol_counts = symbol_counts\n        self.threshold = threshold\n        self.max_len = max_len\n        self.lowercase = lowercase\n\n        self.vocab = []\n        self.symbol_to_ix = dict()\n        self.ix_to_symbol = dict()\n\n        self.init_mappings(threshold)\n\n    def init_mappings(self, check_coverage=True):\n        symbol_counts = sorted(self.symbol_counts, key=lambda x: x[1], reverse=True)\n        self.vocab = [symbol for symbol, count in symbol_counts if count >= self.threshold]\n        self.vocab = self.BASE_ALPHABET + self.vocab\n\n        self.symbol_to_ix = {symbol: ix for ix, symbol in enumerate(self.vocab)}\n        self.ix_to_symbol = {ix: symbol for ix, symbol in enumerate(self.vocab)}\n\n        if check_coverage:\n            self.print_coverage_statistics()\n\n    def set_threshold(self, threshold, check_coverage=True):\n        self.threshold = threshold\n        self.init_mappings(check_coverage)\n\n    def set_max_len(self, max_len):\n        self.max_len = max_len\n\n    def print_top_n_symbols(self, n=10):\n        print([(symbol, count) for symbol, count in self.symbol_counts if count >= self.threshold][:n])\n\n    def print_bot_n_symbols(self, n=10):\n        print([(symbol, count) for symbol, count in self.symbol_counts if count >= self.threshold][-n:])\n\n    def get_symbol_index(self, symbol):\n        if self.lowercase:\n            symbol = symbol.lower()\n        try:\n            num = self.symbol_to_ix[symbol]\n        except KeyError:\n            num = self.symbol_to_ix[self.UNKNOWN_SYMBOL]\n        return num\n\n    def get_vocab_len(self):\n        return len(self.symbol_to_ix)\n\n    def print_coverage_statistics(self, symbols_name='symbol', persist=True):\n        """\n        Simple metric on coverage of symbols\n\n        :param symbols_name: str, printed to distinguish different mappers\n        :param persist: bool, write stats to file rather than stdout\n        """\n        symbol_mappings = self.symbol_to_ix.keys()\n        with open('coverage_stats.txt', 'w') if persist else None as f:\n            print("Number of unique {}: {}".format(symbols_name, len(self.symbol_counts)), file=f)\n            print("Number of unique {} mapped: {}".format(symbols_name, len(symbol_mappings)), file=f)\n            total_tokens = 0\n            mapped_tokens = 0\n            for symbol, count in self.symbol_counts:\n                total_tokens += count\n                if symbol in symbol_mappings:\n                    mapped_tokens += count\n            print("Percent of unique symbols mapped: {}%".format(\n                100 * len(symbol_mappings) / len(self.symbol_counts)),\n                file=f)\n            print("Percent of total symbols mapped: {}%".format(\n                100 * mapped_tokens / total_tokens),\n                file=f)\n\n\nclass WordMapper(SymbolMapper):\n\n    def __init__(self, word_counts, threshold, max_sent_len, word_lowercase):\n        super().__init__(word_counts, threshold, max_sent_len, word_lowercase)\n\n    def print_coverage_statistics(self, symbols_name='words'):\n        super().print_coverage_statistics(symbols_name)\n\n\nclass CharMapper(SymbolMapper):\n\n    def __init__(self, char_counts, threshold, max_word_len, char_lowercase):\n        super().__init__(char_counts, threshold, max_word_len, char_lowercase)\n\n    def print_coverage_statistics(self, symbols_name='chars'):\n        super().print_coverage_statistics(symbols_name)\n''')
    __stickytape_write_module('''src/Embedding.py''', '''import gc\nimport time\n\nimport logging\nimport numpy as np\nimport operator\nimport traceback\nfrom gensim.models import KeyedVectors\n\n\nclass Embedding:\n    def __init__(self, word_vocab):\n        self.embeddings_index = None\n        self.nb_words = None\n        self.embed_size = None\n        self.embedding_matrix = None\n        self.word_vocab = word_vocab\n        self.name = None\n\n    def load(self, embedding_file='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'):\n        logging.info("loading embedding : " + embedding_file)\n        self.name = embedding_file.split('/')[3]\n\n        def get_coefs(word, *arr):\n            return word, np.asarray(arr, dtype='float32')\n\n        if "wiki-news" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" "))\n                                         for i, o in enumerate(open(embedding_file)) if len(o) > 100)\n        elif "glove" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" ")) for i, o in enumerate(open(embedding_file)))\n        elif "paragram" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" ")) for i, o in\n                                         enumerate(open(embedding_file, encoding="utf8", errors='ignore'))\n                                         if len(o) > 100)\n        elif "GoogleNews" in embedding_file:\n            self.embeddings_index = {}\n            wv_from_bin = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n            for i, (word, vector) in enumerate(zip(wv_from_bin.vocab, wv_from_bin.vectors)):\n                coefs = np.asarray(vector, dtype='float32')\n                self.embeddings_index[word] = coefs\n        else:\n            raise ValueError("Unsupported embedding file: " + embedding_file)\n\n        try:\n            all_embs = np.stack(self.embeddings_index.values())\n\n        except ValueError as e:\n            logging.error(e)\n            tb = traceback.format_exc()\n            logging.error(tb)\n            logging.debug("len(self.embeddings_index.values()): "\n                          + str(len(self.embeddings_index.values())))\n            logging.debug("type(self.embeddings_index.values()[0]): "\n                          + str(type(list(self.embeddings_index.values())[0])))\n            logging.debug("first few self.embeddings_index.values(): "\n                          + str(list(self.embeddings_index.values())[:5]))\n            raise\n\n        emb_mean, emb_std = all_embs.mean(), all_embs.std()\n        self.embed_size = all_embs.shape[1]\n\n        self.nb_words = len(self.word_vocab)\n        self.embedding_matrix = np.random.normal(emb_mean, emb_std, (self.nb_words, self.embed_size))\n        for ind, word in enumerate(self.word_vocab):\n            # todo: freezing pre-trained embeddings and unfreezing these unknown, random word embeddings\n            embedding_vector = self.embeddings_index.get(word)\n            if embedding_vector is not None:\n                self.embedding_matrix[ind] = embedding_vector\n\n    def get_embedding_matrix(self):\n        return self.embedding_matrix\n\n    def check_coverage(self, vocab):\n        known_words = {}\n        unknown_words = {}\n        nb_known_words = 0\n        nb_unknown_words = 0\n        for word in vocab.keys():\n            try:\n                known_words[word] = self.embeddings_index[word]\n                nb_known_words += vocab[word]\n            except:\n                unknown_words[word] = vocab[word]\n                nb_unknown_words += vocab[word]\n                pass\n\n        logging.info('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n        logging.info('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n        unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n        return unknown_words\n\n    def cleanup(self):\n        logging.info("Releasing memory...")\n        try:\n            del self.embeddings_index, self.embedding_matrix\n            gc.collect()\n            time.sleep(10)\n        except AttributeError:\n            logging.warning('embeddings index not found. They were probably already cleaned up.')\n\n    def cleanup_index(self):\n        logging.info("Releasing memory...")\n        try:\n            del self.embeddings_index\n            gc.collect()\n            time.sleep(10)\n        except AttributeError:\n            logging.warning('embeddings index not found. They were probably already cleaned up.')\n''')
    __stickytape_write_module('''src/Models.py''', '''import matplotlib\nfrom keras import Input, Model\n\nfrom src.InsincereModel import InsincereModel\n\nmatplotlib.use('Agg')\n\nfrom src.Models import *  # Make all models available for easy script generation.\n\nfrom keras.layers import TimeDistributed, Embedding as EmbeddingLayer, Bidirectional, CuDNNLSTM, Dense, Conv1D\nfrom keras.layers import GlobalMaxPooling1D, Concatenate\n\n\nclass BiLSTMCharCNNModel(InsincereModel):\n    def define_model(self, model_config=None):\n        # if model_config is None:\n        #     model_config = self.default_config()\n\n\n        max_sent_len = self.text_mapper.max_sent_len\n        max_word_len = self.text_mapper.max_word_len\n        word_vocab_size = self.text_mapper.word_mapper.get_vocab_len()\n        char_vocab_size = self.text_mapper.char_mapper.get_vocab_len()\n\n        chars_input = Input(shape=(max_sent_len, max_word_len), name='chars_input', dtype='int64')\n\n        char_features = char_level_feature_model(chars_input, max_word_len, char_vocab_size)\n\n        words_input = Input(shape=(max_sent_len,), name='words_input', dtype='int64')\n        if self.embedding is not None:\n            words_embedding = EmbeddingLayer(input_dim=word_vocab_size, output_dim=self.embedding.embedding_matrix.shape[1],\n                                             input_length=max_sent_len,\n                                             weights=[self.embedding.embedding_matrix] if self.embedding else None,\n                                             trainable=False)(words_input)\n        else:\n            words_embedding = EmbeddingLayer(input_dim=word_vocab_size, output_dim=10,\n                                             input_length=max_sent_len)(words_input)\n\n        word_rep = Concatenate()([char_features, words_embedding])\n\n        # todo: maybe bidirectional lstm is just too slow, can try a deeper convolutional network\n        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(word_rep)\n        x = Bidirectional(CuDNNLSTM(64))(x)\n        # x = Conv1D(filters=100, kernel_size=2)(x)\n        # max_x = GlobalMaxPooling1D()(x)\n        # avg_x = GlobalAveragePooling1D()(x)\n        # x = Concatenate()([max_x, avg_x])\n        # x = Dense(16)(x)\n        # x = Flatten()(char_sum)\n        preds = Dense(1, activation='sigmoid')(x)\n\n        inputs = [chars_input, words_input]\n\n        self.model = Model(inputs=inputs, outputs=preds)\n        self.model.compile(loss=self.loss, optimizer='adam', metrics=['accuracy', self.f1_score])\n\n        return self.model\n\n\nclass CharCNNWordModel(InsincereModel):\n    """ this is an experiment to check that character convolutions are outputting as expected """\n    def define_model(self, model_config=None):\n        # if model_config is None:\n        #     model_config = self.default_config()\n\n        max_sent_len = self.text_mapper.max_sent_len\n        max_word_len = self.text_mapper.max_word_len\n        char_vocab_size = self.text_mapper.char_mapper.get_vocab_len()\n\n        # load in character input\n        chars_input = Input(shape=(max_sent_len, max_word_len), name='chars_input', dtype='int64')\n\n        # time distributed applies the same layer to each time step (for each word)\n        chars_words_embedding = TimeDistributed(EmbeddingLayer(char_vocab_size, output_dim=16, input_length=max_word_len))(chars_input)\n\n        # todo: add another input here with additional character information (caps, number, punc, etc)\n\n        # do one dimensional convolutions over each word. filter size will determine size of vector for each word (if globalpool)\n        char_conv = TimeDistributed(Conv1D(filters=300, kernel_size=3))(chars_words_embedding)\n\n        # represent each filter with it's max value -  each filter looks for one feature\n        x = TimeDistributed(GlobalMaxPooling1D())(char_conv)\n        preds = Dense(1, activation='sigmoid')(x)\n\n        inputs = [chars_input]\n\n        self.model = Model(inputs=inputs, outputs=preds)\n        self.model.compile(loss=self.loss, optimizer='adam', metrics=['accuracy', self.f1_score])\n\n        return self.model\n\n\ndef char_level_feature_model(input_layer, max_word_len, char_vocab_size):\n    chars_words_embedding = TimeDistributed(EmbeddingLayer(char_vocab_size, output_dim=16, input_length=max_word_len))(input_layer)\n    # todo: add additional char features here\n    conv_outputs = []\n    # todo: tune these conv kernels\n    conv_kernels = [[32, 2], [32, 3], [32, 4]]\n    for num_filter, kernel_size in conv_kernels:\n        char_conv = TimeDistributed(Conv1D(filters=num_filter, kernel_size=kernel_size))(chars_words_embedding)\n        # todo: add dropout or batchnorm here? global average pooling?\n        x = TimeDistributed(GlobalMaxPooling1D())(char_conv)\n        conv_outputs.append(x)\n    char_features_rep = Concatenate()(conv_outputs)\n    # todo: add dense layers here to better represent character features\n    return char_features_rep\n\n''')
    __stickytape_write_module('''src/InsincereModel.py''', '''import keras.backend as K\nimport logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom src.Embedding import Embedding\nfrom src.LRFinder import LRFinder\nfrom src.OneCycleLR import OneCycleLR\nfrom src.config import config_insincere_model\n\n\nclass InsincereModel:\n    def __init__(self, data, corpus_info, text_mapper, batch_size=16, name=None, loss='binary_crossentropy'):\n        self.data = data\n        self.corpus_info = corpus_info\n        self.text_mapper = text_mapper\n        self.name = name\n        self.embedding = None\n        self.model = None\n        self.history = None\n        self.loss = loss\n        self.lr_finder = None\n        self.config = config_insincere_model\n        self.batch_size = batch_size\n\n    def load_embedding(self, embedding_file='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'):\n        self.embedding = Embedding(self.corpus_info.word_counts)\n        self.embedding.load(embedding_file)\n\n    def set_embedding(self, embedding):\n        if type(embedding) is str:\n            self.load_embedding(embedding)\n        else:\n            self.embedding = embedding\n\n    def blend_embeddings(self, embeddings, cleanup=False):\n        """Average embedding matrix given list of embedding files."""\n        if self.embedding is None:\n            self.set_embedding(embeddings[0])\n        embedding_matrices = list()\n        for emb in embeddings:\n            embedding_matrices.append(emb.embedding_matrix)\n        blend = np.mean(embedding_matrices, axis=0)\n        self.embedding.embedding_matrix = blend\n        if cleanup:\n            for e in embeddings:\n                e.cleanup()\n        return blend\n\n    def concat_embeddings(self, embeddings, cleanup=False):\n        if self.embedding is None:\n            self.set_embedding(embeddings[0])\n        self.embedding.embedding_matrix = np.concatenate(tuple([e.embedding_matrix for e in embeddings]), axis=1)\n        self.embedding.embed_size = self.embedding.embedding_matrix.shape[1]\n        if cleanup:\n            for e in embeddings:\n                e.cleanup()\n        return self.embedding.embedding_matrix\n\n    @staticmethod\n    def f1_score(y_true, y_pred):\n        def recall(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n            recall = true_positives / (possible_positives + K.epsilon())\n            return recall\n\n        def precision(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n            precision = true_positives / (predicted_positives + K.epsilon())\n            return precision\n\n        precision = precision(y_true, y_pred)\n        recall = recall(y_true, y_pred)\n        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n    def define_model(self):\n        raise NotImplementedError\n\n    def print(self):\n        print(self.model.summary())\n\n    def _get_callbacks(self, epochs, batch_size, checkpoint=False, one_cycle=False):\n        config = self.config.get('callbacks')\n        early_stop = EarlyStopping(monitor=config.get('early_stopping').get('monitor'),\n                                   mode=config.get('early_stopping').get('mode'),\n                                   patience=config.get('early_stopping').get('patience'),\n                                   verbose=config.get('early_stopping').get('verbose'),\n                                   restore_best_weights=True)\n        cbs = [early_stop]\n        if one_cycle:\n            num_samples = len(self.data.train_qs)\n            self.lr_finder = LRFinder(num_samples, batch_size)\n            lr_manager = OneCycleLR(num_samples, epochs, batch_size)\n            cbs += [self.lr_finder, lr_manager]\n        if checkpoint:\n            check_point = ModelCheckpoint('model.hdf5',\n                                          monitor=config.get('checkpoint').get('monitor'),\n                                          mode=config.get('checkpoint').get('mode'),\n                                          verbose=config.get('checkpoint').get('verbose'),\n                                          save_best_only=config.get('checkpoint').get('save_best_only'))\n            cbs += [check_point]\n\n        return cbs\n\n    def fit(self, curve_file_suffix=None):\n        logging.info("Fitting model...")\n        config = self.config.get('fit')\n\n        train_questions = self.data.train_qs\n        val_questions = self.data.val_qs\n\n        train_x = self.prepare_model_inputs(train_questions)\n        val_x = self.prepare_model_inputs(val_questions)\n\n        train_y = np.array(self.data.train_labels)\n        val_y = np.array(self.data.val_labels)\n\n        # train_generator = DataGenerator(text=self.data.train_qs, labels=self.data.train_labels,\n        #                                 text_mapper=self.text_mapper, batch_size=self.batch_size)\n        # val_generator = DataGenerator(text=self.data.val_qs, labels=self.data.val_labels,\n        #                               text_mapper=self.text_mapper, batch_size=self.batch_size)\n\n        callbacks = self._get_callbacks(config.get('epochs'), config.get('batch_size'))\n\n        self.history = self.model.fit(x=train_x,\n                                      y=train_y,\n                                      epochs=2,\n                                      batch_size=32,\n                                      validation_data=(val_x, val_y),\n                                      callbacks=callbacks)\n\n        if config.get('save_curve'):\n            if self.lr_finder:\n                self.lr_finder.plot_schedule(filename="lr_schedule_" + str(self.name) + ".png")\n            filename = 'training_curve'\n            if self.name:\n                filename += '_' + self.name\n            if curve_file_suffix:\n                filename += '_' + curve_file_suffix\n            filename += '.png'\n            self.print_curve(filename)\n\n    def predict_subset(self, subset='train'):\n        if subset == 'train':\n            questions = self.data.train_qs\n        elif subset == 'val':\n            questions = self.data.val_qs\n        elif subset == 'test':\n            questions = self.data.get_questions(subset)\n\n        input_x = self.prepare_model_inputs(questions)\n        preds = self.predict(input_x)\n\n        return preds\n\n    def print_curve(self, filename='training_curve.png'):\n        plt.plot(self.history.history['loss'])\n        plt.plot(self.history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'val'], loc='best')\n        plt.savefig(filename)\n        plt.close()\n\n    def predict(self, x):\n        logging.info("Predicting ...")\n        batch_size = self.config.get('predict').get('batch_size')\n        verbose = self.config.get('predict').get('verbose')\n        prediction = self.model.predict(x, batch_size=batch_size, verbose=verbose)\n        return prediction\n\n    def cleanup(self):\n        self.embedding.cleanup()\n\n    def prepare_model_inputs(self, questions):\n        model_input = self.text_mapper.texts_to_x(questions)\n        words_input = model_input['words_input']\n        chars_input = model_input['chars_input']\n        return {'words_input': words_input, 'chars_input': chars_input}\n''')
    __stickytape_write_module('''src/LRFinder.py''', '''import keras.backend as K\nimport numpy as np\nimport os\nimport warnings\nfrom keras.callbacks import Callback\n\nfrom src.config import config_lrfinder as config\n\n\nclass LRFinder(Callback):\n\n    def __init__(self, num_samples, batch_size, validation_data=None):\n        """\n        This class uses the Cyclic Learning Rate history to find a\n        set of learning rates that can be good initializations for the\n        One-Cycle training proposed by Leslie Smith in the paper referenced\n        below.\n        A port of the Fast.ai implementation for Keras.\n        # Note\n        This requires that the model be trained for exactly 1 epoch. If the model\n        is trained for more epochs, then the metric calculations are only done for\n        the first epoch.\n        # Interpretation\n        Upon visualizing the loss plot, check where the loss starts to increase\n        rapidly. Choose a learning rate at somewhat prior to the corresponding\n        position in the plot for faster convergence. This will be the maximum_lr lr.\n        Choose the max value as this value when passing the `max_val` argument\n        to OneCycleLR callback.\n        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n        # Arguments:\n            num_samples: Integer. Number of samples in the dataset.\n            batch_size: Integer. Batch size during training.\n            minimum_lr: Float. Initial learning rate (and the minimum).\n            maximum_lr: Float. Final learning rate (and the maximum).\n            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n                scaling for each update to the learning rate during subsequent\n                batches. Choose 'exp' for large range and 'linear' for small range.\n            validation_data: Requires the validation dataset as a tuple of\n                (X, y) belonging to the validation set. If provided, will use the\n                validation set to compute the loss metrics. Else uses the training\n                batch loss. Will warn if not provided to alert the user.\n            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n                validation set per iteration of the LRFinder. Larger number of\n                samples will reduce the variance but will take longer time to execute\n                per batch.\n                If Positive > 0, will sample from the validation dataset\n                If Megative, will use the entire dataset\n            stopping_criterion_factor: Integer or None. A factor which is used\n                to measure large increase in the loss value during training.\n                Since callbacks cannot stop training of a model, it will simply\n                stop logging the additional values from the epochs after this\n                stopping criterion has been met.\n                If None, this check will not be performed.\n            loss_smoothing_beta: Float. The smoothing factor for the moving\n                average of the loss function.\n            save_dir: Optional, String. If passed a directory path, the callback\n                will save the running loss and learning rates to two separate numpy\n                arrays inside this directory. If the directory in this path does not\n                exist, they will be created.\n            verbose: Whether to print the learning rate after every batch of training.\n        # References:\n            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n        """\n        super(LRFinder, self).__init__()\n\n        minimum_lr = config.get('minimum_lr')\n        maximum_lr = config.get('maximum_lr')\n        lr_scale = config.get('lr_scale')\n        validation_sample_rate = config.get('validation_sample_rate')\n        stopping_criterion_factor = config.get('stopping_criterion_factor')\n        loss_smoothing_beta = config.get('loss_smoothing_beta')\n        save_dir = config.get('save_dir')\n        verbose = config.get('verbose')\n        if lr_scale not in ['exp', 'linear']:\n            raise ValueError("`lr_scale` must be one of ['exp', 'linear']")\n\n        if validation_data is not None:\n            self.validation_data = validation_data\n            self.use_validation_set = True\n\n            if validation_sample_rate > 0 or validation_sample_rate < 0:\n                self.validation_sample_rate = validation_sample_rate\n            else:\n                raise ValueError("`validation_sample_rate` must be a positive or negative integer other than o")\n        else:\n            self.use_validation_set = False\n            self.validation_sample_rate = 0\n\n        self.num_samples = num_samples\n        self.batch_size = batch_size\n        self.initial_lr = minimum_lr\n        self.final_lr = maximum_lr\n        self.lr_scale = lr_scale\n        self.stopping_criterion_factor = stopping_criterion_factor\n        self.loss_smoothing_beta = loss_smoothing_beta\n        self.save_dir = save_dir\n        self.verbose = verbose\n\n        self.num_batches_ = num_samples // batch_size - 1\n        self.current_lr_ = minimum_lr\n\n        if lr_scale == 'exp':\n            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (1. / float(self.num_batches_))\n        else:\n            extra_batch = int((num_samples % batch_size) != 0)\n            self.lr_multiplier_ = np.linspace(minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n\n        # If negative, use entire validation set\n        if self.validation_sample_rate < 0:\n            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n\n        self.current_batch_ = 0\n        self.current_epoch_ = 0\n        self.best_loss_ = 1e6\n        self.running_loss_ = 0.\n\n        self.history = {}\n\n    def on_train_begin(self, logs=None):\n        self.current_epoch_ = 1\n        K.set_value(self.model.optimizer.lr, self.initial_lr)\n\n        warnings.simplefilter("ignore")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.current_batch_ = 0\n\n        if self.current_epoch_ > 1:\n            warnings.warn("\\n\\nLearning rate finder should be used only with a single epoch. "\n                          "Hereafter, the callback will not measure the losses.\\n\\n")\n\n    def on_batch_begin(self, batch, logs=None):\n        self.current_batch_ += 1\n\n    def on_batch_end(self, batch, logs=None):\n        if self.current_epoch_ > 1:\n            return\n\n        if self.use_validation_set:\n            X, Y = self.validation_data[0], self.validation_data[1]\n\n            # use 5 random batches from test set for fast approximate of loss\n            num_samples = self.batch_size * self.validation_sample_rate\n\n            if num_samples > X.shape[0]:\n                num_samples = X.shape[0]\n\n            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n            x = X[idx]\n            y = Y[idx]\n\n            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n            loss = values[0]\n        else:\n            loss = logs['loss']\n\n        # smooth the loss value and bias correct\n        running_loss = self.loss_smoothing_beta * loss + (1. - self.loss_smoothing_beta) * loss\n        running_loss = running_loss / (1. - self.loss_smoothing_beta ** self.current_batch_)\n\n        # stop logging if loss is too large\n        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n                running_loss > self.stopping_criterion_factor * self.best_loss_):\n\n            if self.verbose:\n                print(" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)" % (\n                    self.stopping_criterion_factor, self.best_loss_\n                ))\n            return\n\n        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n            self.best_loss_ = running_loss\n\n        current_lr = K.get_value(self.model.optimizer.lr)\n\n        self.history.setdefault('running_loss_', []).append(running_loss)\n        if self.lr_scale == 'exp':\n            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n        else:\n            self.history.setdefault('log_lrs', []).append(current_lr)\n\n        # compute the lr for the next batch and update the optimizer lr\n        if self.lr_scale == 'exp':\n            current_lr *= self.lr_multiplier_\n        else:\n            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n\n        K.set_value(self.model.optimizer.lr, current_lr)\n\n        # save the other metrics as well\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        if self.verbose:\n            if self.use_validation_set:\n                print(" - LRFinder: val_loss: %1.4f - lr = %1.8f " % (values[0], current_lr))\n            else:\n                print(" - LRFinder: lr = %1.8f " % current_lr)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.save_dir is not None and self.current_epoch_ <= 1:\n            if not os.path.exists(self.save_dir):\n                os.makedirs(self.save_dir)\n\n            losses_path = os.path.join(self.save_dir, 'losses.npy')\n            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n\n            np.save(losses_path, self.losses)\n            np.save(lrs_path, self.lrs)\n\n            if self.verbose:\n                print("\\tLR Finder : Saved the losses and learning rate values in path : {%s}" % (self.save_dir))\n\n        self.current_epoch_ += 1\n\n        warnings.simplefilter("default")\n\n    def plot_schedule(self, filename="lr_schedule.png", clip_beginning=None, clip_endding=None):\n        """\n        Plots the schedule from the callback itself.\n        # Arguments:\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        """\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print("Matplotlib not found. Please use `pip install matplotlib` first.")\n            return\n\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses = self.losses\n        lrs = self.lrs\n\n        if clip_beginning:\n            losses = losses[clip_beginning:]\n            lrs = lrs[clip_beginning:]\n\n        if clip_endding:\n            losses = losses[:clip_endding]\n            lrs = lrs[:clip_endding]\n\n        plt.plot(lrs, losses)\n        plt.gca().set_yscale('log')\n        plt.title('Learning rate vs Loss')\n        plt.xlabel('log(learning rate)')\n        plt.ylabel('log(loss)')\n        plt.savefig(filename)\n\n    @classmethod\n    def restore_schedule_from_dir(cls, directory, clip_beginning=None, clip_endding=None):\n        """\n        Loads the training history from the saved numpy files in the given directory.\n        # Arguments:\n            directory: String. Path to the directory where the serialized numpy\n                arrays of the loss and learning rates are saved.\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        Returns:\n            tuple of (losses, learning rates)\n        """\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses_path = os.path.join(directory, 'losses.npy')\n        lrs_path = os.path.join(directory, 'lrs.npy')\n\n        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n            print("%s and %s could not be found at directory : {%s}" % (\n                losses_path, lrs_path, directory\n            ))\n\n            losses = None\n            lrs = None\n\n        else:\n            losses = np.load(losses_path)\n            lrs = np.load(lrs_path)\n\n            if clip_beginning:\n                losses = losses[clip_beginning:]\n                lrs = lrs[clip_beginning:]\n\n            if clip_endding:\n                losses = losses[:clip_endding]\n                lrs = lrs[:clip_endding]\n\n        return losses, lrs\n\n    @classmethod\n    def plot_schedule_from_file(cls, directory, clip_beginning=None, clip_endding=None):\n        """\n        Plots the schedule from the saved numpy arrays of the loss and learning\n        rate values in the specified directory.\n        # Arguments:\n            directory: String. Path to the directory where the serialized numpy\n                arrays of the loss and learning rates are saved.\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        """\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print("Matplotlib not found. Please use `pip install matplotlib` first.")\n            return\n\n        losses, lrs = cls.restore_schedule_from_dir(directory,\n                                                    clip_beginning=clip_beginning,\n                                                    clip_endding=clip_endding)\n\n        if losses is None or lrs is None:\n            return\n        else:\n            plt.plot(lrs, losses)\n            plt.title('Learning rate vs Loss')\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.show()\n\n    @property\n    def lrs(self):\n        return np.array(self.history['log_lrs'])\n\n    @property\n    def losses(self):\n        return np.array(self.history['running_loss_'])\n''')
    __stickytape_write_module('''src/OneCycleLR.py''', '''import keras.backend as K\nfrom keras.callbacks import Callback\n\nfrom src.config import config_one_cycle as config\n\n\nclass OneCycleLR(Callback):\n    def __init__(self, num_samples, num_epochs, batch_size):\n        """ This callback implements a cyclical learning rate policy (CLR).\n        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n        After the completion of 1 cycle, the learning rate will decrease rapidly to\n        100th its initial lowest value.\n        # Arguments:\n            num_samples: Integer. Number of sample points in the dataset\n            num_epochs: Integer. Number of training epochs\n            batch_size: Integer. Batch size per training epoch\n            max_lr: Float. Initial learning rate. This also sets the\n                starting learning rate (which will be 10x smaller than\n                this), and will increase to this value during the first cycle.\n            end_percentage: Float. The percentage of all the epochs of training\n                that will be dedicated to sharply decreasing the learning\n                rate after the completion of 1 cycle. Must be between 0 and 1.\n            scale_percentage: Float or None. If float, must be between 0 and 1.\n                If None, it will compute the scale_percentage automatically\n                based on the `end_percentage`.\n            maximum_momentum: Optional. Sets the maximum momentum (initial)\n                value, which gradually drops to its lowest value in half-cycle,\n                then gradually increases again to stay constant at this max value.\n                Can only be used with SGD Optimizer.\n            minimum_momentum: Optional. Sets the minimum momentum at the end of\n                the half-cycle. Can only be used with SGD Optimizer.\n            verbose: Bool. Whether to print the current learning rate after every\n                epoch.\n        # Reference\n            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n        """\n        super(OneCycleLR, self).__init__()\n\n        end_percentage = config.get('end_percentage')\n        scale_percentage = config.get('scale_percentage')\n        maximum_momentum = config.get('maximum_momentum')\n        minimum_momentum = config.get('minimum_momentum')\n        max_lr = config.get('max_lr')\n        verbose = config.get('verbose')\n\n        if end_percentage < 0. or end_percentage > 1.:\n            raise ValueError("`end_percentage` must be between 0 and 1")\n\n        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n            raise ValueError("`scale_percentage` must be between 0 and 1")\n\n        self.num_samples = num_samples\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.num_samples_per_batch = max(num_samples // batch_size, 3)\n        self.initial_lr = max_lr\n        self.end_percentage = end_percentage\n        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n        self.max_momentum = maximum_momentum\n        self.min_momentum = minimum_momentum\n        self.verbose = verbose\n\n        self.num_iterations = self.num_epochs * self.num_samples_per_batch\n        self.mid_cycle_id = int(self.num_iterations * ((1. - end_percentage)) / float(2))\n\n        if self.max_momentum is not None and self.min_momentum is not None:\n            self._update_momentum = True\n        else:\n            self._update_momentum = False\n\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def _reset(self):\n        """\n        Reset the callback.\n        """\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def compute_lr(self):\n        """\n        Compute the learning rate based on which phase of the cycle it is in.\n        - If in the first half of training, the learning rate gradually increases.\n        - If in the second half of training, the learning rate gradually decreases.\n        - If in the final `end_percentage` portion of training, the learning rate\n            is quickly reduced to near 100th of the original min learning rate.\n        # Returns:\n            the new learning rate\n        """\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n            new_lr = self.initial_lr * (1. + (current_percentage * (1. - 100.) / 100.)) * self.scale\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - (self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage * (self.scale * 100 - 1.)) * self.scale\n\n        else:\n            current_percentage = self.clr_iterations / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage * (self.scale * 100 - 1.)) * self.scale\n\n        if self.clr_iterations == self.num_iterations:\n            self.clr_iterations = 0\n\n        return new_lr\n\n    def compute_momentum(self):\n        """\n         Compute the momentum based on which phase of the cycle it is in.\n        - If in the first half of training, the momentum gradually decreases.\n        - If in the second half of training, the momentum gradually increases.\n        - If in the final `end_percentage` portion of training, the momentum value\n            is kept constant at the maximum initial value.\n        # Returns:\n            the new momentum value\n        """\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            new_momentum = self.max_momentum\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(self.mid_cycle_id))\n            new_momentum = self.max_momentum - current_percentage * (self.max_momentum - self.min_momentum)\n\n        else:\n            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n            new_momentum = self.max_momentum - current_percentage * (self.max_momentum - self.min_momentum)\n\n        return new_momentum\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        self._reset()\n        K.set_value(self.model.optimizer.lr, self.compute_lr())\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError("Momentum can be updated only on SGD optimizer !")\n\n            new_momentum = self.compute_momentum()\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n    def on_batch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        self.clr_iterations += 1\n        new_lr = self.compute_lr()\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        K.set_value(self.model.optimizer.lr, new_lr)\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError("Momentum can be updated only on SGD optimizer !")\n\n            new_momentum = self.compute_momentum()\n\n            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.verbose:\n            if self._update_momentum:\n                print(" - lr: %0.5f - momentum: %0.2f " % (self.history['lr'][-1],\n                                                           self.history['momentum'][-1]))\n\n            else:\n                print(" - lr: %0.5f " % (self.history['lr'][-1]))\n''')
    import logging
    import matplotlib
    from pprint import pprint
    
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import random
    import tensorflow as tf
    import spacy
    
    from sklearn import metrics
    from sklearn.metrics import precision_recall_curve
    from sklearn.model_selection import StratifiedKFold
    
    from src.Data import Data, CorpusInfo
    from src.data_mappers import TextMapper
    from src.Embedding import Embedding
    from src.Models import *  # Make all models available for easy script generation.
    from src.config import random_state as SEED, config_main as config
    
    np.random.seed(SEED)
    tf.set_random_seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    random.seed(SEED)
    
    
    def find_best_threshold(y_proba, y_true, plot=False):
        logging.info("Finding best threshold...")
        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
        thresholds = np.append(thresholds, 1.001)
        F = 2.0 / (1.0/precision + 1.0/recall)
        best_score = np.max(F)
        best_th = thresholds[np.argmax(F)]
        logging.info("Best score = {}. Best threshold = {}".format(best_score, best_th))
        if plot:
            plt.plot(thresholds, F, '-b')
            plt.plot([best_th], [best_score], '*r')
            plt.savefig('threshold.png')
            plt.close()
        return best_th
    
    
    def write_predictions(data, preds, thresh=0.5):
        logging.info("Writing predictions ...")
        preds = (preds > thresh).astype(int)
        out_df = pd.DataFrame({"qid": data.test_df["qid"].values})
        out_df['prediction'] = preds
        out_df.to_csv("submission.csv", index=False)
    
    
    def print_diagnostics(y_true, y_pred, file_suffix='', persist=True):
        try:
            cfn_matrix = metrics.confusion_matrix(y_true, y_pred)
        except ValueError:
            logging.warning("Warning: mix of binary and continuous targets used. Searching for best threshold.")
            thresh = find_best_threshold(y_pred, y_true)
            logging.warning("Applying threshold {} to predictions.".format(thresh))
            y_pred = (y_pred > thresh).astype(int)
            cfn_matrix = metrics.confusion_matrix(y_true, y_pred)
        with open('diagnostics' + file_suffix + '.txt', 'w') if persist else None as f:
            print("Confusion Matrix", file=f)
            print(cfn_matrix, file=f)
            print("-"*40, file=f)
            print("F1 score: " + str(metrics.f1_score(y_true, y_pred)), file=f)
            print("MCC score: " + str(metrics.matthews_corrcoef(y_true, y_pred)), file=f)
            print("precision: " + str(metrics.precision_score(y_true, y_pred)), file=f)
            print("Recall: " + str(metrics.recall_score(y_true, y_pred)), file=f)
    
    
    def get_wrongest(X, y_true, y_pred, num_wrongest=5):
        logging.info("Finding the worst predictions...")
        df = pd.DataFrame({'qid': X['qid'],
                           'question_text': X['question_text'],
                           'y_true': y_true,
                           'y_pred': y_pred.reshape(len(y_pred))})
        df['prediction_error'] = df['y_true'] - df['y_pred']
        df = df.sort_values('prediction_error')
        return df[df['y_true'] == 0].head(num_wrongest), df[df['y_true'] == 1].tail(num_wrongest)
    
    
    def print_wrongest(X, y_true, y_pred, num_wrongest=100, print_them=False, persist=True, file_suffix=None):
        def print_row(row):
            print("Q:" + row['question_text'])
            print("qid: " + row['qid'])
            print("Target: " + str(row['y_true']))
            print("Prediction: " + str(row['y_pred']))
            print("-"*40)
    
        wrongest_fps, wrongest_fns = get_wrongest(X, y_true, y_pred, num_wrongest=num_wrongest)
        if print_them:
            print("Wrongest {} false positives:".format(num_wrongest))
            print("-" * 40)
            for i, row in wrongest_fps.iterrows():
                print_row(row)
            print()
            print("Wrongest {} false negatives:".format(num_wrongest))
            print("-" * 40)
            for i, row in wrongest_fns.iterrows():
                print_row(row)
        if persist:
            filename = 'wrongest'
            if file_suffix:
                filename += '_' + file_suffix
            wrongest_fps.to_csv(filename + '_fps.csv', index=False)
            wrongest_fns.to_csv(filename + '_fns.csv', index=False)
        return wrongest_fps, wrongest_fns
    
    
    def cross_validate(model_class, data, embeddings, n_splits=4, show_wrongest=True, model_config=None,):
        logging.info("Cross validating model {} using {} folds...".format(model_class.__name__, str(n_splits)))
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
        models = list()
        for i, (train, test) in enumerate(skf.split(data.train_X, data.train_y)):
            logging.info("Running Fold {} of {}".format(i + 1, n_splits))
            models.append(None)
            cv_name = model_class.__name__ + '_cv_' + str(i)
            models[-1] = model_class(data=data, name=cv_name)
            models[-1].blend_embeddings(embeddings)
            models[-1].define_model(model_config)
            models[-1].fit(train_indices=train, val_indices=test, curve_file_suffix=str(i))
            if data.custom_features:
                predict_X = [data.train_X[test], data.train_features[test]]
            else:
                predict_X = [data.train_X[test]]
            pred_y_val = models[-1].predict(predict_X)
            print_diagnostics(data.train_y[test], pred_y_val, file_suffix='_' + cv_name)
            if show_wrongest:
                print_wrongest(data.train_df.iloc[test],
                               data.train_y[test],
                               pred_y_val,
                               num_wrongest=20,
                               persist=True,
                               file_suffix=models[-1].name)
        return models
    
    
    def load_embeddings(word_vocab, embedding_files, keep_index=True):
        embeddings = list()
        for f in embedding_files:
            embeddings.append(Embedding(word_vocab=word_vocab))
            embeddings[-1].load(f)
            if not keep_index:
                embeddings[-1].cleanup_index()
        return embeddings
    
    
    def save_unknown_words(data, embeddings, max_words=None):
        vocab = data.get_train_vocab()
        nb_words = 0
        for v in vocab.items():
            nb_words += v[1]
        for emb in embeddings:
            unknown_words = emb.check_coverage(vocab)
            df_unknown_words = pd.DataFrame(unknown_words, columns=['word', 'count'])\
                .sort_values('count', ascending=False)
            df_unknown_words['frequency'] = df_unknown_words['count'] / nb_words
            df_unknown_words = df_unknown_words.head(max_words)
            df_unknown_words.to_csv('unknown_words_' + emb.name + '.csv', index=False)
    
    
    def cleanup_models(models):
        for m in models:
            m.cleanup()
    
    def save_configs():
        from src.config import random_state, \
            config_data, \
            config_insincere_model, \
            config_lrfinder, \
            config_one_cycle, \
            config_main
        config_dict = locals().copy()
        with open('configs.txt', 'wt') as f:
            pprint(config_dict, stream=f)
        logging.info('Configurations: ')
        logging.info(pprint(config_dict))
    
    
    def main():
        embedding_files = config.get('embedding_files')
        dev_size = config.get('dev_size')
        data = Data()
        data.load(dev_size)
        data.split()
    
        nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])
        corpus_info = CorpusInfo(data.get_questions(subset='train'), nlp)
        word_counts = corpus_info.word_counts
        char_counts = corpus_info.char_counts
    
        text_mapper = TextMapper(word_counts=word_counts, char_counts=char_counts, word_threshold=10, max_word_len=12,
                                 char_threshold=350, max_sent_len=50, nlp=nlp, word_lowercase=True, char_lowercase=True)
    
        word_vocab = text_mapper.get_words_vocab()
        embeddings = load_embeddings(word_vocab, embedding_files)
        # save_unknown_words(data, embeddings, max_words=200)
        # models_all = list()
        # for model in config.get('models'):
        #     model_class = globals()[model.get('class')]
        #     models_all.extend(cross_validate(model_class,
        #                                      data,
        #                                      embeddings,
        #                                      model_config=model.get('args')))
    
        model = BiLSTMCharCNNModel(data=data, corpus_info=corpus_info, text_mapper=text_mapper, batch_size=32)
        model.blend_embeddings(embeddings)
        model.define_model()
        model.fit()
    
        # cleanup_models([model])  # embedding/memory cleanup
    
        val_preds = model.predict_subset(subset='val')
    
        # ensemble_cv = Ensemble(m odels_all)
        # train_X = [data.train_X]
        # val_X = [data.val_X]
        # test_X = [data.test_X]
        # if data.custom_features:
        #     train_X += [data.train_features]
        #     val_X += [data.val_features]
        #     test_X += [data.test_features]
    
        # find the best threshold
    
        # pred_train_y = ensemble_cv.predict_linear_regression(train_X, data.train_y, train_X)
        val_y = np.array(data.val_labels)
        thresh = find_best_threshold(val_preds, val_y)
    
        # pred_val_y = ensemble_cv.predict_linear_regression(train_X, data.train_y, val_X)
        print_diagnostics(val_y, (val_preds > thresh).astype(int))
        # pred_y_test = ensemble_cv.predict_linear_regression(train_X, data.train_y, test_X)
        pred_y_test = model.predict_subset('test')
        write_predictions(data, pred_y_test, thresh)
    
    
    if __name__ == "__main__":
        logging.getLogger()
        logging.basicConfig(
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.DEBUG,
            datefmt='%Y-%m-%d %H:%M:%S')
        save_configs()
        config_tf = tf.ConfigProto()
        config_tf.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU
        config_tf.log_device_placement = True  # to log device placement (on which device the operation ran)
        sess = tf.Session(config=config_tf)
        main()
        sess.close()
        logging.info("Done!")
    