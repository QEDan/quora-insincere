#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path, errno

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")
                    
        make_package(os.path.dirname(path))
        
        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('''src/UnknownWords.py''', '''import logging\nimport numpy as np\nfrom keras import Input, Model\nfrom keras.layers import Embedding as EmbeddingLayer, Bidirectional, CuDNNLSTM, Dense, LSTM\nfrom sklearn.model_selection import train_test_split\n\n\nclass UnknownWords:\n    def __init__(self, char_mapper, embedding, word_vocab, max_word_len, text_mapper,\n                 loss='mean_squared_error'):\n        self.char_mapper = char_mapper\n        self.max_word_len = max_word_len\n        self.embedding = embedding\n        self.word_vocab = word_vocab\n        self.loss = loss\n        self.text_mapper = text_mapper\n        self.model = None\n\n    def word_to_x(self, word):\n        """ Handles mapping one word into model inputs """\n        chars_x = np.zeros(self.max_word_len)\n\n        for char_ind, char in enumerate(word[:self.max_word_len]):\n            chars_x[char_ind] = self.char_mapper.get_symbol_index(char)\n\n        return chars_x\n\n    def define_model(self):\n        max_word_len = self.max_word_len\n        char_vocab_size = self.char_mapper.get_vocab_len()\n        chars_input = Input(shape=(max_word_len,), name='chars_input', dtype='int32')\n        m = EmbeddingLayer(input_dim=char_vocab_size,\n                           output_dim=32,\n                           input_length=max_word_len,\n                           mask_zero=True)(chars_input)\n        m = Bidirectional(LSTM(64))(m)\n        m = Dense(self.embedding.embedding_matrix.shape[1])(m)\n        self.model = Model(inputs=chars_input, outputs=m)\n        self.model.compile(loss=self.loss, optimizer='adam', metrics=['accuracy'])\n        return self.model\n\n    def training_data(self):\n        known_words = self.embedding.known_words\n        known_words_x = np.array([self.word_to_x(word) for word in known_words])\n        known_words_y = np.array([self.embedding.embeddings_index.get(word) for word in known_words])\n        train_X, val_X, train_y, val_y = train_test_split(known_words_x, known_words_y, test_size=0.02)\n        return train_X, val_X, train_y, val_y\n\n    def fit(self):\n        logging.info('Fitting UnknownWords model...')\n        train_X, val_X, train_y, val_y = self.training_data()\n        self.model.fit(x=train_X, y=train_y, epochs=4, batch_size=16, verbose=2, validation_data=(val_X, val_y))\n\n    def predict(self, words):\n        input_x = np.array([self.word_to_x(word) for word in words])\n        predictions = self.model.predict(input_x)\n        return predictions\n\n    def improve_embedding(self):\n        logging.info('Improving unknown embeddings with predicted values...')\n        unknown_words = self.embedding.unknown_words\n        for word in unknown_words:\n            matrix_ind = self.text_mapper.get_word_ind(word)\n            pred_embedding = self.predict(np.array([word]))\n            self.embedding.embedding_matrix[matrix_ind] = pred_embedding[0]\n''')
    __stickytape_write_module('''src/__init__.py''', '''''')
    __stickytape_write_module('''src/data_generator.py''', '''import keras\n\nimport numpy as np\n\n\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, text, text_mapper, labels=None, batch_size=128, shuffle=True):\n        self.batch_size = batch_size\n        self.text = text\n        self.labels = labels\n        self.shuffle = shuffle\n        self.text_mapper = text_mapper\n\n        self.indexes = np.arange(len(self.text))\n        self.on_epoch_end()\n\n    def __len__(self):\n        """Denotes the number of batches per epoch"""\n        return int(np.ceil(len(self.text) / self.batch_size))\n\n    def shuffle_data(self):\n        np.random.shuffle(self.indexes)\n\n    def __getitem__(self, index):\n        """Generate one batch of data\n\n        :return X: dictionary with keys: values\n            words_input: np.array with shape (batch_size, max_sent_len)\n            chars_input: np.array with shape (batch_size, max_sent_len, max_word_len)\n                y: np.array with shape (batch_size, )\n        """\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Generate data\n        if self.labels is not None:\n            X, y = self.__data_generation(indexes, return_y=True)\n            return X, [y, y, y]\n        else:\n            X = self.__data_generation(indexes, return_y=False)\n            return X\n\n    def on_epoch_end(self):\n        """ Updates indexes after each epoch """\n        # todo: if loss plateaus, increase batch size\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes, return_y=True):\n        """ Generates data containing batch_size samples """\n        # text samples\n        text_samples = [self.text[i] for i in indexes]\n        X = self.text_mapper.texts_to_x(text_samples)\n\n        if return_y:\n            labels = [self.labels[i] for i in indexes]\n            y = np.array(labels)\n            return X, y\n        else:\n            return X\n''')
    __stickytape_write_module('''src/Data.py''', '''from collections import defaultdict\n\nimport logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom src.config import config_data\nfrom src.text_cleaning import clean_apos, clean_specials, clean_spelling, \\\n    clean_acronyms, clean_non_dictionary, clean_numbers\n\n\nclass Data:\n    """ Loads and preprocesses data """\n    def __init__(self, train_path="../input/train.csv", test_path="../input/test.csv",\n                 text_col='question_text', id_col='qid', label_col='target'):\n\n        self.text_col = text_col\n        self.id_col = id_col\n        self.label_col = label_col\n        self.train_path = train_path\n        self.test_path = test_path\n\n        self.train_df = None\n        self.test_df = None\n\n    def load(self, dev_size=None):\n        logging.info("Loading data...")\n        if dev_size is not None:\n            logging.warning("Using dev set of size=" + str(dev_size))\n        self.train_df = pd.read_csv(self.train_path, nrows=dev_size)\n        self.test_df = pd.read_csv(self.test_path, nrows=dev_size)\n        logging.info("Train shape : {}".format(self.train_df.shape))\n        logging.info("Test shape : {}".format(self.test_df.shape))\n\n    def split(self):\n        self.train_qs, self.val_qs, self.train_labels, self.val_labels = self.get_training_split()\n\n    def perform_preprocessing(self):\n        self.train_df['question_text'] = self.preprocessing(self.train_df['question_text'])\n        self.test_df['question_text'] = self.preprocessing(self.test_df['question_text'])\n\n    @staticmethod\n    def preprocessing(questions):\n\n        questions = questions.fillna("_na_")\n        # preprocess_config = config_data.get('preprocess')\n        # case_sensitive = not preprocess_config.get('lower_case')\n        # trouble removing stop words before we have tokenized the text, this has to happen later\n        # if preprocess_config.get('remove_stop_words'):\n            # questions = questions.apply(remove_stops)\n        # if preprocess_config.get('remove_specials'):\n        #     questions = questions.apply(lambda x: clean_specials(x))\n        # if preprocess_config.get('correct_spelling'):\n        #     questions = questions.apply(lambda x: clean_spelling(x, case_sensitive=case_sensitive))\n        # if preprocess_config.get('replace_acronyms'):\n        #     questions = questions.apply(lambda x: clean_acronyms(x, case_sensitive=case_sensitive))\n        # if preprocess_config.get('replace_non_words'):\n        #     questions = questions.apply(lambda x: clean_non_dictionary(x, case_sensitive=case_sensitive))\n        # if preprocess_config.get('replace_numbers'):\n        #     questions = questions.apply(lambda x: clean_numbers(x))\n        questions = questions.apply(lambda x: clean_specials(x))\n        return questions\n\n    def get_questions(self, subset='train'):\n        # todo: add functionality to only get data with a certain label (if we want to fine tune word embeddings...)\n        if subset == 'train':\n            data = list(self.train_df[self.text_col])\n        if subset == 'test':\n            data = list(self.test_df[self.text_col])\n        if subset == 'all':\n            data = list(self.train_df[self.text_col]) + list(self.test_df[self.text_col])\n        return data\n\n    def get_training_labels(self):\n        labels = self.train_df.loc[:, self.label_col].values\n        return labels\n\n    def get_training_split(self, test_size=0.1, seed=0):\n        train_qs, val_qs, train_labels, val_labels = train_test_split(self.train_df[self.text_col].tolist(),\n                                                                      self.train_df[self.label_col].tolist(),\n                                                                      stratify=self.train_df[self.label_col].tolist(),\n                                                                      test_size=test_size,\n                                                                      random_state=seed)\n        return train_qs, val_qs, train_labels, val_labels\n\n\nclass CorpusInfo:\n    """ Calculates corpus information to be referenced during feature engineering later """\n    # todo: pass in a general tokenizer (so that it matches the tokenizer in the rest of the pipeline)\n    # todo: how can we make this run faster? can be parallelized...?\n    def __init__(self, questions, nlp, word_lowercase=True, char_lowercase=True):\n        self.nlp = nlp\n        self.word_lowercase = word_lowercase\n        self.char_lowercase = char_lowercase\n\n        self.word_counts = []\n        self.char_counts = []\n        self.sent_lengths = []\n        self.word_lengths = []\n\n        self.calc_corpus_info(questions)\n\n    def calc_corpus_info(self, questions):\n        word_counters = defaultdict(int)\n        char_counters = defaultdict(int)\n\n        for question in questions:\n            tokenized_question = self.nlp(question)\n            self.sent_lengths.append(len(tokenized_question))\n            for token in tokenized_question:\n                text = token.text\n                self.word_lengths.append(len(text))\n                word_to_count = text.lower() if self.word_lowercase else text\n                word_counters[word_to_count] += 1\n                for char in text:\n                    char_to_count = char.lower() if self.char_lowercase else char\n                    char_counters[char_to_count] += 1\n\n        self.word_counts = sorted(word_counters.items(), key=lambda x: x[1], reverse=True)\n        self.char_counts = sorted(char_counters.items(), key=lambda x: x[1], reverse=True)\n\n    def plot_word_lengths(self, max_len):\n        plt.hist(self.word_lengths, bins=np.arange(0, max_len, 2), cumulative=True, normed=1)\n        plt.hlines(0.975, 0, 30, colors='red')\n\n    def plot_sent_lengths(self, max_len):\n        plt.hist(self.sent_lengths, bins=np.arange(0, max_len, 2), cumulative=True, normed=1)\n        plt.hlines(0.975, 0, 30, colors='red')\n\n''')
    __stickytape_write_module('''src/config.py''', '''"""Model and hyperparameter configurations. This file is generated by generate_config(). Do not edit!"""\n\n\nrandom_state = 2018\n\nconfig_data = {'max_feature': 120000,\n 'max_seq_len': 75,\n 'preprocess': {'correct_spelling': True,\n                'lower_case': False,\n                'remove_contractions': True,\n                'remove_specials': True,\n                'remove_stop_words': False,\n                'replace_acronyms': True,\n                'replace_non_words': True,\n                'replace_numbers': False,\n                'use_custom_features': True},\n 'test_size': 0.1}\n\nconfig_insincere_model = {'callbacks': {'checkpoint': {'mode': 'max',\n                              'monitor': 'val_f1_score',\n                              'save_best_only': True,\n                              'verbose': True},\n               'early_stopping': {'mode': 'max',\n                                  'monitor': 'val_f1_score',\n                                  'patience': 2,\n                                  'verbose': True}},\n 'fit': {'batch_size': 1000,\n         'epochs': 15,\n         'pseudo_labels': False,\n         'save_curve': True},\n 'predict': {'batch_size': 1024, 'verbose': True}}\n\nconfig_lrfinder = {'loss_smoothing_beta': 0.98,\n 'lr_scale': 'exp',\n 'maximum_lr': 10.0,\n 'minimum_lr': 0.0001,\n 'save_dir': None,\n 'stopping_criterion_factor': 4,\n 'validation_sample_rate': 5,\n 'verbose': True}\n\nconfig_one_cycle = {'end_percentage': 0.1,\n 'max_lr': 0.1,\n 'maximum_momentum': 0.95,\n 'minimum_momentum': 0.85,\n 'scale_percentage': None,\n 'verbose': True}\n\nconfig_main = {'dev_size': None,\n 'embedding_files': ['../input/embeddings/glove.840B.300d/glove.840B.300d.txt',\n                     '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt']}\n\n''')
    __stickytape_write_module('''src/text_cleaning.py''', '''import re\nfrom nltk.corpus import stopwords\nimport string\n\ndef remove_stops(sentence):\n    """ Should apply only after tokenization """\n    stop = set(stopwords.words('english'))\n    filtered = list()\n    for w in sentence.split(" "):\n        if w not in stop:\n            filtered.append(w)\n    return " ".join(filtered)\n\n\ndef clean_apos(text):\n    apos = ["\u2019", "\u2018", "\xb4", "`"]\n    for s in apos:\n        text = text.replace(s, "'")\n    quotes = ['"', '"', '\u201c', '\u201d', '\u2019']\n    for q in quotes:\n        text = text.replace(q, '"')\n    return text\n\n\ndef clean_specials(text):\n    # punct = "/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~" + '""\u201c\u201d\u2019' + '\u221e\u03b8\xf7\u03b1\u2022\xe0\u2212\u03b2\u2205\xb3\u03c0\u2018\u20b9\xb4\xb0\xa3\u20ac\\\xd7\u2122\u221a\xb2\u2014\u2013&'\n    # puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']', '>', '%', '=',\n    #           '#', '*', '+', '\\\\', '\u2022', '~', '@', '\xa3',\n    #           '\xb7', '_', '{', '}', '\xa9', '^', '\xae', '`', '<', '\u2192', '\xb0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\xd7', '\xa7', '\u2033', '\u2032',\n    #           '\xc2', '\u2588', '\xbd', '\xe0', '\u2026',\n    #           '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\xe2', '\u25ba', '\u2212', '\xa2', '\xb2', '\xac', '\u2591', '\xb6', '\u2191', '\xb1', '\xbf', '\u25be', '\u2550', '\xa6', '\u2551',\n    #           '\u2015', '\xa5', '\u2593', '\u2014', '\u2039', '\u2500',\n    #           '\u2592', '\uff1a', '\xbc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\xa8', '\u2584', '\u266b', '\u2606', '\xe9', '\xaf', '\u2666', '\xa4', '\u25b2', '\xe8',\n    #           '\xb8', '\xbe', '\xc3', '\u22c5', '\u2018', '\u221e',\n    #           '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\xbb', '\uff0c', '\u266a', '\u2569', '\u255a', '\xb3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\xef',\n    #           '\xd8', '\xb9', '\u2264', '\u2021', '\u221a']\n    punct_mapping = {"\u2018": "'", "\xb4": "'", "\u2032": "'", '\u2033': '"', '\xa8': '"', '\u02bb': "'", '\u060c': ',',\n                     "\u2014": "-", "\u2013": "-", '\u2212': '-', "\u2019": "'", "`": "'", '\u201c': '"', '\u201d': '"',\n                     '\uff1f': '?',\n                     '\u221e': ' infinity ', '\u03b8': ' theta ', '\xf7': ' divide ', '\u2103': ' temperature ',\n                     '\u016b': 'u', '\xfa': 'u',\n                     '\xe1': 'a', '\xe3': 'a', '\xe0': 'a', '\u0430': 'a', '\u03b1': 'a', '\xe2': 'a', '\xe5': 'a', '\u0101': 'a', '\u1ea1': 'a', '\xe4': 'a',\n                     '\u0103': 'a',\n                     '\u044c': 'b', '\xdf': 'b', '\u0432': 'b', '\u03b2': 'b',\n                     '\u03c2': 'c', '\xe7': 'c', '\u0107': 'c', '\u0441': 'c',\n                     '\xeb': 'e', '\u0435': 'e', '\xe9': 'e', '\xea': 'e', '\xe8': 'e',\n                     '\u011f': 'g',\n                     '\u043d': 'h', '\u1e25': 'h',\n                     '\xed': 'i', '\xee': 'i', '\xec': 'i', 'i\u0307': 'i', '\xa1': 'i',\n                     '\u03b7': 'n',\n                     '\u043e': 'o', '\xf4': 'o', '\xf3': 'o', '\u03bf': 'o', '\u014d': 'o',\n                     '\u03c1': 'p',\n                     '\u0219': 's', '\u0455': 's', '\u015f': 's',\n                     '\u0442': 't',\n                     '\xfc': 'u', '\xfb': 'u',\n                     '\u03c5': 'v', '\u03bd': 'v',\n                     '\u03c9': 'w',\n                     '\xd7': 'x',\n                     '\xfd': 'y', '\u0443': 'y',\n                     '\u017e': 'z',\n                     '\xbf': '?',\n                     '\xbd': ' 0.5 ', '\xbc': ' 0.25 ', '\u03c0': ' pi ', '\u5350': ' nazi symbol '}\n    for p in punct_mapping.keys():\n        text = text.replace(p, punct_mapping[p])\n    # for p in set(list(punct) + puncts) - set(punct_mapping.keys()):\n    #     text = text.replace(p, f' {p} ')\n\n    # specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '',\n    #             '\u0939\u0948': ''}\n    # for s in specials:\n    #     text = text.replace(s, specials[s])\n    return text\n\n\ndef clean_spelling(text, case_sensitive=False):\n    misspell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n                     'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                     'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n                     'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                     'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n                     'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n                     'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n                     'mastrubate': 'masturbate', "mastrubating": 'masturbating', 'pennis': 'penis',\n                     'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n                     '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n                     "whst": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                     'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n    for word in misspell_dict.keys():\n        if case_sensitive:\n            text = text.replace(word, misspell_dict[word])\n        else:\n            re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n            text = re_insensitive.sub(misspell_dict[word], text)\n    return text\n\n\ndef clean_acronyms(text, case_sensitive=False):\n    acronym_dict = {'upsc': 'union public service commission',\n                    'aiims': 'all india institute of medical sciences',\n                    'cgl': 'graduate level examination',\n                    'icse': 'indian school certificate exam',\n                    'iiit': 'indian institute of information technology',\n                    'cgpa': 'cumulative grade point average',\n                    'ielts': 'international english language training system',\n                    'ncert': 'national council of education research training',\n                    'isro': 'indian space research organization',\n                    'clat': 'common law admission test',\n                    'ibps': 'institute of banking personnel selection',\n                    'iiser': 'indian institute of science education and research',\n                    'iisc': 'indian institute of science',\n                    'iims': 'indian institutes of management',\n                    'cpec': 'china pakistan economic corridor'\n\n                    }\n    for word in acronym_dict.keys():\n        if case_sensitive:\n            text = text.replace(word, acronym_dict[word])\n        else:\n            re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n            text = re_insensitive.sub(acronym_dict[word], text)\n    return text\n\n\ndef clean_non_dictionary(text, case_sensitive=False):\n    replace_dict = {'quorans': 'users',\n                    'quoran': 'user',\n                    'jio': 'phone manufacturer',\n                    'manipal': 'city',\n                    'bitsat': 'exam',\n                    'mtech': 'technical university',\n                    'pilani': 'town',\n                    'bhu': 'university',\n                    'h1b': 'visa',\n                    'redmi': 'phone manufacturer',\n                    'nift': 'university',\n                    'kvpy': 'exam',\n                    'thanos': 'comic villain',\n                    'paytm': 'payment system',\n                    'comedk': 'medical consortium',\n                    'accenture': 'management consulting company',\n                    'llb': 'bachelor of laws',\n                    'ignou': 'university',\n                    'dtu': 'university',\n                    'aadhar': 'social number',\n                    'lenovo': 'computer manufacturer',\n                    'gmat': 'exam',\n                    'kiit': 'institute of technology',\n                    'shopify': 'music streaming',\n                    'fitjee': 'exam',\n                    'kejriwal': 'politician',\n                    'wbjee': 'exam',\n                    'pgdm': 'master of business administration',\n                    'trudeau': 'politician',\n                    'nri': 'research institute',\n                    'deloitte': 'accounting company',\n                    'jinping': 'politician',\n                    'bcom': 'bachelor of commerce',\n                    'mcom': 'masters of commerce',\n                    'virat': 'athlete',\n                    'kcet': 'television network',\n                    'wipro': 'information technology company',\n                    'articleship': 'internship',\n                    'comey': 'law enforcement director',\n                    'jnu': 'university',\n                    'acca': 'chartered accountants',\n                    'aakash': 'phone manufacturer',\n                    'brexit': 'british succession',\n                    'crypto': 'digital currency',\n                    'cryptocurrency': 'digital currency',\n                    'cryptocurrencies': 'digital currencies',\n                    'etherium': 'digital currency',\n                    'bitcoin': 'digital currency',\n                    'viteee': 'exam',\n                    'iocl': 'indian oil company',\n                    'nmims': 'management school',\n                    'rohingya': 'myanmar people',\n                    'fortnite': 'videogame',\n                    'upes': 'university',\n                    'nsit': 'university',\n                    'coinbase': 'digital currency exchange'\n                    }\n    for word in replace_dict.keys():\n        if case_sensitive:\n            text = text.replace(word, replace_dict[word])\n        else:\n            re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n            text = re_insensitive.sub(replace_dict[word], text)\n    return text\n\n\ndef clean_numbers(text, min_magnitude=2, max_magnitude=10):\n    for n in range(min_magnitude, max_magnitude):\n        text = re.sub('[0-9]{' + str(n) + '}', '#' * n, text)\n    return text\n''')
    __stickytape_write_module('''src/data_mappers.py''', '''import numpy as np\nimport string\nfrom keras.preprocessing.text import text_to_word_sequence\nimport math\n\n\nclass TextMapper:\n    """ Maps text into model input x """\n\n    def __init__(self, word_counts, char_counts, nlp, max_sent_len=100, word_threshold=10, word_lowercase=False,\n                 max_word_len=12, char_threshold=5, char_lowercase=False, letters_only=False):\n        """\n        :param word_counts: list of tuples of (word, count)\n        :param nlp: from pre-loaded spaCy\n        :param max_sent_len: maximum words in a doc\n        :param word_threshold: number of times a word must appear in word_counts for it to get a representation\n        :param word_lowercase: boolean, whether words should be lowercased\n        """\n        self.max_sent_len = max_sent_len\n        self.max_word_len = max_word_len\n        self.word_mapper = WordMapper(word_counts=word_counts, threshold=word_threshold,\n                                      word_lowercase=word_lowercase)\n        self.char_mapper = CharMapper(char_counts=char_counts, threshold=char_threshold,\n                                      char_lowercase=char_lowercase, letters_only=False)\n\n        self.max_sent_len = max_sent_len\n        self.max_word_len = max_word_len\n        self.nlp = nlp\n        self.num_sent_feats = 4\n\n    def text_to_x(self, text):\n        """ Handles mapping one text doc into model inputs """\n        words_x = np.zeros(self.max_sent_len)\n        words_feats_x = np.zeros((self.max_sent_len, self.word_mapper.num_add_feats))\n        chars_x = np.zeros((self.max_sent_len, self.max_word_len))\n        chars_feats_x = np.zeros((self.max_sent_len, self.max_word_len, self.char_mapper.num_add_feats))\n        sent_feats_x = np.zeros(self.num_sent_feats)\n\n        tokenized_question = self.nlp(text)\n\n        word_lens = []\n        sent_len = 0\n        unknown_chars = 0\n        unknown_words = 0\n        total_chars = 0\n\n        for word_ind, token in enumerate(tokenized_question[:self.max_sent_len]):\n            sent_len += 1\n            word = token.text\n            word_lens.append(len(word))\n            word_mapping = self.word_mapper.get_symbol_index(word)\n            if word_mapping == 1:\n                unknown_words += 1\n            words_x[word_ind] = word_mapping\n            words_feats_x[word_ind] = self.word_mapper.get_add_features(word)\n            for char_ind, char in enumerate(word[:self.max_word_len]):\n                total_chars += 1\n                char_mapping = self.char_mapper.get_symbol_index(char)\n                chars_x[word_ind][char_ind] = char_mapping\n                if char_mapping == 1:\n                    unknown_chars += 1\n                chars_feats_x[word_ind][char_ind] = self.char_mapper.get_add_features(char)\n\n        log_sent_length = math.log(sent_len)\n        sent_feats_x[0] = log_sent_length\n\n        num_caps = sum(1 for c in text if c.isupper())\n        proportion_caps = num_caps/total_chars\n        sent_feats_x[1] = proportion_caps\n\n        # avg_word_length = np.mean(word_lens)\n        # sent_feats_x[2] = avg_word_length\n        #\n        # std_word_length = np.std(word_lens)\n        # sent_feats_x[3] = std_word_length\n\n        proportion_unknown_chars = unknown_chars/total_chars\n        sent_feats_x[2] = proportion_unknown_chars\n\n        proportion_unknown_words = unknown_words/sent_len\n        sent_feats_x[3] = proportion_unknown_words\n\n        # if len(tokenized_question) > sent_len:\n        #     sent_feats_x[6] = 1\n\n        return words_x, words_feats_x, chars_x, chars_feats_x, sent_feats_x\n\n    def x_to_words(self, words_x, remove_padding=True):\n        words = [self.word_mapper.ix_to_symbol[int(i)] for i in words_x]\n        comment_text = " ".join(words)\n\n        # remove padding\n        if remove_padding:\n            comment_text = comment_text.split(self.word_mapper.PADDING_SYMBOL)[0]\n\n        return comment_text\n\n    def x_to_chars(self, chars_x, remove_padding=True):\n        chars = []\n        for word_x in chars_x:\n            word_chars = [self.char_mapper.ix_to_symbol[x] for x in word_x]\n            word = "".join(word_chars)\n            # remove_padding\n            if remove_padding:\n                word = word.split(self.char_mapper.PADDING_SYMBOL)[0]\n\n            # don't add empty words\n            if word:\n                chars.append(word)\n\n        question_chars = " ".join(chars)\n\n        return question_chars\n\n    def texts_to_x(self, texts):\n        inputs_x = [self.text_to_x(text) for text in texts]\n        words_input, words_feats_input, chars_input, chars_feats_input, sent_feats_input = map(np.array, zip(*inputs_x))\n        return {"words_input": words_input, "words_feats_input": words_feats_input,\n                "chars_input": chars_input, "chars_feats_input": chars_feats_input,\n                "sent_feats_input": sent_feats_input}\n\n    def set_max_sentence_len(self, max_sent_len):\n        self.max_sent_len = max_sent_len\n\n    def set_max_char_len(self, max_word_len):\n        self.max_word_len = max_word_len\n\n    def get_words_vocab(self):\n        return self.word_mapper.vocab\n\n\nclass SymbolMapper:\n    """ Handles mapping of any symbol (words or characters) into something an model an ingest """\n    PADDING_SYMBOL = "<PAD>"\n    UNKNOWN_SYMBOL = "<UNK>"\n    BASE_ALPHABET = [PADDING_SYMBOL, UNKNOWN_SYMBOL]\n\n    def __init__(self, symbol_counts, threshold, lowercase):\n        self.symbol_counts = symbol_counts\n        self.threshold = threshold\n        self.lowercase = lowercase\n\n        self.vocab = []\n        self.symbol_to_ix = dict()\n        self.ix_to_symbol = dict()\n\n        symbol_counts = sorted(symbol_counts, key=lambda x: x[1], reverse=True)\n\n        # initialize alphabet\n        self.vocab = [symbol for symbol, count in symbol_counts if count >= self.threshold]\n        self.vocab = self.BASE_ALPHABET + self.vocab\n\n        self.init_mappings(False)\n\n    def init_mappings(self, check_coverage=True):\n\n        self.symbol_to_ix = {symbol: ix for ix, symbol in enumerate(self.vocab)}\n        self.ix_to_symbol = {ix: symbol for ix, symbol in enumerate(self.vocab)}\n\n        if check_coverage:\n            self.print_coverage_statistics()\n\n    def print_top_n_symbols(self, n=10):\n        print([(symbol, count) for symbol, count in self.symbol_counts if count >= self.threshold][:n])\n\n    def print_bot_n_symbols(self, n=10):\n        print([(symbol, count) for symbol, count in self.symbol_counts if count >= self.threshold][-n:])\n\n    def add_symbols_to_map(self, add_vocab):\n        missing_from_current_vocab = set(self.vocab) - set(add_vocab)\n        self.vocab = self.vocab + list(missing_from_current_vocab)\n        self.init_mappings()\n\n    def get_symbol_index(self, symbol):\n        if self.lowercase:\n            symbol = symbol.lower()\n        try:\n            num = self.symbol_to_ix[symbol]\n        except KeyError:\n            num = self.symbol_to_ix[self.UNKNOWN_SYMBOL]\n        return num\n\n    def get_vocab_len(self):\n        return len(self.symbol_to_ix)\n\n    def print_coverage_statistics(self, symbols_name='symbol'):\n        """\n        Simple metric on coverage of symbols\n\n        :param symbols_name: str, printed to distinguish different mappers\n        :param persist: bool, write stats to file rather than stdout\n        """\n        symbol_mappings = self.symbol_to_ix.keys()\n        print("Number of unique {}: {}".format(symbols_name, len(self.symbol_counts)))\n        print("Number of unique {} mapped: {}".format(symbols_name, len(symbol_mappings)))\n        total_tokens = 0\n        mapped_tokens = 0\n        for symbol, count in self.symbol_counts:\n            total_tokens += count\n            if symbol in symbol_mappings:\n                mapped_tokens += count\n        print("Percent of unique symbols mapped: {}%".format(\n            100 * len(symbol_mappings) / len(self.symbol_counts)))\n        print("Percent of total symbols mapped: {}%".format(\n            100 * mapped_tokens / total_tokens))\n\n\nclass WordMapper(SymbolMapper):\n\n    def __init__(self, word_counts, threshold, word_lowercase):\n        super().__init__(word_counts, threshold, word_lowercase)\n        self.num_add_feats = 1\n\n    def print_coverage_statistics(self, symbols_name='words'):\n        super().print_coverage_statistics(symbols_name)\n\n    def set_vocab(self, vocab_list):\n        self.vocab = vocab_list\n        self.init_mappings(True)\n\n    def get_add_features(self, word):\n        add_feats = np.zeros(self.num_add_feats)\n        add_feats[0] = math.log(len(word))\n        # if char in self.puncutation_list:\n        #     add_feats[2] = 1\n        return add_feats\n\n\nclass CharMapper(SymbolMapper):\n\n    def __init__(self, char_counts, threshold, char_lowercase, letters_only=False):\n        super().__init__(char_counts, threshold, char_lowercase)\n        self.num_add_feats = 2\n        if letters_only:\n            self.init_letter_mapping()\n\n    def init_letter_mapping(self):\n        self.vocab = self.BASE_ALPHABET + list('abcdefghijklmnopqrstuvwxyz')\n        self.symbol_to_ix = {symbol: ix for ix, symbol in enumerate(self.vocab)}\n        self.ix_to_symbol = {ix: symbol for ix, symbol in enumerate(self.vocab)}\n\n    def print_coverage_statistics(self, symbols_name='chars'):\n        super().print_coverage_statistics(symbols_name)\n\n    def get_add_features(self, char):\n        add_feats = np.zeros(self.num_add_feats)\n        if char.isupper():\n            add_feats[0] = 1\n        if char.isdigit():\n            add_feats[1] = 1\n        # if char in self.puncutation_list:\n        #     add_feats[2] = 1\n        return add_feats\n''')
    __stickytape_write_module('''src/Embedding.py''', '''import gc\nimport time\n\nimport logging\nimport numpy as np\nimport operator\nimport traceback\nfrom gensim.models import KeyedVectors\n\n\nclass Embedding:\n    def __init__(self, word_counts, word_threshold):\n        self.embeddings_index = None\n        self.nb_words = None\n        self.embed_size = None\n        self.embedding_matrix = None\n        self.word_counts = word_counts\n        self.word_threshold = word_threshold\n        self.word_map_list = list()\n        self.name = None\n        self.unknown_words = list()\n        self.known_words = list()\n\n    def load(self, embedding_file='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'):\n        logging.info("loading embedding : " + embedding_file)\n        self.name = embedding_file.split('/')[3]\n\n        def get_coefs(word, *arr):\n            return word, np.asarray(arr, dtype='float32')\n\n        if "wiki-news" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" "))\n                                         for i, o in enumerate(open(embedding_file)) if len(o) > 100)\n        elif "glove" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" ")) for i, o in enumerate(open(embedding_file)))\n        elif "paragram" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" ")) for i, o in\n                                         enumerate(open(embedding_file, encoding="utf8", errors='ignore'))\n                                         if len(o) > 100)\n        elif "GoogleNews" in embedding_file:\n            self.embeddings_index = {}\n            wv_from_bin = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n            for i, (word, vector) in enumerate(zip(wv_from_bin.vocab, wv_from_bin.vectors)):\n                coefs = np.asarray(vector, dtype='float32')\n                self.embeddings_index[word] = coefs\n        else:\n            raise ValueError("Unsupported embedding file: " + embedding_file)\n\n        try:\n            all_embs = np.stack(self.embeddings_index.values())\n\n        except ValueError as e:\n            logging.error(e)\n            tb = traceback.format_exc()\n            logging.error(tb)\n            logging.debug("len(self.embeddings_index.values()): "\n                          + str(len(self.embeddings_index.values())))\n            logging.debug("type(self.embeddings_index.values()[0]): "\n                          + str(type(list(self.embeddings_index.values())[0])))\n            logging.debug("first few self.embeddings_index.values(): "\n                          + str(list(self.embeddings_index.values())[:5]))\n            raise\n\n        emb_mean, emb_std = all_embs.mean(), all_embs.std()\n        self.embed_size = all_embs.shape[1]\n\n\n        # count padding and unknown as part of vocab so start from 2\n        self.word_map_list.append('<PAD>')\n        self.word_map_list.append('<UNK>')\n        for word, count in self.word_counts[2:]:\n            if self.embeddings_index.get(word) is not None or count >= self.word_threshold:\n                self.word_map_list.append(word)\n\n        self.nb_words = len(self.word_map_list)\n        self.embedding_matrix = np.random.normal(emb_mean, emb_std, (self.nb_words, self.embed_size))\n        self.embedding_matrix[0] = np.zeros(self.embed_size)\n\n        # this might not be a good idea\n        # self.embedding_matrix[1] = all_embs.mean(axis=0)\n        for ind, word in enumerate(self.word_map_list[2:], 2):\n            embedding_vector = self.embeddings_index.get(word)\n            if embedding_vector is not None:\n                self.embedding_matrix[ind] = embedding_vector\n                self.known_words.append(word)\n            else:\n                self.unknown_words.append(word)\n\n        self.cleanup_index()\n\n    def get_embedding_matrix(self):\n        return self.embedding_matrix\n\n    def check_coverage(self, vocab):\n        known_words = {}\n        unknown_words = {}\n        nb_known_words = 0\n        nb_unknown_words = 0\n        for word in vocab.keys():\n            try:\n                known_words[word] = self.embeddings_index[word]\n                nb_known_words += vocab[word]\n            except:\n                unknown_words[word] = vocab[word]\n                nb_unknown_words += vocab[word]\n                pass\n\n        logging.info('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n        logging.info('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n        unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n        return unknown_words\n\n    def cleanup(self):\n        logging.info("Releasing memory...")\n        try:\n            del self.embeddings_index, self.embedding_matrix\n            gc.collect()\n            time.sleep(10)\n        except AttributeError:\n            logging.warning('embeddings index not found. They were probably already cleaned up.')\n\n    def cleanup_index(self):\n        logging.info("Releasing memory...")\n        try:\n            del self.embeddings_index\n            gc.collect()\n            time.sleep(10)\n        except AttributeError:\n            logging.warning('embeddings index not found. They were probably already cleaned up.')\n''')
    __stickytape_write_module('''src/Models.py''', '''import matplotlib\nfrom keras import Input, Model\n\nfrom src.InsincereModel import InsincereModel\n\nmatplotlib.use('Agg')\n\nfrom src.Models import *  # Make all models available for easy script generation.\n\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Layer\nfrom keras.layers import TimeDistributed, Embedding as EmbeddingLayer, Bidirectional, CuDNNLSTM, Dense, Conv1D\nfrom keras.layers import GlobalMaxPooling1D, Concatenate, BatchNormalization, Dropout, SpatialDropout1D, CuDNNGRU\nfrom keras.layers import GlobalAveragePooling1D, Add, Activation, Average, Maximum, Multiply, Dot, Flatten\nfrom keras.initializers import glorot_normal\nfrom keras.initializers import orthogonal\nimport keras.backend as K\nimport numpy as np\nimport tensorflow as tf\n\n\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n\n# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n\n\ndef capsule(inputs, cps, word_emb):\n    word_rep = word_rep_with_char_info(inputs, cps, word_emb)\n    x = Bidirectional(CuDNNGRU(100, return_sequences=True,\n                               kernel_initializer=glorot_normal(seed=2019),\n                               recurrent_initializer=orthogonal(gain=1.0, seed=1337)))(word_rep)\n\n    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n    x = Flatten()(x)\n\n    x = Dense(100, activation="linear", kernel_initializer=glorot_normal(seed=2019))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.2)(x)\n\n    x = Dense(1, activation="sigmoid")(x)\n    return x\n\nclass BiLSTMCharCNNModel(InsincereModel):\n    """\n    notes:\n\n    dropout should never be used before any batchnorm, only after\n    batchnorm momentum should be decreased when using larger batch sizes\n\n    """\n\n\n    def define_model(self, model_config=None):\n        # todo: look up adjusting batch norm momentum (talked about in forums)\n\n        # corpus params\n        cps = {\n            'max_sent_len': self.text_mapper.max_sent_len,\n            'max_word_len': self.text_mapper.max_word_len,\n            'word_vocab_size': self.text_mapper.word_mapper.get_vocab_len(),\n            'char_vocab_size':self.text_mapper.char_mapper.get_vocab_len()\n        }\n\n        # model inputs\n        chars_input = Input(shape=(cps['max_sent_len'], cps['max_word_len']), name='chars_input', dtype='int64')\n        char_feats_input = Input(shape=(cps['max_sent_len'], cps['max_word_len'],\n                                        self.text_mapper.char_mapper.num_add_feats),\n                                 name='chars_feats_input', dtype='float32')\n        sent_feats_input = Input(shape=(self.text_mapper.num_sent_feats,), name="sent_feats_input", dtype='float32')\n        words_input = Input(shape=(cps['max_sent_len'],), name='words_input', dtype='int64')\n        words_feats_input = Input(shape=(cps['max_sent_len'], self.text_mapper.word_mapper.num_add_feats),\n                                  name='words_feats_input', dtype='float32')\n\n        model_inputs = {\n            "chars_input": chars_input,\n            "char_feats_input": char_feats_input,\n            "sent_feats_input": sent_feats_input,\n            "words_input": words_input,\n            "words_feats_input": words_feats_input,\n        }\n\n        # load this large matrix only once, and then reuse if when needed\n        word_emb = self.get_word_embedding_layer(model_inputs, cps)\n\n        lstm_pred, lstm_logits = lstm_model(model_inputs, cps, word_emb)\n        conv_pred, conv_logits = conv_model(model_inputs, cps, word_emb)\n\n        caps_pred = capsule(model_inputs, cps, word_emb)\n\n        # ensemble_weights = ensemble_weights_model(model_inputs, cps, word_emb, lstm_logits, conv_logits)\n        #\n        # ensemble_preds = Concatenate()([lstm_pred, conv_pred])\n        final_pred = Average()([caps_pred, conv_pred])\n\n        inputs = list(model_inputs.values())\n        preds = [caps_pred, conv_pred, final_pred]\n        # preds = [lstm_pred, conv_pred, final_pred]\n        self.model = Model(inputs=inputs, outputs=preds)\n        return self.model\n\n    def get_word_embedding_layer(self, inputs, cps):\n        # are we using pretrained weights?\n        if self.embedding is not None:\n            # todo: make this trainable at the end\n            matrix_shape = self.embedding.embedding_matrix.shape\n            untrainable_word_embedding = EmbeddingLayer(input_dim=cps['word_vocab_size'],\n                                                        output_dim=matrix_shape[1],\n                                                        input_length=cps['max_sent_len'],\n                                                        weights=[self.embedding.embedding_matrix],\n                                                        trainable=False,\n                                                        name='static_word_emb')(inputs['words_input'])\n            # You can make this next embedding trainable to apply a loss to changing these word representations\n            # consider making this trainable at the very end.\n\n            # regularized_word_embedding = EmbeddingLayer(input_dim=word_vocab_size,\n            #                                             output_dim=matrix_shape[1],\n            #                                             input_length=max_sent_len,\n            #                                             weights=[np.zeros(matrix_shape)],\n            #                                             todo: tune this regularization\n            #                                             todo: add function to turn training on/off (access by name)\n            #                                             embeddings_regularizer=regularizers.l2(0.1),\n            #                                             trainable=False,\n            #                                             name='reg_word_emb')(inputs['words_input'])\n\n            # uncomment out as appropriate\n            # word_emb = Add()([untrainable_word_embedding, regularized_word_embedding])\n            word_emb = untrainable_word_embedding\n\n        # do we want to train from scratch? (most likely, no), but for fast pipeline checks - don't need emb loading\n        else:\n            trainable_lstm_embedding = EmbeddingLayer(input_dim=cps['word_vocab_size'],\n                                                      output_dim=50,\n                                                      input_length=cps['max_sent_len'],\n                                                      trainable=True)(inputs['words_input'])\n            word_emb = trainable_lstm_embedding\n        return word_emb\n\n\n\n\ndef lstm_model(inputs, cps, word_emb):\n    word_rep = word_rep_with_char_info(inputs, cps, word_emb)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(word_rep)\n    lstm_out = Bidirectional(CuDNNGRU(64))(x)\n    lstm_logits = Dense(1, activation='linear')(lstm_out)\n    lstm_pred = Activation('sigmoid', name='lstm_pred')(lstm_logits)\n    return lstm_pred, lstm_logits\n\n\ndef conv_model(inputs, cps, word_emb):\n    word_rep = word_rep_with_char_info(inputs, cps, word_emb)\n    bilstm = Bidirectional(CuDNNLSTM(64, return_sequences=True))(word_rep)\n    conv_cell_out = conv_cell([bilstm, word_rep])\n\n    # add additional sentence features? can comment in/out\n    conv_cell_out = Concatenate()([conv_cell_out, inputs['sent_feats_input']])\n\n    # conv_dense = Dense(64, activation='relu')(conv_cell_out)\n    # # maybe add another dense layer?\n    # conv_dense = Dropout(0.2)(conv_dense)\n    conv_logits = Dense(1, activation='linear')(conv_cell_out)\n    conv_pred = Activation('sigmoid', name='conv_pred')(conv_logits)\n    return conv_pred, conv_logits\n\n\ndef ensemble_weights_model(inputs, cps, word_emb, lstm_logits, conv_logits):\n    # todo: make this simpler\n\n    word_rep = word_rep_with_char_info(inputs, cps, word_emb)\n    # x = Bidirectional(CuDNNLSTM(16, return_sequences=True))(word_rep)\n    # conv_cell_out = conv_cell([x, word_rep])\n\n    # add additional sentence features? can comment in/out\n    # conv_cell_out = Concatenate()([conv_cell_out, inputs['sent_feats_input'],\n    #                                lstm_logits, conv_logits])\n\n    # conv_dense = Dense(32, activation='relu')(conv_cell_out)\n\n    ensemble_weights = Dense(2, activation='softmax')(inputs['sent_feats_input'])\n    return ensemble_weights\n\n\ndef word_rep_with_char_info(inputs, cps, word_emb):\n\n    char_features = char_level_feature_model(inputs, cps)\n\n    # Can optionally allow each word_rep to have some of it's word embedding trainable by including this:\n    # and then concatenating at the end\n\n    # trainable_char_embedding = EmbeddingLayer(input_dim=word_vocab_size, output_dim=10,\n    #                                           input_length=max_sent_len)(words_input)\n    # trainable_char_embedding = SpatialDropout1D(0.1)(trainable_char_embedding)\n\n    word_emb = SpatialDropout1D(0.2)(word_emb)\n    word_rep = Concatenate()([char_features, word_emb, inputs["words_feats_input"]])\n    # word_rep = Concatenate()([char_features, word_emb, inputs["words_feats_input"], trainable_conv_embedding])\n\n    return word_rep\n\n\ndef char_level_feature_model(inputs, cps, outdim=128):\n\n    chars_words_embedding = TimeDistributed(EmbeddingLayer(cps['char_vocab_size'],\n                                                           output_dim=16,\n                                                           # embeddings_regularizer=regularizers.l1(),\n                                                           input_length=cps['max_word_len']))(inputs['chars_input'])\n    x = TimeDistributed(SpatialDropout1D(0.1))(chars_words_embedding)\n    char_rep = Concatenate()([x, inputs['char_feats_input']])\n    conv_outputs = []\n    # todo: tune these conv kernels\n    conv_kernels = [[32, 1], [32, 2], [32, 3], [32, 4]]\n    for num_filter, kernel_size in conv_kernels:\n        char_conv = TimeDistributed(Conv1D(filters=num_filter, kernel_size=kernel_size))(char_rep)\n        x = TimeDistributed(BatchNormalization())(char_conv)\n        x = TimeDistributed(Activation('relu'))(x)\n        # x = TimeDistributed(Dropout(0.1))(x)\n        m = TimeDistributed(GlobalMaxPooling1D())(x)\n        conv_outputs.append(m)\n    conv_outs = Concatenate()(conv_outputs)\n    # feats_1 = Dense(outdim)(conv_outs)\n    # feats_1 = BatchNormalization()(feats_1)\n    # feats_1 = Activation('relu')(feats_1)\n    # feats_2 = Dense(outdim)(feats_1)\n    # feats_2 = BatchNormalization()(feats_2)\n    # feats_2 = Activation('relu')(feats_2)\n    # x = Concatenate()([feats_1, feats_2])\n    return conv_outs\n\n\ndef conv_cell(input_layers, conv_kernels=[[32, 1], [32, 3], [32, 5], [32, 7]]):\n    conv_outputs = []\n\n    if not isinstance(input_layers, list):\n        input_layers = [input_layers]\n    for num_filter, kernel_size in conv_kernels:\n        # todo: consider adding output of bilstm back here\n        # why is conv model weak? but we don't want to affect learning (turn off backprop?)\n        for i in input_layers:\n            char_conv = Conv1D(filters=num_filter, kernel_size=kernel_size)(i)\n            batch_norm = BatchNormalization()(char_conv)\n            activation = Activation('relu')(batch_norm)\n            conv_features = []\n            conv_features.append(GlobalMaxPooling1D()(activation))\n            # conv_features.append(GlobalAveragePooling1D()(activation))\n            if len(conv_features) > 1:\n                conv_feats = Concatenate()(conv_features)\n            else:\n                conv_feats = conv_features[0]\n            conv_outputs.append(conv_feats)\n    conv_cell_out = Concatenate()(conv_outputs)\n    return conv_cell_out\n''')
    __stickytape_write_module('''src/InsincereModel.py''', '''import keras.backend as K\nimport logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam\n\nfrom src.Embedding import Embedding\nfrom src.LRFinder import LRFinder\nfrom src.OneCycleLR import OneCycleLR\nfrom src.config import config_insincere_model\nfrom src.data_generator import DataGenerator\n\n\nclass InsincereModel:\n    def __init__(self, data, corpus_info, text_mapper, batch_size=16, name=None, loss='binary_crossentropy'):\n        self.data = data\n        self.corpus_info = corpus_info\n        self.text_mapper = text_mapper\n        self.name = name\n        self.embedding = None\n        self.model = None\n        self.history = None\n        self.loss = loss\n        self.lr_finder = None\n        self.config = config_insincere_model\n        self.batch_size = batch_size\n\n    def load_embedding(self, embedding_file='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'):\n        self.embedding = Embedding(self.corpus_info.word_counts)\n        self.embedding.load(embedding_file)\n\n    def set_embedding(self, embedding):\n        if type(embedding) is str:\n            self.load_embedding(embedding)\n        else:\n            self.embedding = embedding\n\n    def blend_embeddings(self, embeddings, cleanup=False):\n        """Average embedding matrix given list of embedding files."""\n        if self.embedding is None:\n            self.set_embedding(embeddings[0])\n        embedding_matrices = list()\n        for emb in embeddings:\n            embedding_matrices.append(emb.embedding_matrix)\n        blend = np.mean(embedding_matrices, axis=0)\n        self.embedding.embedding_matrix = blend\n        if cleanup:\n            for e in embeddings:\n                e.cleanup()\n        return blend\n\n    def concat_embeddings(self, embeddings, cleanup=False):\n        if self.embedding is None:\n            self.set_embedding(embeddings[0])\n        self.embedding.embedding_matrix = np.concatenate(tuple([e.embedding_matrix for e in embeddings]), axis=1)\n        self.embedding.embed_size = self.embedding.embedding_matrix.shape[1]\n        if cleanup:\n            for e in embeddings:\n                e.cleanup()\n        return self.embedding.embedding_matrix\n\n    @staticmethod\n    def f1_score(y_true, y_pred):\n        def recall(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n            recall = true_positives / (possible_positives + K.epsilon())\n            return recall\n\n        def precision(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n            precision = true_positives / (predicted_positives + K.epsilon())\n            return precision\n\n        precision = precision(y_true, y_pred)\n        recall = recall(y_true, y_pred)\n        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n    def define_model(self):\n        raise NotImplementedError\n\n    def print(self):\n        print(self.model.summary())\n\n    def _get_callbacks(self, epochs, batch_size, checkpoint=False, one_cycle=False):\n        config = self.config.get('callbacks')\n        early_stop = EarlyStopping(monitor=config.get('early_stopping').get('monitor'),\n                                   mode=config.get('early_stopping').get('mode'),\n                                   patience=1,\n                                   verbose=config.get('early_stopping').get('verbose'),\n                                   restore_best_weights=True)\n        cbs = [early_stop]\n        if one_cycle:\n            num_samples = len(self.data.train_qs)\n            self.lr_finder = LRFinder(num_samples, batch_size)\n            lr_manager = OneCycleLR(num_samples, epochs, batch_size)\n            cbs += [self.lr_finder, lr_manager]\n        if checkpoint:\n            check_point = ModelCheckpoint('model.hdf5',\n                                          monitor=config.get('checkpoint').get('monitor'),\n                                          mode=config.get('checkpoint').get('mode'),\n                                          verbose=config.get('checkpoint').get('verbose'),\n                                          save_best_only=config.get('checkpoint').get('save_best_only'))\n            cbs += [check_point]\n\n        return cbs\n\n    def fit(self, curve_file_suffix=None):\n        logging.info("Fitting model...")\n        self.model.summary()\n\n        config = self.config.get('fit')\n\n        train_generator = DataGenerator(text=self.data.train_qs, labels=self.data.train_labels,\n                                        text_mapper=self.text_mapper, batch_size=self.batch_size)\n        val_generator = DataGenerator(text=self.data.val_qs, labels=self.data.val_labels,\n                                      text_mapper=self.text_mapper, batch_size=self.batch_size)\n\n        callbacks = self._get_callbacks(config.get('epochs'), config.get('batch_size'))\n\n        batch_size = [64, 128, 256]\n        loss_weights = [[1, 1, 0], [1, 1, 0], [1, 1, 0]]\n        epsilon = [1e-7, 1e-6, 1e-5]\n        fraction_of_training = [1/4, 1/2, 3/4]\n        lr = [0.001, 0.001, 0.0003]\n        # todo: write this in a for loop and change batch size, learning rate, and epsilon (K.set_epsilon(1e-2))\n        # todo: optimize learning rates\n        # todo: experiment with epsilon smoothing\n        for i in range(3):\n            train_generator.batch_size = batch_size[i]\n            steps_per_epoch = int(len(train_generator)*fraction_of_training[i])\n\n            # only if time allows more experimentation\n            # K.set_epsilon(epsilon[i])\n\n            self.model.compile(loss=self.loss, loss_weights=loss_weights[i],\n                               optimizer=Adam(lr=lr[i]), metrics=[self.f1_score])\n            self.model.fit_generator(generator=train_generator, steps_per_epoch=steps_per_epoch,\n                                     verbose=1, callbacks=callbacks,\n                                     validation_data=val_generator,\n                                     max_queue_size=10,\n                                     workers=1,\n                                     use_multiprocessing=False,\n                                     shuffle=True)\n            train_generator.shuffle_data()\n\n\n        # self.history = self.model.fit(x=train_x,\n        #                               y=train_y,\n        #                               epochs=2,\n        #                               batch_size=32,\n        #                               validation_data=(val_x, val_y),\n        #                               callbacks=callbacks)\n\n        if config.get('save_curve'):\n            if self.lr_finder:\n                self.lr_finder.plot_schedule(filename="lr_schedule_" + str(self.name) + ".png")\n            filename = 'training_curve'\n            if self.name:\n                filename += '_' + self.name\n            if curve_file_suffix:\n                filename += '_' + curve_file_suffix\n            filename += '.png'\n            # self.print_curve(filename)\n\n    def predict_subset(self, subset='train'):\n        if subset == 'train':\n            questions = self.data.train_qs\n        elif subset == 'val':\n            questions = self.data.val_qs\n        elif subset == 'test':\n            questions = self.data.get_questions(subset)\n\n        # input_x = self.prepare_model_inputs(questions)\n        # preds = self.predict(input_x)\n        data_gen = DataGenerator(text=questions, text_mapper=self.text_mapper, shuffle=False)\n        preds = self.model.predict_generator(data_gen, workers=1, use_multiprocessing=False, max_queue_size=10)\n        return preds[2][:, 0]\n\n    def print_curve(self, filename='training_curve.png'):\n        plt.plot(self.history.history['loss'])\n        plt.plot(self.history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'val'], loc='best')\n        plt.savefig(filename)\n        plt.close()\n\n    def predict(self, x):\n        logging.info("Predicting ...")\n        batch_size = self.config.get('predict').get('batch_size')\n        verbose = self.config.get('predict').get('verbose')\n        prediction = self.model.predict(x, batch_size=batch_size, verbose=verbose)\n        return prediction\n\n    def cleanup(self):\n        self.embedding.cleanup()\n\n    def prepare_model_inputs(self, questions):\n        model_input = self.text_mapper.texts_to_x(questions)\n        words_input = model_input['words_input']\n        chars_input = model_input['chars_input']\n        char_feats_input = model_input['chars_feats_input']\n        return {'words_input': words_input, 'chars_input': chars_input, 'chars_feats_input': char_feats_input}\n''')
    __stickytape_write_module('''src/LRFinder.py''', '''import keras.backend as K\nimport numpy as np\nimport os\nimport warnings\nfrom keras.callbacks import Callback\n\nfrom src.config import config_lrfinder as config\n\n\nclass LRFinder(Callback):\n\n    def __init__(self, num_samples, batch_size, validation_data=None):\n        """\n        This class uses the Cyclic Learning Rate history to find a\n        set of learning rates that can be good initializations for the\n        One-Cycle training proposed by Leslie Smith in the paper referenced\n        below.\n        A port of the Fast.ai implementation for Keras.\n        # Note\n        This requires that the model be trained for exactly 1 epoch. If the model\n        is trained for more epochs, then the metric calculations are only done for\n        the first epoch.\n        # Interpretation\n        Upon visualizing the loss plot, check where the loss starts to increase\n        rapidly. Choose a learning rate at somewhat prior to the corresponding\n        position in the plot for faster convergence. This will be the maximum_lr lr.\n        Choose the max value as this value when passing the `max_val` argument\n        to OneCycleLR callback.\n        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n        # Arguments:\n            num_samples: Integer. Number of samples in the dataset.\n            batch_size: Integer. Batch size during training.\n            minimum_lr: Float. Initial learning rate (and the minimum).\n            maximum_lr: Float. Final learning rate (and the maximum).\n            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n                scaling for each update to the learning rate during subsequent\n                batches. Choose 'exp' for large range and 'linear' for small range.\n            validation_data: Requires the validation dataset as a tuple of\n                (X, y) belonging to the validation set. If provided, will use the\n                validation set to compute the loss metrics. Else uses the training\n                batch loss. Will warn if not provided to alert the user.\n            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n                validation set per iteration of the LRFinder. Larger number of\n                samples will reduce the variance but will take longer time to execute\n                per batch.\n                If Positive > 0, will sample from the validation dataset\n                If Megative, will use the entire dataset\n            stopping_criterion_factor: Integer or None. A factor which is used\n                to measure large increase in the loss value during training.\n                Since callbacks cannot stop training of a model, it will simply\n                stop logging the additional values from the epochs after this\n                stopping criterion has been met.\n                If None, this check will not be performed.\n            loss_smoothing_beta: Float. The smoothing factor for the moving\n                average of the loss function.\n            save_dir: Optional, String. If passed a directory path, the callback\n                will save the running loss and learning rates to two separate numpy\n                arrays inside this directory. If the directory in this path does not\n                exist, they will be created.\n            verbose: Whether to print the learning rate after every batch of training.\n        # References:\n            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n        """\n        super(LRFinder, self).__init__()\n\n        minimum_lr = config.get('minimum_lr')\n        maximum_lr = config.get('maximum_lr')\n        lr_scale = config.get('lr_scale')\n        validation_sample_rate = config.get('validation_sample_rate')\n        stopping_criterion_factor = config.get('stopping_criterion_factor')\n        loss_smoothing_beta = config.get('loss_smoothing_beta')\n        save_dir = config.get('save_dir')\n        verbose = config.get('verbose')\n        if lr_scale not in ['exp', 'linear']:\n            raise ValueError("`lr_scale` must be one of ['exp', 'linear']")\n\n        if validation_data is not None:\n            self.validation_data = validation_data\n            self.use_validation_set = True\n\n            if validation_sample_rate > 0 or validation_sample_rate < 0:\n                self.validation_sample_rate = validation_sample_rate\n            else:\n                raise ValueError("`validation_sample_rate` must be a positive or negative integer other than o")\n        else:\n            self.use_validation_set = False\n            self.validation_sample_rate = 0\n\n        self.num_samples = num_samples\n        self.batch_size = batch_size\n        self.initial_lr = minimum_lr\n        self.final_lr = maximum_lr\n        self.lr_scale = lr_scale\n        self.stopping_criterion_factor = stopping_criterion_factor\n        self.loss_smoothing_beta = loss_smoothing_beta\n        self.save_dir = save_dir\n        self.verbose = verbose\n\n        self.num_batches_ = num_samples // batch_size - 1\n        self.current_lr_ = minimum_lr\n\n        if lr_scale == 'exp':\n            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (1. / float(self.num_batches_))\n        else:\n            extra_batch = int((num_samples % batch_size) != 0)\n            self.lr_multiplier_ = np.linspace(minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n\n        # If negative, use entire validation set\n        if self.validation_sample_rate < 0:\n            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n\n        self.current_batch_ = 0\n        self.current_epoch_ = 0\n        self.best_loss_ = 1e6\n        self.running_loss_ = 0.\n\n        self.history = {}\n\n    def on_train_begin(self, logs=None):\n        self.current_epoch_ = 1\n        K.set_value(self.model.optimizer.lr, self.initial_lr)\n\n        warnings.simplefilter("ignore")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.current_batch_ = 0\n\n        if self.current_epoch_ > 1:\n            warnings.warn("\\n\\nLearning rate finder should be used only with a single epoch. "\n                          "Hereafter, the callback will not measure the losses.\\n\\n")\n\n    def on_batch_begin(self, batch, logs=None):\n        self.current_batch_ += 1\n\n    def on_batch_end(self, batch, logs=None):\n        if self.current_epoch_ > 1:\n            return\n\n        if self.use_validation_set:\n            X, Y = self.validation_data[0], self.validation_data[1]\n\n            # use 5 random batches from test set for fast approximate of loss\n            num_samples = self.batch_size * self.validation_sample_rate\n\n            if num_samples > X.shape[0]:\n                num_samples = X.shape[0]\n\n            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n            x = X[idx]\n            y = Y[idx]\n\n            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n            loss = values[0]\n        else:\n            loss = logs['loss']\n\n        # smooth the loss value and bias correct\n        running_loss = self.loss_smoothing_beta * loss + (1. - self.loss_smoothing_beta) * loss\n        running_loss = running_loss / (1. - self.loss_smoothing_beta ** self.current_batch_)\n\n        # stop logging if loss is too large\n        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n                running_loss > self.stopping_criterion_factor * self.best_loss_):\n\n            if self.verbose:\n                print(" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)" % (\n                    self.stopping_criterion_factor, self.best_loss_\n                ))\n            return\n\n        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n            self.best_loss_ = running_loss\n\n        current_lr = K.get_value(self.model.optimizer.lr)\n\n        self.history.setdefault('running_loss_', []).append(running_loss)\n        if self.lr_scale == 'exp':\n            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n        else:\n            self.history.setdefault('log_lrs', []).append(current_lr)\n\n        # compute the lr for the next batch and update the optimizer lr\n        if self.lr_scale == 'exp':\n            current_lr *= self.lr_multiplier_\n        else:\n            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n\n        K.set_value(self.model.optimizer.lr, current_lr)\n\n        # save the other metrics as well\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        if self.verbose:\n            if self.use_validation_set:\n                print(" - LRFinder: val_loss: %1.4f - lr = %1.8f " % (values[0], current_lr))\n            else:\n                print(" - LRFinder: lr = %1.8f " % current_lr)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.save_dir is not None and self.current_epoch_ <= 1:\n            if not os.path.exists(self.save_dir):\n                os.makedirs(self.save_dir)\n\n            losses_path = os.path.join(self.save_dir, 'losses.npy')\n            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n\n            np.save(losses_path, self.losses)\n            np.save(lrs_path, self.lrs)\n\n            if self.verbose:\n                print("\\tLR Finder : Saved the losses and learning rate values in path : {%s}" % (self.save_dir))\n\n        self.current_epoch_ += 1\n\n        warnings.simplefilter("default")\n\n    def plot_schedule(self, filename="lr_schedule.png", clip_beginning=None, clip_endding=None):\n        """\n        Plots the schedule from the callback itself.\n        # Arguments:\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        """\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print("Matplotlib not found. Please use `pip install matplotlib` first.")\n            return\n\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses = self.losses\n        lrs = self.lrs\n\n        if clip_beginning:\n            losses = losses[clip_beginning:]\n            lrs = lrs[clip_beginning:]\n\n        if clip_endding:\n            losses = losses[:clip_endding]\n            lrs = lrs[:clip_endding]\n\n        plt.plot(lrs, losses)\n        plt.gca().set_yscale('log')\n        plt.title('Learning rate vs Loss')\n        plt.xlabel('log(learning rate)')\n        plt.ylabel('log(loss)')\n        plt.savefig(filename)\n\n    @classmethod\n    def restore_schedule_from_dir(cls, directory, clip_beginning=None, clip_endding=None):\n        """\n        Loads the training history from the saved numpy files in the given directory.\n        # Arguments:\n            directory: String. Path to the directory where the serialized numpy\n                arrays of the loss and learning rates are saved.\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        Returns:\n            tuple of (losses, learning rates)\n        """\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses_path = os.path.join(directory, 'losses.npy')\n        lrs_path = os.path.join(directory, 'lrs.npy')\n\n        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n            print("%s and %s could not be found at directory : {%s}" % (\n                losses_path, lrs_path, directory\n            ))\n\n            losses = None\n            lrs = None\n\n        else:\n            losses = np.load(losses_path)\n            lrs = np.load(lrs_path)\n\n            if clip_beginning:\n                losses = losses[clip_beginning:]\n                lrs = lrs[clip_beginning:]\n\n            if clip_endding:\n                losses = losses[:clip_endding]\n                lrs = lrs[:clip_endding]\n\n        return losses, lrs\n\n    @classmethod\n    def plot_schedule_from_file(cls, directory, clip_beginning=None, clip_endding=None):\n        """\n        Plots the schedule from the saved numpy arrays of the loss and learning\n        rate values in the specified directory.\n        # Arguments:\n            directory: String. Path to the directory where the serialized numpy\n                arrays of the loss and learning rates are saved.\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        """\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print("Matplotlib not found. Please use `pip install matplotlib` first.")\n            return\n\n        losses, lrs = cls.restore_schedule_from_dir(directory,\n                                                    clip_beginning=clip_beginning,\n                                                    clip_endding=clip_endding)\n\n        if losses is None or lrs is None:\n            return\n        else:\n            plt.plot(lrs, losses)\n            plt.title('Learning rate vs Loss')\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.show()\n\n    @property\n    def lrs(self):\n        return np.array(self.history['log_lrs'])\n\n    @property\n    def losses(self):\n        return np.array(self.history['running_loss_'])\n''')
    __stickytape_write_module('''src/OneCycleLR.py''', '''import keras.backend as K\nfrom keras.callbacks import Callback\n\nfrom src.config import config_one_cycle as config\n\n\nclass OneCycleLR(Callback):\n    def __init__(self, num_samples, num_epochs, batch_size):\n        """ This callback implements a cyclical learning rate policy (CLR).\n        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n        After the completion of 1 cycle, the learning rate will decrease rapidly to\n        100th its initial lowest value.\n        # Arguments:\n            num_samples: Integer. Number of sample points in the dataset\n            num_epochs: Integer. Number of training epochs\n            batch_size: Integer. Batch size per training epoch\n            max_lr: Float. Initial learning rate. This also sets the\n                starting learning rate (which will be 10x smaller than\n                this), and will increase to this value during the first cycle.\n            end_percentage: Float. The percentage of all the epochs of training\n                that will be dedicated to sharply decreasing the learning\n                rate after the completion of 1 cycle. Must be between 0 and 1.\n            scale_percentage: Float or None. If float, must be between 0 and 1.\n                If None, it will compute the scale_percentage automatically\n                based on the `end_percentage`.\n            maximum_momentum: Optional. Sets the maximum momentum (initial)\n                value, which gradually drops to its lowest value in half-cycle,\n                then gradually increases again to stay constant at this max value.\n                Can only be used with SGD Optimizer.\n            minimum_momentum: Optional. Sets the minimum momentum at the end of\n                the half-cycle. Can only be used with SGD Optimizer.\n            verbose: Bool. Whether to print the current learning rate after every\n                epoch.\n        # Reference\n            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n        """\n        super(OneCycleLR, self).__init__()\n\n        end_percentage = config.get('end_percentage')\n        scale_percentage = config.get('scale_percentage')\n        maximum_momentum = config.get('maximum_momentum')\n        minimum_momentum = config.get('minimum_momentum')\n        max_lr = config.get('max_lr')\n        verbose = config.get('verbose')\n\n        if end_percentage < 0. or end_percentage > 1.:\n            raise ValueError("`end_percentage` must be between 0 and 1")\n\n        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n            raise ValueError("`scale_percentage` must be between 0 and 1")\n\n        self.num_samples = num_samples\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.num_samples_per_batch = max(num_samples // batch_size, 3)\n        self.initial_lr = max_lr\n        self.end_percentage = end_percentage\n        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n        self.max_momentum = maximum_momentum\n        self.min_momentum = minimum_momentum\n        self.verbose = verbose\n\n        self.num_iterations = self.num_epochs * self.num_samples_per_batch\n        self.mid_cycle_id = int(self.num_iterations * ((1. - end_percentage)) / float(2))\n\n        if self.max_momentum is not None and self.min_momentum is not None:\n            self._update_momentum = True\n        else:\n            self._update_momentum = False\n\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def _reset(self):\n        """\n        Reset the callback.\n        """\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def compute_lr(self):\n        """\n        Compute the learning rate based on which phase of the cycle it is in.\n        - If in the first half of training, the learning rate gradually increases.\n        - If in the second half of training, the learning rate gradually decreases.\n        - If in the final `end_percentage` portion of training, the learning rate\n            is quickly reduced to near 100th of the original min learning rate.\n        # Returns:\n            the new learning rate\n        """\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n            new_lr = self.initial_lr * (1. + (current_percentage * (1. - 100.) / 100.)) * self.scale\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - (self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage * (self.scale * 100 - 1.)) * self.scale\n\n        else:\n            current_percentage = self.clr_iterations / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage * (self.scale * 100 - 1.)) * self.scale\n\n        if self.clr_iterations == self.num_iterations:\n            self.clr_iterations = 0\n\n        return new_lr\n\n    def compute_momentum(self):\n        """\n         Compute the momentum based on which phase of the cycle it is in.\n        - If in the first half of training, the momentum gradually decreases.\n        - If in the second half of training, the momentum gradually increases.\n        - If in the final `end_percentage` portion of training, the momentum value\n            is kept constant at the maximum initial value.\n        # Returns:\n            the new momentum value\n        """\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            new_momentum = self.max_momentum\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(self.mid_cycle_id))\n            new_momentum = self.max_momentum - current_percentage * (self.max_momentum - self.min_momentum)\n\n        else:\n            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n            new_momentum = self.max_momentum - current_percentage * (self.max_momentum - self.min_momentum)\n\n        return new_momentum\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        self._reset()\n        K.set_value(self.model.optimizer.lr, self.compute_lr())\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError("Momentum can be updated only on SGD optimizer !")\n\n            new_momentum = self.compute_momentum()\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n    def on_batch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        self.clr_iterations += 1\n        new_lr = self.compute_lr()\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        K.set_value(self.model.optimizer.lr, new_lr)\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError("Momentum can be updated only on SGD optimizer !")\n\n            new_momentum = self.compute_momentum()\n\n            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.verbose:\n            if self._update_momentum:\n                print(" - lr: %0.5f - momentum: %0.2f " % (self.history['lr'][-1],\n                                                           self.history['momentum'][-1]))\n\n            else:\n                print(" - lr: %0.5f " % (self.history['lr'][-1]))\n''')
    import logging
    import matplotlib
    from pprint import pprint
    
    from src.UnknownWords import UnknownWords
    from src.data_generator import DataGenerator
    
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import random
    import tensorflow as tf
    import spacy
    import pickle
    import os
    
    from sklearn import metrics
    from sklearn.metrics import precision_recall_curve
    from sklearn.model_selection import StratifiedKFold
    
    from src.Data import Data, CorpusInfo
    from src.data_mappers import TextMapper, CharMapper
    from src.Embedding import Embedding
    from src.Models import *  # Make all models available for easy script generation.
    from src.config import random_state as SEED, config_main as config
    
    np.random.seed(SEED)
    tf.set_random_seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    random.seed(SEED)
    
    
    def find_best_threshold(y_proba, y_true, plot=False):
        logging.info("Finding best threshold...")
        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
        thresholds = np.append(thresholds, 1.001)
        F = 2.0 / (1.0/precision + 1.0/recall)
        best_score = np.max(F)
        best_th = thresholds[np.argmax(F)]
        logging.info("Best score = {}. Best threshold = {}".format(best_score, best_th))
        if plot:
            plt.plot(thresholds, F, '-b')
            plt.plot([best_th], [best_score], '*r')
            plt.savefig('threshold.png')
            plt.close()
        return best_th
    
    
    def write_predictions(data, preds, thresh=0.5):
        logging.info("Writing predictions ...")
        preds = (preds > thresh).astype(int)
        out_df = pd.DataFrame({"qid": data.test_df["qid"].values})
        out_df['prediction'] = preds
        return out_df
    
    
    def print_diagnostics(y_true, y_pred, file_suffix='', persist=True):
        try:
            cfn_matrix = metrics.confusion_matrix(y_true, y_pred)
        except ValueError:
            logging.warning("Warning: mix of binary and continuous targets used. Searching for best threshold.")
            thresh = find_best_threshold(y_pred, y_true)
            logging.warning("Applying threshold {} to predictions.".format(thresh))
            y_pred = (y_pred > thresh).astype(int)
            cfn_matrix = metrics.confusion_matrix(y_true, y_pred)
        with open('diagnostics' + file_suffix + '.txt', 'w') if persist else None as f:
            print("Confusion Matrix", file=f)
            print(cfn_matrix, file=f)
            print("-"*40, file=f)
            print("F1 score: " + str(metrics.f1_score(y_true, y_pred)), file=f)
            print("MCC score: " + str(metrics.matthews_corrcoef(y_true, y_pred)), file=f)
            print("precision: " + str(metrics.precision_score(y_true, y_pred)), file=f)
            print("Recall: " + str(metrics.recall_score(y_true, y_pred)), file=f)
    
    
    def get_wrongest(X, y_true, y_pred, num_wrongest=5):
        logging.info("Finding the worst predictions...")
        df = pd.DataFrame({'qid': X['qid'],
                           'question_text': X['question_text'],
                           'y_true': y_true,
                           'y_pred': y_pred.reshape(len(y_pred))})
        df['prediction_error'] = df['y_true'] - df['y_pred']
        df = df.sort_values('prediction_error')
        return df[df['y_true'] == 0].head(num_wrongest), df[df['y_true'] == 1].tail(num_wrongest)
    
    
    def print_wrongest(X, y_true, y_pred, num_wrongest=100, print_them=False, persist=True, file_suffix=None):
        def print_row(row):
            print("Q:" + row['question_text'])
            print("qid: " + row['qid'])
            print("Target: " + str(row['y_true']))
            print("Prediction: " + str(row['y_pred']))
            print("-"*40)
    
        wrongest_fps, wrongest_fns = get_wrongest(X, y_true, y_pred, num_wrongest=num_wrongest)
        if print_them:
            print("Wrongest {} false positives:".format(num_wrongest))
            print("-" * 40)
            for i, row in wrongest_fps.iterrows():
                print_row(row)
            print()
            print("Wrongest {} false negatives:".format(num_wrongest))
            print("-" * 40)
            for i, row in wrongest_fns.iterrows():
                print_row(row)
        if persist:
            filename = 'wrongest'
            if file_suffix:
                filename += '_' + file_suffix
            wrongest_fps.to_csv(filename + '_fps.csv', index=False)
            wrongest_fns.to_csv(filename + '_fns.csv', index=False)
        return wrongest_fps, wrongest_fns
    
    
    def cross_validate(model_class, data, embeddings, n_splits=4, show_wrongest=True, model_config=None,):
        logging.info("Cross validating model {} using {} folds...".format(model_class.__name__, str(n_splits)))
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
        models = list()
        for i, (train, test) in enumerate(skf.split(data.train_X, data.train_y)):
            logging.info("Running Fold {} of {}".format(i + 1, n_splits))
            models.append(None)
            cv_name = model_class.__name__ + '_cv_' + str(i)
            models[-1] = model_class(data=data, name=cv_name)
            models[-1].blend_embeddings(embeddings)
            models[-1].define_model(model_config)
            models[-1].fit(train_indices=train, val_indices=test, curve_file_suffix=str(i))
            if data.custom_features:
                predict_X = [data.train_X[test], data.train_features[test]]
            else:
                predict_X = [data.train_X[test]]
            pred_y_val = models[-1].predict(predict_X)
            print_diagnostics(data.train_y[test], pred_y_val, file_suffix='_' + cv_name)
            if show_wrongest:
                print_wrongest(data.train_df.iloc[test],
                               data.train_y[test],
                               pred_y_val,
                               num_wrongest=20,
                               persist=True,
                               file_suffix=models[-1].name)
        return models
    
    
    def load_embeddings(word_counts, word_threshold, embedding_files, keep_index=True):
        embeddings = list()
        for f in embedding_files:
            embeddings.append(Embedding(word_counts, word_threshold))
            embeddings[-1].load(f)
            if not keep_index:
                embeddings[-1].cleanup_index()
        return embeddings
    
    
    def save_unknown_words(data, embeddings, max_words=None):
        vocab = data.get_train_vocab()
        nb_words = 0
        for v in vocab.items():
            nb_words += v[1]
        for emb in embeddings:
            unknown_words = emb.check_coverage(vocab)
            df_unknown_words = pd.DataFrame(unknown_words, columns=['word', 'count'])\
                .sort_values('count', ascending=False)
            df_unknown_words['frequency'] = df_unknown_words['count'] / nb_words
            df_unknown_words = df_unknown_words.head(max_words)
            df_unknown_words.to_csv('unknown_words_' + emb.name + '.csv', index=False)
    
    
    def cleanup_models(models):
        for m in models:
            m.cleanup()
    
    def save_configs():
        from src.config import random_state, \
            config_data, \
            config_insincere_model, \
            config_lrfinder, \
            config_one_cycle, \
            config_main
        config_dict = locals().copy()
        with open('configs.txt', 'wt') as f:
            pprint(config_dict, stream=f)
        logging.info('Configurations: ')
        logging.info(pprint(config_dict))
    
    
    def main():
        word_threshold = 8
        char_threshold = 15
        fix_unknowns = False
    
        embedding_files = config.get('embedding_files')
        dev_size = config.get('dev_size')
        # dev_size = 50000
        data = Data()
        data.load(dev_size)
        data.perform_preprocessing()
        data.split()
    
        nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])
    
        ci_path = '/home/matt/ci.p'
        if os.path.isfile(ci_path):
            corpus_info = pickle.load(open(ci_path, 'rb'))
        else:
            corpus_info = CorpusInfo(data.get_questions(subset='train'), nlp)
            # pickle.dump(corpus_info, open(ci_path, 'wb'))
    
        word_counts = corpus_info.word_counts
        char_counts = corpus_info.char_counts
    
        emb_path = f'/home/matt/emb{word_threshold}.p'
        unk_emb_path = f'/home/matt/emb_unk{word_threshold}.p'
        text_mapper_path = f'/home/matt/text_map{word_threshold}.p'
    
        if os.path.isfile(unk_emb_path) and os.path.isfile(text_mapper_path):
            embeddings = pickle.load(open(unk_emb_path, 'rb'))
            text_mappers = pickle.load(open(text_mapper_path, 'rb'))
        else:
            if os.path.isfile(emb_path):
                embeddings = pickle.load(open(emb_path, 'rb'))
            else:
                embeddings = load_embeddings(word_counts, word_threshold, embedding_files)
                # pickle.dump(embeddings, open(emb_path, 'wb'))
    
            text_mappers = []
    
            unknown_char_mapper = CharMapper(char_counts=char_counts, threshold=10, char_lowercase=True)
            for ind, embedding in enumerate(embeddings):
                text_mapper = TextMapper(word_counts=word_counts, char_counts=char_counts,
                                         word_threshold=word_threshold,
                                         max_word_len=15, char_threshold=char_threshold, max_sent_len=70, nlp=nlp,
                                         word_lowercase=True, char_lowercase=True)
                text_mapper.word_mapper.set_vocab(embedding.word_map_list)
                text_mappers.append(text_mapper)
    
                if fix_unknowns:
                    unknown_char_len = 20
                    unknown_word_model = UnknownWords(char_mapper=unknown_char_mapper, max_word_len=unknown_char_len,
                                                      embedding=embedding, text_mapper=text_mapper)
                    unknown_word_model.define_model()
                    unknown_word_model.fit()
                    unknown_word_model.improve_embedding()
    
            # pickle.dump(embeddings, open(unk_emb_path, 'wb'))
            # pickle.dump(text_mappers, open(text_mapper_path, 'wb'))
    
        preds = []
        submit_preds = []
        for text_mapper, embedding in zip(text_mappers, embeddings):
        # embedding = embeddings[0]
        # print("{} unknown words or of {}".format(len(embedding.unknown_words), len(embedding.word_map_list)))
        # text_mapper = text_mappers[0]
            model = BiLSTMCharCNNModel(data=data, corpus_info=corpus_info, text_mapper=text_mapper, batch_size=128)
            model.set_embedding(embedding)
            model.define_model()
            model.fit()
    
            test_preds = model.predict_subset('test')
            submit_preds.append(test_preds)
    
            val_preds = model.predict_subset(subset='val')
            preds.append(val_preds)
    
        val_preds_np = np.array(preds)
        val_preds_y = val_preds_np.mean(axis=0)
    
        val_y = np.array(data.val_labels)
        thresh = find_best_threshold(val_preds_y, val_y)
    
        # pred_val_y = ensemble_cv.predict_linear_regression(train_X, data.train_y, val_X)
        print_diagnostics(val_y, (val_preds_y > thresh).astype(int))
        # pred_y_test = ensemble_cv.predict_linear_regression(train_X, data.train_y, test_X)
    
        submit_preds_np = np.array(submit_preds)
        submit_preds_y = submit_preds_np.mean(axis=0)
    
        write_predictions(data, submit_preds_y, thresh)
    
    
    if __name__ == "__main__":
        logging.getLogger()
        logging.basicConfig(
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.DEBUG,
            datefmt='%Y-%m-%d %H:%M:%S')
        save_configs()
        config_tf = tf.ConfigProto()
        config_tf.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU
        config_tf.log_device_placement = True  # to log device placement (on which device the operation ran)
        sess = tf.Session(config=config_tf)
        main()
        sess.close()
        logging.info("Done!")
    