#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path, errno

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")
                    
        make_package(os.path.dirname(path))
        
        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('''src/Data.py''', '''import logging\nimport numpy as np\nimport pandas as pd\nimport re\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom src.config import config_data, random_state\n\n\nclass Data:\n    def __init__(self, train_path="../input/train.csv", test_path="../input/test.csv"):\n        self.train_path = train_path\n        self.test_path = test_path\n        self.train_df = None\n        self.test_df = None\n        self.val_df = None\n        self.train_X = None\n        self.val_X = None\n        self.test_X = None\n        self.full_X = None\n        self.full_y = None\n        self.train_y = None\n        self.val_y = None\n        self.maxlen = None\n        self.tokenizer = None\n        self.max_feature = None\n        self.custom_features = None\n        self.train_features = None\n        self.val_features = None\n        self.test_features = None\n        self.feature_scaler = None\n        self.config = config_data\n\n    def load(self, dev_size=None):\n        logging.info("Loading data...")\n        if dev_size is not None:\n            logging.warning("Using dev set of size=" + str(dev_size))\n        self.train_df = pd.read_csv(self.train_path, nrows=dev_size)\n        self.test_df = pd.read_csv(self.test_path, nrows=dev_size)\n        logging.info("Train shape : {}".format(self.train_df.shape))\n        logging.info("Test shape : {}".format(self.test_df.shape))\n\n    @staticmethod\n    def _remove_stops(sentence):\n        stop = set(stopwords.words('english'))\n        filtered = list()\n        for w in sentence.split(" "):\n            if w not in stop:\n                filtered.append(w)\n        return " ".join(filtered)\n\n    def preprocess_questions(self, questions):\n        questions = questions.fillna("_na_")\n        preprocess_config = self.config.get('preprocess')\n        case_sensitive = not preprocess_config.get('lower_case')\n        if preprocess_config.get('lower_case'):\n            questions = questions.str.lower()\n        if preprocess_config.get('remove_stop_words'):\n            questions = questions.apply(self._remove_stops)\n        if preprocess_config.get('remove_contractions'):\n            questions = questions.apply(lambda x: self.clean_contractions(x))\n        if preprocess_config.get('remove_specials'):\n            questions = questions.apply(lambda x: self.clean_specials(x))\n        if preprocess_config.get('correct_spelling'):\n            questions = questions.apply(lambda x: self.clean_spelling(x, case_sensitive=case_sensitive))\n        if preprocess_config.get('replace_acronyms'):\n            questions = questions.apply(lambda x: self.clean_acronyms(x, case_sensitive=case_sensitive))\n        if preprocess_config.get('replace_non_words'):\n            questions = questions.apply(lambda x: self.clean_non_dictionary(x, case_sensitive=case_sensitive))\n        if preprocess_config.get('replace_numbers'):\n            questions = questions.apply(lambda x: self.clean_numbers(x))\n        return questions\n\n    def preprocessing(self):\n        logging.info("Preprocessing data...")\n        for df in [self.train_df, self.test_df]:\n            if self.config.get('preprocess').get('use_custom_features'):\n                df = self.add_features(df)\n            df['question_text'] = self.preprocess_questions(df['question_text'])\n        if self.config.get('preprocess').get('use_custom_features'):\n            self.scale_features()\n        self.split()\n        self.get_xs_ys()\n        self.tokenize()\n        self.pad_sequences()\n\n    def split(self):\n        logging.info("Train/Eval split...")\n        self.train_df, self.val_df = train_test_split(self.train_df,\n                                                      test_size=self.config.get('test_size'),\n                                                      random_state=random_state)\n        if self.custom_features:\n            self.train_features = self.train_df[self.custom_features].values\n            self.val_features = self.val_df[self.custom_features].values\n            self.test_features = self.test_df[self.custom_features].values\n\n    def get_xs_ys(self):\n        self.train_X = self.train_df["question_text"].values\n        self.val_X = self.val_df["question_text"].values\n        self.test_X = self.test_df["question_text"].values\n        self.train_y = self.train_df['target'].values\n        self.val_y = self.val_df['target'].values\n\n    def tokenize(self):\n        logging.info("Tokenizing...")\n        max_feature = self.config.get('max_feature')\n        tokenizer = Tokenizer(num_words=max_feature)\n        tokenizer.fit_on_texts(list(self.train_X))\n        self.train_X = tokenizer.texts_to_sequences(self.train_X)\n        self.val_X = tokenizer.texts_to_sequences(self.val_X)\n        self.test_X = tokenizer.texts_to_sequences(self.test_X)\n        self.tokenizer = tokenizer\n        self.max_feature = max_feature\n\n    def append_features(self):\n        self.train_X = [self.train_X, np.array(self.train_df[self.custom_features])]\n        self.val_X = [self.val_X, np.array(self.val_df[self.custom_features])]\n        self.test_X = [self.test_X, np.array(self.test_df[self.custom_features])]\n\n    def pad_sequences(self):\n        logging.info("Padding Sequences...")\n        maxlen = self.config.get('max_seq_len')\n        self.train_X = pad_sequences(self.train_X, maxlen=maxlen)\n        self.val_X = pad_sequences(self.val_X, maxlen=maxlen)\n        self.test_X = pad_sequences(self.test_X, maxlen=maxlen)\n        self.maxlen = maxlen\n\n    def add_pseudo_data(self, pred_test_y):\n        logging.warning("Using pseudo data...")\n        self.full_X = np.vstack([self.train_X, self.val_X, self.test_X])\n        self.full_y = np.vstack([self.train_y.reshape((len(self.train_y), 1)),\n                                 self.val_y.reshape((len(self.val_y), 1)), pred_test_y])\n\n    @staticmethod\n    def clean_contractions(text):\n        contraction_mapping = {"ain't": "is not", "aren't": "are not", "can't": "cannot", "'cause": "because",\n                               "could've": "could have", "couldn't": "could not", "didn't": "did not",\n                               "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not",\n                               "haven't": "have not", "he'd": "he would", "he'll": "he will", "he's": "he is",\n                               "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",\n                               "I'd": "i would", "I'd've": "I would have", "I'll": "i will", "I'll've": "i will have",\n                               "I'm": "i am", "I've": "i have", "i'd": "i would", "i'd've": "i would have",\n                               "i'll": "i will", "i'll've": "i will have", "i'm": "i am", "i've": "i have",\n                               "isn't": "is not", "it'd": "it would", "it'd've": "it would have", "it'll": "it will",\n                               "it'll've": "it will have", "it's": "it is", "let's": "let us", "ma'am": "madam",\n                               "mayn't": "may not", "might've": "might have", "mightn't": "might not",\n                               "mightn't've": "might not have", "must've": "must have", "mustn't": "must not",\n                               "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have",\n                               "o'clock": "of the clock", "oughtn't": "ought not", "oughtn't've": "ought not have",\n                               "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",\n                               "she'd": "she would", "she'd've": "she would have", "she'll": "she will",\n                               "she'll've": "she will have", "she's": "she is", "should've": "should have",\n                               "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have",\n                               "so's": "so as", "this's": "this is", "that'd": "that would",\n                               "that'd've": "that would have", "that's": "that is", "there'd": "there would",\n                               "there'd've": "there would have", "there's": "there is", "here's": "here is",\n                               "they'd": "they would", "they'd've": "they would have", "they'll": "they will",\n                               "they'll've": "they will have", "they're": "they are", "they've": "they have",\n                               "to've": "to have", "wasn't": "was not", "we'd": "we would", "we'd've": "we would have",\n                               "we'll": "we will", "we'll've": "we will have", "we're": "we are", "we've": "we have",\n                               "weren't": "were not", "what'll": "what will", "what'll've": "what will have",\n                               "what're": "what are", "what's": "what is", "what've": "what have", "when's": "when is",\n                               "when've": "when have", "where'd": "where did", "where's": "where is",\n                               "where've": "where have", "who'll": "who will", "who'll've": "who will have",\n                               "who's": "who is", "who've": "who have", "why's": "why is", "why've": "why have",\n                               "will've": "will have", "won't": "will not", "won't've": "will not have",\n                               "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have",\n                               "y'all": "you all", "y'all'd": "you all would", "y'all'd've": "you all would have",\n                               "y'all're": "you all are", "y'all've": "you all have", "you'd": "you would",\n                               "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",\n                               "you're": "you are", "you've": "you have"}\n        specials = ["\u2019", "\u2018", "\xb4", "`"]\n        for s in specials:\n            text = text.replace(s, "'")\n        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(" ")])\n        return text\n\n    @staticmethod\n    def clean_specials(text):\n        punct = "/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~" + '""\u201c\u201d\u2019' + '\u221e\u03b8\xf7\u03b1\u2022\xe0\u2212\u03b2\u2205\xb3\u03c0\u2018\u20b9\xb4\xb0\xa3\u20ac\\\xd7\u2122\u221a\xb2\u2014\u2013&'\n        puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']', '>', '%', '=',\n                  '#', '*', '+', '\\\\', '\u2022', '~', '@', '\xa3',\n                  '\xb7', '_', '{', '}', '\xa9', '^', '\xae', '`', '<', '\u2192', '\xb0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\xd7', '\xa7', '\u2033', '\u2032',\n                  '\xc2', '\u2588', '\xbd', '\xe0', '\u2026',\n                  '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\xe2', '\u25ba', '\u2212', '\xa2', '\xb2', '\xac', '\u2591', '\xb6', '\u2191', '\xb1', '\xbf', '\u25be', '\u2550', '\xa6', '\u2551',\n                  '\u2015', '\xa5', '\u2593', '\u2014', '\u2039', '\u2500',\n                  '\u2592', '\uff1a', '\xbc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\xa8', '\u2584', '\u266b', '\u2606', '\xe9', '\xaf', '\u2666', '\xa4', '\u25b2', '\xe8',\n                  '\xb8', '\xbe', '\xc3', '\u22c5', '\u2018', '\u221e',\n                  '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\xbb', '\uff0c', '\u266a', '\u2569', '\u255a', '\xb3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\xef',\n                  '\xd8', '\xb9', '\u2264', '\u2021', '\u221a', ]\n        punct_mapping = {"\u2018": "'", "\u20b9": "e", "\xb4": "'", "\xb0": "", "\u20ac": "e", "\u2122": "tm", "\u221a": " sqrt ", "\xd7": "x", "\xb2": "2",\n                         "\u2014": "-", "\u2013": "-", "\u2019": "'", "_": "-", "`": "'", '\u201c': '"', '\u201d': '"', '\u201c': '"', "\xa3": "e",\n                         '\u221e': 'infinity', '\u03b8': 'theta', '\xf7': '/', '\u03b1': 'alpha', '\u2022': '.', '\xe0': 'a', '\u2212': '-',\n                         '\u03b2': 'beta', '\u2205': '', '\xb3': '3', '\u03c0': 'pi', }\n        for p in punct_mapping:\n            text = text.replace(p, punct_mapping[p])\n        for p in set(list(punct) + puncts) - set(punct_mapping.keys()):\n            text = text.replace(p, f' {p} ')\n\n        specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '',\n                    '\u0939\u0948': ''}\n        for s in specials:\n            text = text.replace(s, specials[s])\n        return text\n\n    @staticmethod\n    def clean_spelling(text, case_sensitive=False):\n        misspell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n                         'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                         'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n                         'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                         'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n                         'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n                         'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n                         'mastrubate': 'masturbate', "mastrubating": 'masturbating', 'pennis': 'penis',\n                         'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n                         '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n                         "whst": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                         'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n        for word in misspell_dict.keys():\n            if case_sensitive:\n                text = text.replace(word, misspell_dict[word])\n            else:\n                re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n                text = re_insensitive.sub(misspell_dict[word], text)\n        return text\n\n    @staticmethod\n    def clean_acronyms(text, case_sensitive=False):\n        acronym_dict = {'upsc': 'union public service commission',\n                        'aiims': 'all india institute of medical sciences',\n                        'cgl': 'graduate level examination',\n                        'icse': 'indian school certificate exam',\n                        'iiit': 'indian institute of information technology',\n                        'cgpa': 'cumulative grade point average',\n                        'ielts': 'international english language training system',\n                        'ncert': 'national council of education research training',\n                        'isro': 'indian space research organization',\n                        'clat': 'common law admission test',\n                        'ibps': 'institute of banking personnel selection',\n                        'iiser': 'indian institute of science education and research',\n                        'iisc': 'indian institute of science',\n                        'iims': 'indian institutes of management',\n                        'cpec': 'china pakistan economic corridor'\n\n                        }\n        for word in acronym_dict.keys():\n            if case_sensitive:\n                text = text.replace(word, acronym_dict[word])\n            else:\n                re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n                text = re_insensitive.sub(acronym_dict[word], text)\n        return text\n\n    @staticmethod\n    def clean_non_dictionary(text, case_sensitive=False):\n        replace_dict = {'quorans': 'users',\n                        'quoran': 'user',\n                        'jio': 'phone manufacturer',\n                        'manipal': 'city',\n                        'bitsat': 'exam',\n                        'mtech': 'technical university',\n                        'pilani': 'town',\n                        'bhu': 'university',\n                        'h1b': 'visa',\n                        'redmi': 'phone manufacturer',\n                        'nift': 'university',\n                        'kvpy': 'exam',\n                        'thanos': 'comic villain',\n                        'paytm': 'payment system',\n                        'comedk': 'medical consortium',\n                        'accenture': 'management consulting company',\n                        'llb': 'bachelor of laws',\n                        'ignou': 'university',\n                        'dtu': 'university',\n                        'aadhar': 'social number',\n                        'lenovo': 'computer manufacturer',\n                        'gmat': 'exam',\n                        'kiit': 'institute of technology',\n                        'shopify': 'music streaming',\n                        'fitjee': 'exam',\n                        'kejriwal': 'politician',\n                        'wbjee': 'exam',\n                        'pgdm': 'master of business administration',\n                        'trudeau': 'politician',\n                        'nri': 'research institute',\n                        'deloitte': 'accounting company',\n                        'jinping': 'politician',\n                        'bcom': 'bachelor of commerce',\n                        'mcom': 'masters of commerce',\n                        'virat': 'athlete',\n                        'kcet': 'television network',\n                        'wipro': 'information technology company',\n                        'articleship': 'internship',\n                        'comey': 'law enforcement director',\n                        'jnu': 'university',\n                        'acca': 'chartered accountants',\n                        'aakash': 'phone manufacturer',\n                        'brexit': 'british succession',\n                        'crypto': 'digital currency',\n                        'cryptocurrency': 'digital currency',\n                        'cryptocurrencies': 'digital currencies',\n                        'etherium': 'digital currency',\n                        'bitcoin': 'digital currency',\n                        'viteee': 'exam',\n                        'iocl': 'indian oil company',\n                        'nmims': 'management school',\n                        'rohingya': 'myanmar people',\n                        'fortnite': 'videogame',\n                        'upes': 'university',\n                        'nsit': 'university',\n                        'coinbase': 'digital currency exchange'\n                        }\n        for word in replace_dict.keys():\n            if case_sensitive:\n                text = text.replace(word, replace_dict[word])\n            else:\n                re_insensitive = re.compile(re.escape(word), re.IGNORECASE)\n                text = re_insensitive.sub(replace_dict[word], text)\n        return text\n\n    @staticmethod\n    def clean_numbers(text, min_magnitude=2, max_magnitude=10):\n        for n in range(min_magnitude, max_magnitude):\n            text = re.sub('[0-9]{' + str(n) + '}', '#'*n, text)\n        return text\n\n    def add_features(self, df):\n        df['question_text'] = df['question_text'].apply(lambda x: str(x))\n        df['total_length'] = df['question_text'].apply(len)\n        df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n        df['caps_vs_length'] = df.apply(lambda row: float(row['capitals']) / float(row['total_length']),\n                                        axis=1)\n        df['num_words'] = df.question_text.str.count('\\S+')\n        df['num_unique_words'] = df['question_text'].apply(\n            lambda comment: len(set(w for w in comment.split())))\n        df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n        df['caps_vs_length'] = df['caps_vs_length'].fillna(0)\n        df['words_vs_unique'] = df['words_vs_unique'].fillna(0)\n        df['num_exclamation_marks'] = df['question_text'].apply(lambda comment: comment.count('!'))\n        df['num_question_marks'] = df['question_text'].apply(lambda comment: comment.count('?'))\n        df['num_smilies'] = df['question_text'].apply(\n            lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n        self.custom_features = ['total_length', 'capitals', 'caps_vs_length', 'num_words',\n                                'num_unique_words', 'words_vs_unique', 'caps_vs_length',\n                                'words_vs_unique', 'num_exclamation_marks', 'num_question_marks', 'num_smilies']\n        return df\n\n    def scale_features(self):\n        self.feature_scaler = StandardScaler()\n        features = self.train_df[self.custom_features]\n        test_features = self.test_df[self.custom_features]\n        self.feature_scaler.fit(features)\n        self.train_df[self.custom_features] = self.feature_scaler.transform(features)\n        self.test_df[self.custom_features] = self.feature_scaler.transform(test_features)\n\n    def get_train_vocab(self):\n        sentences = self.train_df['question_text'].apply(lambda x: x.split()).values\n        vocab = {}\n        for sentence in sentences:\n            for word in sentence:\n                try:\n                    vocab[word] += 1\n                except KeyError:\n                    vocab[word] = 1\n        return vocab\n''')
    __stickytape_write_module('''src/config.py''', '''"""Model and hyperparameter configurations. This file is generated by generate_config(). Do not edit!"""\n\n\nrandom_state = 2018\n\nconfig_data = {'max_feature': 200000,\n 'max_seq_len': 50,\n 'preprocess': {'correct_spelling': True,\n                'lower_case': False,\n                'remove_contractions': True,\n                'remove_specials': True,\n                'remove_stop_words': False,\n                'replace_acronyms': True,\n                'replace_non_words': True,\n                'replace_numbers': False,\n                'use_custom_features': True},\n 'test_size': 0.1}\n\nconfig_insincere_model = {'callbacks': {'checkpoint': {'mode': 'max',\n                              'monitor': 'val_f1_score',\n                              'save_best_only': True,\n                              'verbose': True},\n               'early_stopping': {'mode': 'max',\n                                  'monitor': 'val_f1_score',\n                                  'patience': 2,\n                                  'verbose': True}},\n 'fit': {'batch_size': 1536,\n         'epochs': 5,\n         'pseudo_labels': False,\n         'save_curve': True},\n 'predict': {'batch_size': 2048, 'verbose': True}}\n\nconfig_lrfinder = {'loss_smoothing_beta': 0.98,\n 'lr_scale': 'exp',\n 'maximum_lr': 10.0,\n 'minimum_lr': 1e-06,\n 'save_dir': None,\n 'stopping_criterion_factor': 4,\n 'validation_sample_rate': 5,\n 'verbose': True}\n\nconfig_one_cycle = {'end_percentage': 0.1,\n 'max_lr': 0.1,\n 'maximum_momentum': 0.95,\n 'minimum_momentum': 0.85,\n 'scale_percentage': None,\n 'verbose': True}\n\nconfig_main = {'dev_size': None,\n 'embedding_files': ['../input/embeddings/glove.840B.300d/glove.840B.300d.txt',\n                     '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'],\n 'models': [{'args': None, 'class': 'LSTMModelAttention'},\n            {'args': None, 'class': 'CNNModel'}]}\n\n''')
    __stickytape_write_module('''src/__init__.py''', '''''')
    __stickytape_write_module('''src/Embedding.py''', '''import gc\nimport time\n\nimport logging\nimport numpy as np\nimport operator\nimport traceback\nfrom gensim.models import KeyedVectors\n\n\nclass Embedding:\n    def __init__(self, data):\n        self.embeddings_index = None\n        self.nb_words = None\n        self.embeddings_index = None\n        self.embed_size = None\n        self.embedding_matrix = None\n        self.data = data\n        self.name = None\n\n    def load(self, embedding_file='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'):\n        logging.info("loading embedding : " + embedding_file)\n        self.name = embedding_file.split('/')[3]\n\n        def get_coefs(word, *arr):\n            return word, np.asarray(arr, dtype='float32')\n        if "wiki-news" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" "))\n                                         for i, o in enumerate(open(embedding_file)) if len(o) > 100)\n        elif "glove" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" ")) for i, o in enumerate(open(embedding_file)))\n        elif "paragram" in embedding_file:\n            self.embeddings_index = dict(get_coefs(*o.split(" ")) for i, o in\n                                         enumerate(open(embedding_file, encoding="utf8", errors='ignore'))\n                                         if len(o) > 100)\n        elif "GoogleNews" in embedding_file:\n            self.embeddings_index = {}\n            wv_from_bin = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n            for i, (word, vector) in enumerate(zip(wv_from_bin.vocab, wv_from_bin.vectors)):\n                coefs = np.asarray(vector, dtype='float32')\n                self.embeddings_index[word] = coefs\n        else:\n            raise ValueError("Unsupported embedding file: " + embedding_file)\n\n        try:\n            all_embs = np.stack(self.embeddings_index.values())\n        except ValueError as e:\n            logging.error(e)\n            tb = traceback.format_exc()\n            logging.error(tb)\n            logging.debug("len(self.embeddings_index.values()): "\n                          + str(len(self.embeddings_index.values())))\n            logging.debug("type(self.embeddings_index.values()[0]): "\n                          + str(type(list(self.embeddings_index.values())[0])))\n            logging.debug("first few self.embeddings_index.values(): "\n                          + str(list(self.embeddings_index.values())[:5]))\n            raise\n        emb_mean, emb_std = all_embs.mean(), all_embs.std()\n        self.embed_size = all_embs.shape[1]\n\n        word_index = self.data.tokenizer.word_index\n        self.nb_words = min(self.data.max_feature, len(word_index))\n        self.embedding_matrix = np.random.normal(emb_mean, emb_std, (self.nb_words, self.embed_size))\n        for word, i in word_index.items():\n            if i >= self.nb_words:\n                continue\n            embedding_vector = self.embeddings_index.get(word)\n            if embedding_vector is not None:\n                self.embedding_matrix[i] = embedding_vector\n        return self.embedding_matrix\n\n    def check_coverage(self, vocab):\n        known_words = {}\n        unknown_words = {}\n        nb_known_words = 0\n        nb_unknown_words = 0\n        for word in vocab.keys():\n            try:\n                known_words[word] = self.embeddings_index[word]\n                nb_known_words += vocab[word]\n            except:\n                unknown_words[word] = vocab[word]\n                nb_unknown_words += vocab[word]\n                pass\n\n        logging.info('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n        logging.info('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n        unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n        return unknown_words\n\n    def cleanup(self):\n        logging.info("Releasing memory...")\n        try:\n            del self.embeddings_index, self.embedding_matrix\n            gc.collect()\n            time.sleep(10)\n        except AttributeError:\n            logging.warning('embeddings index not found. They were probably already cleaned up.')\n\n    def cleanup_index(self):\n        logging.info("Releasing memory...")\n        try:\n            del self.embeddings_index\n            gc.collect()\n            time.sleep(10)\n        except AttributeError:\n            logging.warning('embeddings index not found. They were probably already cleaned up.')\n''')
    __stickytape_write_module('''src/Ensemble.py''', '''import logging\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\nclass Ensemble:\n    def __init__(self, models):\n        self.models = models\n\n    def predict_average(self, X):\n        logging.info("Predicting with ensemble average, size=" + str(len(self.models)))\n        predictions = list()\n        for m in self.models:\n            predictions.append(m.predict(X))\n            logging.debug(type(predictions[-1]))\n        avg_pred = np.mean(predictions, axis=0)\n        return avg_pred\n\n    def predict_linear_regression(self, X_train, y_train, X_predict):\n        predictions_train = [model.predict(X_train) for model in self.models]\n        X = np.asarray(predictions_train)\n        X = X[..., 0]\n        reg = LinearRegression().fit(X.T, y_train)\n        predictions_predict = [model.predict(X_predict) for model in self.models]\n        prediction_lin_reg = np.sum([predictions_predict[i] * reg.coef_[i]\n                                     for i in range(len(predictions_predict))], axis=0)\n        return prediction_lin_reg''')
    __stickytape_write_module('''src/Models.py''', '''from keras.layers import Bidirectional, CuDNNLSTM, Reshape, Conv2D, MaxPool2D, \\\n    Concatenate, Flatten, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\nfrom keras.layers import Dense, Input, Embedding as EmbeddingLayer, Dropout\nfrom keras.models import Model\n\nfrom src.Attention import Attention\nfrom src.InsincereModel import InsincereModel\n\n\nclass LSTMModel(InsincereModel):\n    def define_model(self, model_config=None):\n        if model_config is None:\n            model_config = self.default_config()\n        inp = Input(shape=(self.data.maxlen,))\n        x = EmbeddingLayer(self.embedding.nb_words,\n                           self.embedding.embed_size,\n                           weights=[self.embedding.embedding_matrix],\n                           trainable=False)(inp)\n        x = Bidirectional(CuDNNLSTM(model_config['lstm_size'], return_sequences=True))(x)\n        avg_pool = GlobalAveragePooling1D()(x)\n        max_pool = GlobalMaxPooling1D()(x)\n        concat_layers = [avg_pool, max_pool]\n        inputs = [inp]\n        if self.data.custom_features:\n            inp_features = Input(shape=(len(self.data.custom_features),))\n            concat_layers += [inp_features]\n            inputs += [inp_features]\n        x = concatenate([avg_pool, max_pool, inp_features])\n        x = Dense(model_config['dense_size'], activation="relu")(x)\n        x = Dropout(model_config['dropout_rate'])(x)\n        x = Dense(1, activation="sigmoid")(x)\n        self.model = Model(inputs=inputs, outputs=x)\n        self.model.compile(loss=self.loss, optimizer='sgd', metrics=['accuracy', self.f1_score])\n        return self.model\n\n    def default_config(self):\n        config = {'lstm_size': 64,\n                  'dense_size': 64,\n                  'dropout_rate': 0.1,\n                  }\n        return config\n\n\nclass LSTMModelAttention(InsincereModel):\n    def define_model(self, model_config=None):\n        if model_config is None:\n            model_config = self.default_config()\n        inp = Input(shape=(self.data.maxlen,))\n        x = EmbeddingLayer(self.embedding.nb_words,\n                           self.embedding.embed_size,\n                           weights=[self.embedding.embedding_matrix],\n                           trainable=False)(inp)\n        x = Bidirectional(CuDNNLSTM(model_config['lstm_size'], return_sequences=True))(x)\n        x = Attention(self.data.maxlen)(x)\n        inputs = [inp]\n        if self.data.custom_features:\n            inp_features = Input(shape=(len(self.data.custom_features),))\n            x = concatenate([x, inp_features])\n            x = Dense(model_config['dense_size_1'], activation="relu")(x)\n            inputs += [inp_features]\n        x = Dense(model_config['dense_size_2'], activation="relu")(x)\n        x = Dropout(model_config['dropout_rate'])(x)\n        x = Dense(1, activation="sigmoid")(x)\n        self.model = Model(inputs=inputs, outputs=x)\n        self.model.compile(loss=self.loss, optimizer='sgd', metrics=['accuracy', self.f1_score])\n        return self.model\n\n    def default_config(self):\n        config = {'lstm_size': 64,\n                  'dense_size_1': 32,\n                  'dense_size_2': 16,\n                  'dropout_rate': 0.1,\n                  }\n        return config\n\n\nclass CNNModel(InsincereModel):\n    def define_model(self, model_config=None):\n        if model_config is None:\n            model_config = self.default_config()\n        filter_sizes = model_config['filter_sizes']\n        num_filters = model_config['num_filters']\n        inp = Input(shape=(self.data.maxlen,))\n        x = EmbeddingLayer(self.embedding.nb_words, self.embedding.embed_size,\n                           weights=[self.embedding.embedding_matrix])(inp)\n        x = Reshape((self.data.maxlen, self.embedding.embed_size, 1))(x)\n        maxpool_pool = []\n        inputs = [inp]\n        for i in range(len(filter_sizes)):\n            conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], self.embedding.embed_size),\n                          kernel_initializer='he_normal', activation='elu')(x)\n            maxpool_pool.append(MaxPool2D(pool_size=(self.data.maxlen - filter_sizes[i] + 1, 1))(conv))\n        z = Concatenate(axis=1)(maxpool_pool)\n        z = Flatten()(z)\n        z = Dropout(model_config['dropout_rate'])(z)\n        if self.data.custom_features:\n            inp_features = Input(shape=(len(self.data.custom_features),))\n            z = concatenate([z, inp_features])\n            z = Dense(model_config['dense_size'], activation='relu')(z)\n            inputs += [inp_features]\n        outp = Dense(1, activation="sigmoid")(z)\n        self.model = Model(inputs=inputs, outputs=outp)\n        self.model.compile(loss=self.loss, optimizer='sgd', metrics=['accuracy', self.f1_score])\n\n        return self.model\n\n    def default_config(self):\n        config = {'filter_sizes': [1, 2, 3, 5],\n                  'num_filters': 36,\n                  'dropout_rate': 0.1,\n                  'dense_size': 32}\n        return config''')
    __stickytape_write_module('''src/Attention.py''', '''import keras.backend as K\nfrom keras.engine import Layer\nfrom keras.layers import initializers, regularizers, constraints\n\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        self.built = False\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], self.features_dim\n''')
    __stickytape_write_module('''src/InsincereModel.py''', '''import keras.backend as K\nimport logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom src.Embedding import Embedding\nfrom src.LRFinder import LRFinder\nfrom src.OneCycleLR import OneCycleLR\nfrom src.config import config_insincere_model\n\n\nclass InsincereModel:\n    def __init__(self, data, name=None, loss='binary_crossentropy'):\n        self.data = data\n        self.name = name\n        self.embedding = None\n        self.model = None\n        self.history = None\n        self.loss = loss\n        self.lr_finder = None\n        self.config = config_insincere_model\n\n    def load_embedding(self, embedding_file='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'):\n        self.embedding = Embedding(self.data)\n        self.embedding.load(embedding_file)\n\n    def set_embedding(self, embedding):\n        if type(embedding) is str:\n            self.load_embedding(embedding)\n        else:\n            self.embedding = embedding\n\n    def blend_embeddings(self, embeddings, cleanup=False):\n        """Average embedding matrix given list of embedding files."""\n        if self.embedding is None:\n            self.set_embedding(embeddings[0])\n        embedding_matrices = list()\n        for emb in embeddings:\n            embedding_matrices.append(emb.embedding_matrix)\n        blend = np.mean(embedding_matrices, axis=0)\n        self.embedding.embedding_matrix = blend\n        if cleanup:\n            for e in embeddings:\n                e.cleanup()\n        return blend\n\n    def concat_embeddings(self, embeddings, cleanup=False):\n        if self.embedding is None:\n            self.set_embedding(embeddings[0])\n        self.embedding.embedding_matrix = np.concatenate(tuple([e.embedding_matrix for e in embeddings]), axis=1)\n        self.embedding.embed_size = self.embedding.embedding_matrix.shape[1]\n        if cleanup:\n            for e in embeddings:\n                e.cleanup()\n        return self.embedding.embedding_matrix\n\n    @staticmethod\n    def f1_score(y_true, y_pred):\n        def recall(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n            recall = true_positives / (possible_positives + K.epsilon())\n            return recall\n\n        def precision(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n            precision = true_positives / (predicted_positives + K.epsilon())\n            return precision\n\n        precision = precision(y_true, y_pred)\n        recall = recall(y_true, y_pred)\n        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n    def define_model(self):\n        raise NotImplementedError\n\n    def print(self):\n        print(self.model.summary())\n\n    def _get_callbacks(self, epochs, batch_size):\n        config = self.config.get('callbacks')\n        num_samples = self.data.train_X.shape[0]\n        self.lr_finder = LRFinder(num_samples, batch_size)\n        lr_manager = OneCycleLR(num_samples, epochs, batch_size)\n        check_point = ModelCheckpoint('model.hdf5',\n                                      monitor=config.get('checkpoint').get('monitor'),\n                                      mode=config.get('checkpoint').get('mode'),\n                                      verbose=config.get('checkpoint').get('verbose'),\n                                      save_best_only=config.get('checkpoint').get('save_best_only'))\n        early_stop = EarlyStopping(monitor=config.get('early_stopping').get('monitor'),\n                                   mode=config.get('early_stopping').get('mode'),\n                                   patience=config.get('early_stopping').get('patience'),\n                                   verbose=config.get('early_stopping').get('verbose'))\n        return [self.lr_finder, lr_manager, check_point, early_stop]\n\n    def fit(self,\n            train_indices=None,\n            val_indices=None,\n            curve_file_suffix=None):\n        logging.info("Fitting model...")\n        config = self.config.get('fit')\n        if config.get('pseudo_labels'):\n            train_x, train_y = self.data.full_X, self.data.full_y\n            val_x, val_y = self.data.val_X, self.data.val_y\n            if self.data.custom_features:\n                train_features, val_features = self.data.train_features, self.data.test_features\n        else:\n            if train_indices is not None:\n                train_x = self.data.train_X[train_indices]\n                train_y = self.data.train_y[train_indices]\n                if self.data.custom_features:\n                    train_features = self.data.train_features[train_indices]\n            else:\n                train_x = self.data.train_X\n                train_y = self.data.train_y\n                if self.data.custom_features:\n                    train_features = self.data.train_features\n            if val_indices is not None:\n                val_x = self.data.train_X[val_indices]\n                val_y = self.data.train_y[val_indices]\n                if self.data.custom_features:\n                    val_features = self.data.train_features[val_indices]\n            else:\n                val_x = self.data.val_X\n                val_y = self.data.val_y\n                if self.data.custom_features:\n                    val_features = self.data.val_features\n        callbacks = self._get_callbacks(config.get('epochs'), config.get('batch_size'))\n        if self.data.custom_features:\n            train_x = [train_x, train_features]\n            val_x = [val_x, val_features]\n        self.history = self.model.fit(train_x,\n                                      train_y,\n                                      batch_size=config.get('batch_size'),\n                                      epochs=config.get('epochs'),\n                                      validation_data=(val_x, val_y),\n                                      callbacks=callbacks)\n        if config.get('save_curve'):\n            self.lr_finder.plot_schedule(filename="lr_schedule_" + str(self.name) + ".png")\n            filename = 'training_curve'\n            if self.name:\n                filename += '_' + self.name\n            if curve_file_suffix:\n                filename += '_' + curve_file_suffix\n            filename += '.png'\n            self.print_curve(filename)\n\n    def print_curve(self, filename='training_curve.png'):\n        plt.plot(self.history.history['loss'])\n        plt.plot(self.history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'val'], loc='best')\n        plt.savefig(filename)\n        plt.close()\n\n    def predict(self, x):\n        logging.info("Predicting ...")\n        batch_size = self.config.get('predict').get('batch_size')\n        verbose = self.config.get('predict').get('verbose')\n        prediction = self.model.predict(x, batch_size=batch_size, verbose=verbose)\n        return prediction\n\n    def cleanup(self):\n        self.embedding.cleanup()\n''')
    __stickytape_write_module('''src/LRFinder.py''', '''import keras.backend as K\nimport numpy as np\nimport os\nimport warnings\nfrom keras.callbacks import Callback\n\nfrom src.config import config_lrfinder as config\n\n\nclass LRFinder(Callback):\n\n    def __init__(self, num_samples, batch_size, validation_data=None):\n        """\n        This class uses the Cyclic Learning Rate history to find a\n        set of learning rates that can be good initializations for the\n        One-Cycle training proposed by Leslie Smith in the paper referenced\n        below.\n        A port of the Fast.ai implementation for Keras.\n        # Note\n        This requires that the model be trained for exactly 1 epoch. If the model\n        is trained for more epochs, then the metric calculations are only done for\n        the first epoch.\n        # Interpretation\n        Upon visualizing the loss plot, check where the loss starts to increase\n        rapidly. Choose a learning rate at somewhat prior to the corresponding\n        position in the plot for faster convergence. This will be the maximum_lr lr.\n        Choose the max value as this value when passing the `max_val` argument\n        to OneCycleLR callback.\n        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n        # Arguments:\n            num_samples: Integer. Number of samples in the dataset.\n            batch_size: Integer. Batch size during training.\n            minimum_lr: Float. Initial learning rate (and the minimum).\n            maximum_lr: Float. Final learning rate (and the maximum).\n            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n                scaling for each update to the learning rate during subsequent\n                batches. Choose 'exp' for large range and 'linear' for small range.\n            validation_data: Requires the validation dataset as a tuple of\n                (X, y) belonging to the validation set. If provided, will use the\n                validation set to compute the loss metrics. Else uses the training\n                batch loss. Will warn if not provided to alert the user.\n            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n                validation set per iteration of the LRFinder. Larger number of\n                samples will reduce the variance but will take longer time to execute\n                per batch.\n                If Positive > 0, will sample from the validation dataset\n                If Megative, will use the entire dataset\n            stopping_criterion_factor: Integer or None. A factor which is used\n                to measure large increase in the loss value during training.\n                Since callbacks cannot stop training of a model, it will simply\n                stop logging the additional values from the epochs after this\n                stopping criterion has been met.\n                If None, this check will not be performed.\n            loss_smoothing_beta: Float. The smoothing factor for the moving\n                average of the loss function.\n            save_dir: Optional, String. If passed a directory path, the callback\n                will save the running loss and learning rates to two separate numpy\n                arrays inside this directory. If the directory in this path does not\n                exist, they will be created.\n            verbose: Whether to print the learning rate after every batch of training.\n        # References:\n            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n        """\n        super(LRFinder, self).__init__()\n\n        minimum_lr = config.get('minimum_lr')\n        maximum_lr = config.get('maximum_lr')\n        lr_scale = config.get('lr_scale')\n        validation_sample_rate = config.get('validation_sample_rate')\n        stopping_criterion_factor = config.get('stopping_criterion_factor')\n        loss_smoothing_beta = config.get('loss_smoothing_beta')\n        save_dir = config.get('save_dir')\n        verbose = config.get('verbose')\n        if lr_scale not in ['exp', 'linear']:\n            raise ValueError("`lr_scale` must be one of ['exp', 'linear']")\n\n        if validation_data is not None:\n            self.validation_data = validation_data\n            self.use_validation_set = True\n\n            if validation_sample_rate > 0 or validation_sample_rate < 0:\n                self.validation_sample_rate = validation_sample_rate\n            else:\n                raise ValueError("`validation_sample_rate` must be a positive or negative integer other than o")\n        else:\n            self.use_validation_set = False\n            self.validation_sample_rate = 0\n\n        self.num_samples = num_samples\n        self.batch_size = batch_size\n        self.initial_lr = minimum_lr\n        self.final_lr = maximum_lr\n        self.lr_scale = lr_scale\n        self.stopping_criterion_factor = stopping_criterion_factor\n        self.loss_smoothing_beta = loss_smoothing_beta\n        self.save_dir = save_dir\n        self.verbose = verbose\n\n        self.num_batches_ = num_samples // batch_size - 1\n        self.current_lr_ = minimum_lr\n\n        if lr_scale == 'exp':\n            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (1. / float(self.num_batches_))\n        else:\n            extra_batch = int((num_samples % batch_size) != 0)\n            self.lr_multiplier_ = np.linspace(minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n\n        # If negative, use entire validation set\n        if self.validation_sample_rate < 0:\n            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n\n        self.current_batch_ = 0\n        self.current_epoch_ = 0\n        self.best_loss_ = 1e6\n        self.running_loss_ = 0.\n\n        self.history = {}\n\n    def on_train_begin(self, logs=None):\n        self.current_epoch_ = 1\n        K.set_value(self.model.optimizer.lr, self.initial_lr)\n\n        warnings.simplefilter("ignore")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.current_batch_ = 0\n\n        if self.current_epoch_ > 1:\n            warnings.warn("\\n\\nLearning rate finder should be used only with a single epoch. "\n                          "Hereafter, the callback will not measure the losses.\\n\\n")\n\n    def on_batch_begin(self, batch, logs=None):\n        self.current_batch_ += 1\n\n    def on_batch_end(self, batch, logs=None):\n        if self.current_epoch_ > 1:\n            return\n\n        if self.use_validation_set:\n            X, Y = self.validation_data[0], self.validation_data[1]\n\n            # use 5 random batches from test set for fast approximate of loss\n            num_samples = self.batch_size * self.validation_sample_rate\n\n            if num_samples > X.shape[0]:\n                num_samples = X.shape[0]\n\n            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n            x = X[idx]\n            y = Y[idx]\n\n            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n            loss = values[0]\n        else:\n            loss = logs['loss']\n\n        # smooth the loss value and bias correct\n        running_loss = self.loss_smoothing_beta * loss + (1. - self.loss_smoothing_beta) * loss\n        running_loss = running_loss / (1. - self.loss_smoothing_beta ** self.current_batch_)\n\n        # stop logging if loss is too large\n        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n                running_loss > self.stopping_criterion_factor * self.best_loss_):\n\n            if self.verbose:\n                print(" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)" % (\n                    self.stopping_criterion_factor, self.best_loss_\n                ))\n            return\n\n        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n            self.best_loss_ = running_loss\n\n        current_lr = K.get_value(self.model.optimizer.lr)\n\n        self.history.setdefault('running_loss_', []).append(running_loss)\n        if self.lr_scale == 'exp':\n            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n        else:\n            self.history.setdefault('log_lrs', []).append(current_lr)\n\n        # compute the lr for the next batch and update the optimizer lr\n        if self.lr_scale == 'exp':\n            current_lr *= self.lr_multiplier_\n        else:\n            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n\n        K.set_value(self.model.optimizer.lr, current_lr)\n\n        # save the other metrics as well\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        if self.verbose:\n            if self.use_validation_set:\n                print(" - LRFinder: val_loss: %1.4f - lr = %1.8f " % (values[0], current_lr))\n            else:\n                print(" - LRFinder: lr = %1.8f " % current_lr)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.save_dir is not None and self.current_epoch_ <= 1:\n            if not os.path.exists(self.save_dir):\n                os.makedirs(self.save_dir)\n\n            losses_path = os.path.join(self.save_dir, 'losses.npy')\n            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n\n            np.save(losses_path, self.losses)\n            np.save(lrs_path, self.lrs)\n\n            if self.verbose:\n                print("\\tLR Finder : Saved the losses and learning rate values in path : {%s}" % (self.save_dir))\n\n        self.current_epoch_ += 1\n\n        warnings.simplefilter("default")\n\n    def plot_schedule(self, filename="lr_schedule.png", clip_beginning=None, clip_endding=None):\n        """\n        Plots the schedule from the callback itself.\n        # Arguments:\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        """\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print("Matplotlib not found. Please use `pip install matplotlib` first.")\n            return\n\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses = self.losses\n        lrs = self.lrs\n\n        if clip_beginning:\n            losses = losses[clip_beginning:]\n            lrs = lrs[clip_beginning:]\n\n        if clip_endding:\n            losses = losses[:clip_endding]\n            lrs = lrs[:clip_endding]\n\n        plt.plot(lrs, losses)\n        plt.gca().set_yscale('log')\n        plt.title('Learning rate vs Loss')\n        plt.xlabel('log(learning rate)')\n        plt.ylabel('log(loss)')\n        plt.savefig(filename)\n\n    @classmethod\n    def restore_schedule_from_dir(cls, directory, clip_beginning=None, clip_endding=None):\n        """\n        Loads the training history from the saved numpy files in the given directory.\n        # Arguments:\n            directory: String. Path to the directory where the serialized numpy\n                arrays of the loss and learning rates are saved.\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        Returns:\n            tuple of (losses, learning rates)\n        """\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses_path = os.path.join(directory, 'losses.npy')\n        lrs_path = os.path.join(directory, 'lrs.npy')\n\n        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n            print("%s and %s could not be found at directory : {%s}" % (\n                losses_path, lrs_path, directory\n            ))\n\n            losses = None\n            lrs = None\n\n        else:\n            losses = np.load(losses_path)\n            lrs = np.load(lrs_path)\n\n            if clip_beginning:\n                losses = losses[clip_beginning:]\n                lrs = lrs[clip_beginning:]\n\n            if clip_endding:\n                losses = losses[:clip_endding]\n                lrs = lrs[:clip_endding]\n\n        return losses, lrs\n\n    @classmethod\n    def plot_schedule_from_file(cls, directory, clip_beginning=None, clip_endding=None):\n        """\n        Plots the schedule from the saved numpy arrays of the loss and learning\n        rate values in the specified directory.\n        # Arguments:\n            directory: String. Path to the directory where the serialized numpy\n                arrays of the loss and learning rates are saved.\n            clip_beginning: Integer or None. If positive integer, it will\n                remove the specified portion of the loss graph to remove the large\n                loss values in the beginning of the graph.\n            clip_endding: Integer or None. If negative integer, it will\n                remove the specified portion of the ending of the loss graph to\n                remove the sharp increase in the loss values at high learning rates.\n        """\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print("Matplotlib not found. Please use `pip install matplotlib` first.")\n            return\n\n        losses, lrs = cls.restore_schedule_from_dir(directory,\n                                                    clip_beginning=clip_beginning,\n                                                    clip_endding=clip_endding)\n\n        if losses is None or lrs is None:\n            return\n        else:\n            plt.plot(lrs, losses)\n            plt.title('Learning rate vs Loss')\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.show()\n\n    @property\n    def lrs(self):\n        return np.array(self.history['log_lrs'])\n\n    @property\n    def losses(self):\n        return np.array(self.history['running_loss_'])\n''')
    __stickytape_write_module('''src/OneCycleLR.py''', '''import keras.backend as K\nfrom keras.callbacks import Callback\n\nfrom src.config import config_one_cycle as config\n\n\nclass OneCycleLR(Callback):\n    def __init__(self, num_samples, num_epochs, batch_size):\n        """ This callback implements a cyclical learning rate policy (CLR).\n        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n        After the completion of 1 cycle, the learning rate will decrease rapidly to\n        100th its initial lowest value.\n        # Arguments:\n            num_samples: Integer. Number of sample points in the dataset\n            num_epochs: Integer. Number of training epochs\n            batch_size: Integer. Batch size per training epoch\n            max_lr: Float. Initial learning rate. This also sets the\n                starting learning rate (which will be 10x smaller than\n                this), and will increase to this value during the first cycle.\n            end_percentage: Float. The percentage of all the epochs of training\n                that will be dedicated to sharply decreasing the learning\n                rate after the completion of 1 cycle. Must be between 0 and 1.\n            scale_percentage: Float or None. If float, must be between 0 and 1.\n                If None, it will compute the scale_percentage automatically\n                based on the `end_percentage`.\n            maximum_momentum: Optional. Sets the maximum momentum (initial)\n                value, which gradually drops to its lowest value in half-cycle,\n                then gradually increases again to stay constant at this max value.\n                Can only be used with SGD Optimizer.\n            minimum_momentum: Optional. Sets the minimum momentum at the end of\n                the half-cycle. Can only be used with SGD Optimizer.\n            verbose: Bool. Whether to print the current learning rate after every\n                epoch.\n        # Reference\n            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n        """\n        super(OneCycleLR, self).__init__()\n\n        end_percentage = config.get('end_percentage')\n        scale_percentage = config.get('scale_percentage')\n        maximum_momentum = config.get('maximum_momentum')\n        minimum_momentum = config.get('minimum_momentum')\n        max_lr = config.get('max_lr')\n        verbose = config.get('verbose')\n\n        if end_percentage < 0. or end_percentage > 1.:\n            raise ValueError("`end_percentage` must be between 0 and 1")\n\n        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n            raise ValueError("`scale_percentage` must be between 0 and 1")\n\n        self.num_samples = num_samples\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.num_samples_per_batch = max(num_samples // batch_size, 3)\n        self.initial_lr = max_lr\n        self.end_percentage = end_percentage\n        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n        self.max_momentum = maximum_momentum\n        self.min_momentum = minimum_momentum\n        self.verbose = verbose\n\n        self.num_iterations = self.num_epochs * self.num_samples_per_batch\n        self.mid_cycle_id = int(self.num_iterations * ((1. - end_percentage)) / float(2))\n\n        if self.max_momentum is not None and self.min_momentum is not None:\n            self._update_momentum = True\n        else:\n            self._update_momentum = False\n\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def _reset(self):\n        """\n        Reset the callback.\n        """\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def compute_lr(self):\n        """\n        Compute the learning rate based on which phase of the cycle it is in.\n        - If in the first half of training, the learning rate gradually increases.\n        - If in the second half of training, the learning rate gradually decreases.\n        - If in the final `end_percentage` portion of training, the learning rate\n            is quickly reduced to near 100th of the original min learning rate.\n        # Returns:\n            the new learning rate\n        """\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n            new_lr = self.initial_lr * (1. + (current_percentage * (1. - 100.) / 100.)) * self.scale\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - (self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage * (self.scale * 100 - 1.)) * self.scale\n\n        else:\n            current_percentage = self.clr_iterations / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage * (self.scale * 100 - 1.)) * self.scale\n\n        if self.clr_iterations == self.num_iterations:\n            self.clr_iterations = 0\n\n        return new_lr\n\n    def compute_momentum(self):\n        """\n         Compute the momentum based on which phase of the cycle it is in.\n        - If in the first half of training, the momentum gradually decreases.\n        - If in the second half of training, the momentum gradually increases.\n        - If in the final `end_percentage` portion of training, the momentum value\n            is kept constant at the maximum initial value.\n        # Returns:\n            the new momentum value\n        """\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            new_momentum = self.max_momentum\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(self.mid_cycle_id))\n            new_momentum = self.max_momentum - current_percentage * (self.max_momentum - self.min_momentum)\n\n        else:\n            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n            new_momentum = self.max_momentum - current_percentage * (self.max_momentum - self.min_momentum)\n\n        return new_momentum\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        self._reset()\n        K.set_value(self.model.optimizer.lr, self.compute_lr())\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError("Momentum can be updated only on SGD optimizer !")\n\n            new_momentum = self.compute_momentum()\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n    def on_batch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        self.clr_iterations += 1\n        new_lr = self.compute_lr()\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        K.set_value(self.model.optimizer.lr, new_lr)\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError("Momentum can be updated only on SGD optimizer !")\n\n            new_momentum = self.compute_momentum()\n\n            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.verbose:\n            if self._update_momentum:\n                print(" - lr: %0.5f - momentum: %0.2f " % (self.history['lr'][-1],\n                                                           self.history['momentum'][-1]))\n\n            else:\n                print(" - lr: %0.5f " % (self.history['lr'][-1]))\n''')
    import logging
    import matplotlib
    from pprint import pprint
    
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import random
    import tensorflow as tf
    from sklearn import metrics
    from sklearn.metrics import precision_recall_curve
    from sklearn.model_selection import StratifiedKFold
    
    from src.Data import Data
    from src.Embedding import Embedding
    from src.Ensemble import Ensemble
    from src.Models import *  # Make all models available for easy script generation.
    from src.config import random_state as SEED, config_main as config
    
    np.random.seed(SEED)
    tf.set_random_seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    random.seed(SEED)
    
    
    def find_best_threshold(y_proba, y_true, plot=False):
        logging.info("Finding best threshold...")
        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
        thresholds = np.append(thresholds, 1.001)
        F = 2.0 / (1.0/precision + 1.0/recall)
        best_score = np.max(F)
        best_th = thresholds[np.argmax(F)]
        logging.info("Best score = {}. Best threshold = {}".format(best_score, best_th))
        if plot:
            plt.plot(thresholds, F, '-b')
            plt.plot([best_th], [best_score], '*r')
            plt.savefig('threshold.png')
            plt.close()
        return best_th
    
    
    def write_predictions(data, preds, thresh=0.5):
        logging.info("Writing predictions ...")
        preds = (preds > thresh).astype(int)
        out_df = pd.DataFrame({"qid": data.test_df["qid"].values})
        out_df['prediction'] = preds
        out_df.to_csv("submission.csv", index=False)
    
    
    def print_diagnostics(y_true, y_pred, file_suffix='', persist=True):
        try:
            cfn_matrix = metrics.confusion_matrix(y_true, y_pred)
        except ValueError:
            logging.warning("Warning: mix of binary and continuous targets used. Searching for best threshold.")
            thresh = find_best_threshold(y_pred, y_true)
            logging.warning("Applying threshold {} to predictions.".format(thresh))
            y_pred = (y_pred > thresh).astype(int)
            cfn_matrix = metrics.confusion_matrix(y_true, y_pred)
        with open('diagnostics' + file_suffix + '.txt', 'w') if persist else None as f:
            print("Confusion Matrix", file=f)
            print(cfn_matrix, file=f)
            print("-"*40, file=f)
            print("F1 score: " + str(metrics.f1_score(y_true, y_pred)), file=f)
            print("MCC score: " + str(metrics.matthews_corrcoef(y_true, y_pred)), file=f)
            print("precision: " + str(metrics.precision_score(y_true, y_pred)), file=f)
            print("Recall: " + str(metrics.recall_score(y_true, y_pred)), file=f)
    
    
    def get_wrongest(X, y_true, y_pred, num_wrongest=5):
        logging.info("Finding the worst predictions...")
        df = pd.DataFrame({'qid': X['qid'],
                           'question_text': X['question_text'],
                           'y_true': y_true,
                           'y_pred': y_pred.reshape(len(y_pred))})
        df['prediction_error'] = df['y_true'] - df['y_pred']
        df = df.sort_values('prediction_error')
        return df[df['y_true'] == 0].head(num_wrongest), df[df['y_true'] == 1].tail(num_wrongest)
    
    
    def print_wrongest(X, y_true, y_pred, num_wrongest=100, print_them=False, persist=True, file_suffix=None):
        def print_row(row):
            print("Q:" + row['question_text'])
            print("qid: " + row['qid'])
            print("Target: " + str(row['y_true']))
            print("Prediction: " + str(row['y_pred']))
            print("-"*40)
    
        wrongest_fps, wrongest_fns = get_wrongest(X, y_true, y_pred, num_wrongest=num_wrongest)
        if print_them:
            print("Wrongest {} false positives:".format(num_wrongest))
            print("-" * 40)
            for i, row in wrongest_fps.iterrows():
                print_row(row)
            print()
            print("Wrongest {} false negatives:".format(num_wrongest))
            print("-" * 40)
            for i, row in wrongest_fns.iterrows():
                print_row(row)
        if persist:
            filename = 'wrongest'
            if file_suffix:
                filename += '_' + file_suffix
            wrongest_fps.to_csv(filename + '_fps.csv', index=False)
            wrongest_fns.to_csv(filename + '_fns.csv', index=False)
        return wrongest_fps, wrongest_fns
    
    
    def cross_validate(model_class, data, embeddings, n_splits=4, show_wrongest=True, model_config=None,):
        logging.info("Cross validating model {} using {} folds...".format(model_class.__name__, str(n_splits)))
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
        models = list()
        for i, (train, test) in enumerate(skf.split(data.train_X, data.train_y)):
            logging.info("Running Fold {} of {}".format(i + 1, n_splits))
            models.append(None)
            cv_name = model_class.__name__ + '_cv_' + str(i)
            models[-1] = model_class(data=data, name=cv_name)
            models[-1].blend_embeddings(embeddings)
            models[-1].define_model(model_config)
            models[-1].fit(train_indices=train, val_indices=test, curve_file_suffix=str(i))
            if data.custom_features:
                predict_X = [data.train_X[test], data.train_features[test]]
            else:
                predict_X = [data.train_X[test]]
            pred_y_val = models[-1].predict(predict_X)
            print_diagnostics(data.train_y[test], pred_y_val, file_suffix='_' + cv_name)
            if show_wrongest:
                print_wrongest(data.train_df.iloc[test],
                               data.train_y[test],
                               pred_y_val,
                               num_wrongest=20,
                               persist=True,
                               file_suffix=models[-1].name)
        return models
    
    
    def load_embeddings(data, embedding_files, keep_index=True):
        embeddings = list()
        for f in embedding_files:
            embeddings.append(Embedding(data))
            embeddings[-1].load(f)
            if not keep_index:
                embeddings[-1].cleanup_index()
        return embeddings
    
    
    def save_unknown_words(data, embeddings, max_words=None):
        vocab = data.get_train_vocab()
        nb_words = 0
        for v in vocab.items():
            nb_words += v[1]
        for emb in embeddings:
            unknown_words = emb.check_coverage(vocab)
            df_unknown_words = pd.DataFrame(unknown_words, columns=['word', 'count'])\
                .sort_values('count', ascending=False)
            df_unknown_words['frequency'] = df_unknown_words['count'] / nb_words
            df_unknown_words = df_unknown_words.head(max_words)
            df_unknown_words.to_csv('unknown_words_' + emb.name + '.csv', index=False)
    
    
    def cleanup_models(models):
        for m in models:
            m.cleanup()
    
    def save_configs():
        from src.config import random_state, \
            config_data, \
            config_insincere_model, \
            config_lrfinder, \
            config_one_cycle, \
            config_main
        config_dict = locals().copy()
        with open('configs.txt', 'wt') as f:
            pprint(config_dict, stream=f)
        logging.info('Configurations: ')
        logging.info(pprint(config_dict))
    
    
    def main():
        embedding_files = config.get('embedding_files')
        dev_size = config.get('dev_size')
        data = Data()
        data.load(dev_size=dev_size)
        data.preprocessing()
        embeddings = load_embeddings(data, embedding_files)
        save_unknown_words(data, embeddings, max_words=200)
        models_all = list()
        for model in config.get('models'):
            model_class = globals()[model.get('class')]
            models_all.extend(cross_validate(model_class,
                                             data,
                                             embeddings,
                                             model_config=model.get('args')))
        cleanup_models(models_all)
        ensemble_cv = Ensemble(models_all)
        train_X = [data.train_X]
        val_X = [data.val_X]
        test_X = [data.test_X]
        if data.custom_features:
            train_X += [data.train_features]
            val_X += [data.val_features]
            test_X += [data.test_features]
        pred_train_y = ensemble_cv.predict_linear_regression(train_X, data.train_y, train_X)
        thresh = find_best_threshold(pred_train_y, data.train_y)
        pred_val_y = ensemble_cv.predict_linear_regression(train_X, data.train_y, val_X)
        print_diagnostics(data.val_y, (pred_val_y > thresh).astype(int))
        pred_y_test = ensemble_cv.predict_linear_regression(train_X, data.train_y, test_X)
        write_predictions(data, pred_y_test, thresh)
    
    
    if __name__ == "__main__":
        logging.getLogger()
        logging.basicConfig(
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.DEBUG,
            datefmt='%Y-%m-%d %H:%M:%S')
        save_configs()
        main()
        logging.info("Done!")
    