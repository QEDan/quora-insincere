{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf429078d0963591972679b43cc0dede615e1098"},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2cbbf44a872b84d7d4ee6af78977215fe61d4d3"},"cell_type":"code","source":"train_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea61c582076d663e9ba8393b17089d8c30d014d5"},"cell_type":"code","source":"stop = set(stopwords.words('english'))\ndef remove_stops(sentence):\n    filtered = list()\n    for w in sentence.split(\" \"):\n        if w not in stop:\n            filtered.append(w)\n    return \" \".join(w)\n    \ndef preprocess_questions(questions,\n                        remove_stop_words=True):\n    questions = questions.str.lower()\n    questions = questions.fillna(\"_na_\")\n    if remove_stop_words:\n        questions = questions.apply(remove_stops)\n    return questions\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"828aa1d58f6b1fc5a5a0dd40ae8b338e1c39cd30"},"cell_type":"code","source":"train_df['question_text'] = preprocess_questions(train_df['question_text'], remove_stop_words=False)\ntest_df['question_text'] = preprocess_questions(test_df['question_text'], remove_stop_words=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba5a1b8109dee2c9fbc628d5da4a7c3447d42fb8"},"cell_type":"code","source":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\ntrain_X = train_df[\"question_text\"].values\nval_X = val_df[\"question_text\"].values\ntest_X = test_df[\"question_text\"].values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"731924b9c1110766e0fc5f3e9e930f4825b06494"},"cell_type":"code","source":"dev_size = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ccb538a09f1474aea9b6d5734259edbd2536436"},"cell_type":"code","source":"## Comment this cell for the full run\n# dev_size = 500\n# train_X = train_X[:dev_size]\n# val_X = val_X[:dev_size]\n# test_X = test_X[:dev_size]\n# train_y = train_y[:dev_size]\n# val_y = val_y[:dev_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"047d67bbb18d3292789f69707627c87f00560172"},"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n\n    if c3 == 0:\n        return 0\n\n    precision = c1 / c2\n    recall = c1 / c3\n\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return f1_score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23f130e80159bb1701e449e2e91199dbfff1f1d4"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0526d96312fb22a52d6e6d5f26bca3cddf3ee1e4"},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(nb_words, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel_glove = Model(inputs=inp, outputs=x)\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_glove.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a560ab0dbab9cf6fdbdae6721ec030e300f19d78"},"cell_type":"code","source":"model_glove.fit(train_X, train_y, batch_size=512, epochs=4, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b843de5f261e76154f09b5e45f4bbe02cecb0c4"},"cell_type":"code","source":"def find_best_threshold(preds, y):\n    best_thresh = -100\n    best_score = -100\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        score =  metrics.f1_score(val_y, (preds>thresh).astype(int))\n        if score > best_score:\n            best_thresh = thresh\n        print(\"F1 score at threshold {0} is {1}\".format(thresh,score))\n    return best_thresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff43855164472de035a5a1d80b3db4838684701a"},"cell_type":"code","source":"pred_glove_val_y = model_glove.predict([val_X], batch_size=1024, verbose=1)\n# thresh = find_best_threshold(pred_glove_val_y, val_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2a33c252f31fddcc65896053184226128562776"},"cell_type":"markdown","source":"Results seem to be better than the model without pretrained embeddings."},{"metadata":{"trusted":true,"_uuid":"d51ff8ed6a87b488fec3ac84ca50df661d7c8193"},"cell_type":"code","source":"pred_glove_test_y = model_glove.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39d4fedab4ac170863a0ee1ca3aa9be1ee58fe02"},"cell_type":"code","source":"del word_index, embeddings_index, all_embs, embedding_matrix, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc6bab22dd12a09378f4b8b159cb7a5d88a3e7c0"},"cell_type":"markdown","source":"**Wiki News FastText Embeddings:**\n\nNow let us use the FastText embeddings trained on Wiki News corpus in place of Glove embeddings and rebuild the model."},{"metadata":{"trusted":true,"_uuid":"6f3d0fd28dd2b04eaccb732b96b872e5a223d962"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(nb_words, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel_fasttext = Model(inputs=inp, outputs=x)\nmodel_fasttext.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47238831a4701c8a67dc7ecb130ac1402baf7bb2"},"cell_type":"code","source":"model_fasttext.fit(train_X, train_y, batch_size=512, epochs=4, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ab4100f723ad535528865b1edc7896bce80223"},"cell_type":"code","source":"pred_fasttext_val_y = model_fasttext.predict([val_X], batch_size=1024, verbose=1)\nthresh = find_best_threshold(pred_fasttext_val_y, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3216362afb0f49579d287a06f13adf8cd7d8b0cf"},"cell_type":"code","source":"pred_fasttext_test_y = model_fasttext.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f24f9753ff1d933fa4f75a0ba34df305632d6e93"},"cell_type":"code","source":"del word_index, embeddings_index, all_embs, embedding_matrix, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ca44ac68bf404b9c26e07fbcc9c8ac793e04510"},"cell_type":"markdown","source":"**Paragram Embeddings:**\n\nIn this section, we can use the paragram embeddings and build the model and make predictions."},{"metadata":{"trusted":true,"_uuid":"25ec1aac4aedbf431a2d30de64030ce8e3203c18"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(nb_words, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel_paragram = Model(inputs=inp, outputs=x)\nmodel_paragram.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc188f2787ea7b98d3a40953a95a5fc09ff2764d"},"cell_type":"code","source":"model_paragram.fit(train_X, train_y, batch_size=512, epochs=4, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9abdfd1cf15257f2c0c2181a13327796e8d4584e"},"cell_type":"code","source":"pred_paragram_val_y = model_paragram.predict([val_X], batch_size=1024, verbose=1)\nthresh = find_best_threshold(pred_paragram_val_y, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99cb9f6145da909bd7436e46d47547efc097499d"},"cell_type":"code","source":"pred_paragram_test_y = model_paragram.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af087d21bdb4358701e31aded6b522accd5a8a64"},"cell_type":"code","source":"del word_index, embeddings_index, all_embs, embedding_matrix, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1312b7a4c3b67ca4ebd26fb083dbac3b6635dc2"},"cell_type":"markdown","source":"**Observations:**\n * Overall pretrained embeddings seem to give better results comapred to non-pretrained model. \n * The performance of the different pretrained embeddings are almost similar.\n \n**Final Blend:**\n\nThough the results of the models with different pre-trained embeddings are similar, there is a good chance that they might capture different type of information from the data. So let us do a blend of these three models by averaging their predictions."},{"metadata":{"trusted":true,"_uuid":"449bc59fdc9a719aa0759ac51a4481df113604ca"},"cell_type":"code","source":"pred_val_y = 0.33*pred_glove_val_y + 0.33*pred_fasttext_val_y + 0.34*pred_paragram_val_y ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fdbeffc0f84643d2832eec49234bd9d6c6e216b"},"cell_type":"markdown","source":"The result seems to better than individual pre-trained models and so we let us create a submission file using this model blend."},{"metadata":{"trusted":true,"_uuid":"c90fb4a4ef1b3b2ea06563a6901deac1b38822f3"},"cell_type":"code","source":"pred_test_y = 0.33*pred_glove_test_y + 0.33*pred_fasttext_test_y + 0.34*pred_paragram_test_y\nthresh = find_best_threshold(pred_val_y, val_y)\npred_test_y = (pred_test_y>thresh).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1fc756eae81364744d52c391f9a85c74ff10f57"},"cell_type":"code","source":"full_X = np.vstack([train_X, val_X, test_X])\nfull_y = np.vstack([train_y.reshape((len(train_y), 1)), val_y.reshape((len(val_y), 1)), pred_test_y])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62c8b940a5b6665268ca6aa74c81d16a5d3fb5b2"},"cell_type":"code","source":"model_glove.fit(test_X, pred_test_y, batch_size=512, epochs=4, validation_data=(val_X, val_y))\npred_glove_test_y_pseudo = model_glove.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63aafc91c41b07e93eb0f3f454e1c7283ab2b094"},"cell_type":"code","source":"model_fasttext.fit(test_X, pred_test_y, batch_size=512, epochs=4, validation_data=(val_X, val_y))\npred_fasttext_test_y_pseudo = model_fasttext.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3255e0e680a09965916e6dc64307fd513012682"},"cell_type":"code","source":"model_paragram.fit(test_X, pred_test_y, batch_size=512, epochs=4, validation_data=(val_X, val_y))\npred_paragram_test_y_pseudo = model_paragram.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b4393631745a5f11d1e0dc0f1c3cb81ab32f860"},"cell_type":"code","source":"pred_test_y_pseudo = 0.33*pred_glove_test_y_pseudo \\\n    + 0.33*pred_fasttext_test_y_pseudo \\\n    + 0.34*pred_paragram_test_y_pseudo \nthresh = find_best_threshold(pred_val_y, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d6bf363b3dffaa0a8acf2dff0321ce7f5eecf60"},"cell_type":"code","source":"if dev_size:\n    test_df = test_df.head(dev_size)\npred_test_y_pseudo = (pred_test_y_pseudo>thresh).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y_pseudo\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1327210b68de4190405bce64cb54aa28743d43cf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d6aa4e2b7eff626bdba54ad0d33b4c0e03e6169"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}